{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Gantry","text":"<p>A Python DICOM Object Model and Redaction Toolkit.</p> <p></p> <p>Gantry provides a high-performance, object-oriented interface for managing, analyzing, and de-identifying DICOM datasets. It is designed for large-scale ingestion, precise pixel redaction, and strict PHI compliance.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>Object-Oriented API: Work with <code>Patient</code>, <code>Study</code>, <code>Series</code>, and <code>Instance</code> objects directly.</li> <li>Persistent Sessions: All metadata is indexed in a SQLite database, allowing you to pause/resume large jobs and providing an audit trail.</li> <li>Parallel Processing: Multi-process ingestion and export for maximum throughput.</li> <li>Robust Redaction:</li> <li>Metadata: Configurable tag removal, replacement, and shifting.</li> <li>Pixel Data: Machine-specific redaction zones (ROI) to scrub burned-in PHI.</li> <li>Reversibility: Optional cryptographic identity preservation.</li> <li>Codecs: Robust support for JPEG Lossless, JPEG 2000, and other compressed formats via <code>imagecodecs</code>.</li> <li>Free-threaded Python Ready: Fully compatible with Python 3.13t+ (no-GIL) for true parallelism.</li> <li>Deep Memory Management: Automatic pixel offloading allows processing datasets far exceeding available RAM.</li> </ul>"},{"location":"#performance","title":"Performance","text":"<p>Gantry is designed for massive scale. Recent stress tests verify robust linear scaling on datasets up to 100GB.</p> <p></p>"},{"location":"#100gb-scalability-test","title":"100GB Scalability Test","text":"<ul> <li>Input: 101,000 files (50GB Single-Frame + 50GB Multi-Frame).</li> <li>Import Speed: ~14 seconds (Index-only ingestion).</li> <li>Export Speed: ~79 seconds (Streaming Write).</li> <li>Memory: Peaks at 5.4GB, stable regardless of dataset size.</li> </ul> <p>The architecture uses O(1) memory streaming, ensuring it never runs out of RAM even when processing terabytes of data.</p>"},{"location":"#micro-benchmarks-metadata-operations","title":"Micro-Benchmarks (Metadata Operations)","text":"Operation Scale Time (Mac M3 Max) Throughput Identity Locking 100,000 Instances ~0.13 s 769k / sec Persist Findings 100,000 Issues ~0.13 s 770k / sec"},{"location":"#architecture","title":"Architecture","text":"<p>Gantry acts as a smart indexing layer over your raw DICOM files. It does not modify your original data. Instead, it builds a lightweight metadata index (SQLite) and exposes a clean Python Object Model for manipulation.</p>"},{"location":"#1-the-session-facade","title":"1. The Session Facade","text":"<p>The <code>Session</code> object is your single entry point. It manages:</p> <ul> <li>Persistence: Auto-saving state to <code>gantry.db</code>.</li> <li>Inventory: Tracking Patients, Studies, and Series.</li> <li>Transactions: Atomic persistence of changes.</li> </ul>"},{"location":"#2-object-model","title":"2. Object Model","text":"<p>Gantry abstracts DICOM into a semantic hierarchy, removing the pain of manual tag iteration.</p> <pre><code>graph LR\n    Patient --&gt; Study\n    Study --&gt; Series\n    Series --&gt; Instance\n    Instance --&gt; Pixels((Pixel Data))</code></pre> <ul> <li>Patient: Root entity (Name, ID).</li> <li>Study: A distinct visit/exam.</li> <li>Series: A scan or reconstruction (e.g., \"ct_soft_kernel\").</li> </ul>"},{"location":"analytics/","title":"Analytics &amp; Reporting","text":"<p>Gantry is designed not just for de-identification, but for understanding your data. It includes built-in tools for compliance verification, cohort analysis, and data exploration.</p>"},{"location":"analytics/#compliance-reports","title":"Compliance Reports","text":"<p>For regulatory audits (HIPAA/GDPR), Gantry can generate a formal Compliance Report. This single-document artifact summarizes the entire session, ensuring transparent documentation of your de-identification process.</p> <pre><code># Generate a Markdown report\nsession.generate_report(\"compliance_report.md\")\n</code></pre> <p>The report includes:</p> <ol> <li>Validation Status: Uses Gantry's internal audit logic to grade the session (PASS / REVIEW_REQUIRED).</li> <li>Audit Trail: Aggregated counts of actions taken (e.g., number of patients anonymized, pixels redacted).</li> <li>Exceptions: A detailed list of any warnings or errors encountered (e.g., \"Corrupt pixel data in File X\", \"Burned-In Annotation found\").</li> <li>Manifest: A summary of the processed cohort (Top studies by size).</li> </ol> <p>Format Options</p> <p>Currently, Gantry supports Markdown (<code>.md</code>) reports. PDF support is planned for future releases via Pandoc integration.</p>"},{"location":"analytics/#cohort-analysis-eda","title":"Cohort Analysis (EDA)","text":"<p>Gantry treats your DICOM data as a structured database, not just a pile of files. You can leverage the <code>export_dataframe</code> method to extract a flattened inventory of your cohort for analysis with Pandas, Jupyter, or Tableau.</p>"},{"location":"analytics/#1-export-to-pandas","title":"1. Export to Pandas","text":"<pre><code># Export inventory to a Pandas DataFrame\n# expand_metadata=True parses the JSON attributes into columns\ndf = session.export_dataframe(expand_metadata=True)\n\n# Inspect the data\nprint(df.head())\nprint(df.groupby('Modality')['InstanceCount'].sum())\n</code></pre>"},{"location":"analytics/#2-parquet-export","title":"2. Parquet Export","text":"<p>For massive datasets (100k+ images), exporting to Parquet is recommended for performance and compatibility with external BI tools (PowerBI, Tableau, Apache Spark).</p> <pre><code># Export full cohort to Parquet\nsession.export_to_parquet(\"cohort_inventory.parquet\")\n</code></pre>"},{"location":"analytics/#query-based-export","title":"Query-Based Export","text":"<p>One of Gantry's most powerful features is Query-Based Export. Instead of exporting the entire session, you can filter the export using Pandas-style queries or a subset DataFrame.</p>"},{"location":"analytics/#use-case-export-only-thick-slice-cts","title":"Use Case: \"Export only thick-slice CTs\"","text":"<pre><code># 1. Get the inventory\ndf = session.export_dataframe(expand_metadata=True)\n\n# 2. Define your criteria (Standard Pandas syntax)\n# e.g., Keep only CT scans with SliceThickness &gt; 2.5mm\nsubset = df[ \n    (df['Modality'] == 'CT') &amp; \n    (df['SliceThickness'].astype(float) &gt; 2.5) \n]\n\nprint(f\"Filtering: {len(df)} -&gt; {len(subset)} instances.\")\n\n# 3. Feed the subset back into the exporter\nsession.export(\"export_thick_cts\", subset=subset)\n</code></pre>"},{"location":"analytics/#use-case-export-list-of-accession-numbers","title":"Use Case: \"Export List of Accession Numbers\"","text":"<p>You can also filter by a list of strict identifiers if you have an external manifest.</p> <pre><code># Filter by Series Instance UIDs\ntarget_series = [\"1.2.840...\", \"1.2.840...\"]\n\n# Filter the dataframe\nsubset = df[df['SeriesInstanceUID'].isin(target_series)]\n\nsession.export(\"export_selected_series\", subset=subset)\n</code></pre>"},{"location":"architecture/","title":"Architecture","text":"<p>Gantry acts as a smart indexing layer over your raw DICOM files. It does not modify your original data. Instead, it builds a lightweight metadata index (SQLite) and exposes a clean Python Object Model for manipulation.</p>"},{"location":"architecture/#1-the-session-facade","title":"1. The Session Facade","text":"<p>The <code>Session</code> object is your single entry point. It manages:</p> <ul> <li>Persistence: Auto-saving state to <code>gantry.db</code>.</li> <li>Inventory: Tracking Patients, Studies, and Series.</li> <li>Transactions: Atomic persistence of changes.</li> </ul>"},{"location":"architecture/#2-object-model","title":"2. Object Model","text":"<p>Gantry abstracts DICOM into a semantic hierarchy, removing the pain of manual tag iteration.</p> <pre><code>graph LR\n    Patient --&gt; Study\n    Study --&gt; Series\n    Series --&gt; Instance\n    Instance --&gt; Pixels((Pixel Data))</code></pre> <ul> <li>Patient: Root entity (Name, ID).</li> <li>Study: A distinct visit/exam.</li> <li>Series: A scan or reconstruction (e.g., \"ct_soft_kernel\").</li> <li>Instance: A single DICOM slice. Pixel data is extracted upfront; the heavyweight pixel array is sequestered in a binary sidecar immediately upon ingestion and loaded into memory only when needed.</li> </ul>"},{"location":"architecture/#3-safety-pipeline-the-8-checkpoints","title":"3. Safety Pipeline (The 8 Checkpoints)","text":"<p>Gantry enforces a strict checkpoint system to ensure data safety:</p> <ol> <li>Ingest: Load raw data into the managed session index.</li> <li>Examine: Inventory the cohort and equipment.</li> <li>Configure: Define privacy tags and redaction rules.</li> <li>Audit (Target): Measure PHI risks against the configuration.</li> <li>Backup: (Optional) Securely lock original identities for reversibility.</li> <li>Anonymize: Apply remediation to metadata (in-memory).</li> <li>Redact: Scrub pixel data for specific machines (in-memory).</li> <li>Verify: Re-audit the session to ensure a clean state.</li> <li>Export: Write clean DICOM files to disk.</li> </ol>"},{"location":"architecture/#4-persistence-architecture-hybrid-storage","title":"4. Persistence Architecture (Hybrid Storage)","text":"<p>Gantry uses <code>sqlite3</code> for metadata management, employing a Hybrid Storage Model to balance query performance with schema flexibility.</p>"},{"location":"architecture/#the-problem","title":"The Problem","text":"<p>DICOM data effectively comes in two shapes:</p> <ol> <li>Standard Tags: Always present, well-defined (e.g., <code>Modality</code>, <code>StudyDate</code>).</li> <li>Private Tags: Manufacturer-specific, sparse, and extremely numerous.</li> </ol> <p>Storing everything in a single table with 3000 columns is impossible. Storing everything in a vertical Entity-Attribute-Value (EAV) table is too slow for bulk loading.</p>"},{"location":"architecture/#the-solution-core-json-vertical-split","title":"The Solution: Core JSON + Vertical Split","text":"<p>Values are automatically split during persistence based on their Group ID:</p> Storage Location Table Column Content Rationale Core Attributes <code>instances</code> <code>attributes_json</code> All Standard Tags (Even Groups) + Binary Placeholders Speed. SQLite's JSONB operators allow us to load 10,000 instances in sub-second time without performing 10,000+ joins. Vertical Attributes <code>instance_attributes</code> <code>tag_group</code>, <code>tag_elem</code>, <code>value</code> Private Tags (Odd Groups) Flexibility. Private tags are sparse. This EAV storage prevents the Core JSON from becoming bloated with garbage data while keeping private tags queryable. Pixel Data <code>[name]_pixels.bin</code> Comparison to DB via Offset/Length Raw Byte Stream Offloading. Gigabytes of pixel data are kept out of the DB to prevent bloating and ensure the index remains lightweight."},{"location":"architecture/#database-schema-reference","title":"Database Schema Reference","text":"Table Purpose Key Columns <code>patients</code> Root entity. <code>patient_id</code> (PK), <code>patient_name</code> <code>studies</code> Represents a patient visit. <code>study_instance_uid</code> (PK), <code>study_date</code>, <code>patient_id_fk</code> <code>series</code> Represents a scan/sequence. <code>series_instance_uid</code> (PK), <code>modality</code>, <code>manufacturer</code>, <code>model_name</code>, <code>device_serial_number</code>, <code>study_id_fk</code> <code>instances</code> Represents a single DICOM file. <code>sop_instance_uid</code> (PK), <code>attributes_json</code>, <code>pixel_hash</code>, <code>file_path</code>, <code>series_id_fk</code> <code>instance_attributes</code> Storage for Private/Odd Group tags. <code>instance_uid</code> (FK), <code>group_id</code>, <code>element_id</code>, <code>value</code> <code>audit_log</code> Logs all modification actions. <code>timestamp</code>, <code>action_type</code>, <code>entity_uid</code>, <code>details</code> <code>phi_findings</code> Stores potential PHI detected during audit. <code>entity_uid</code>, <code>field_name</code>, <code>value</code>, <code>remediation_action</code>"},{"location":"architecture/#schema-visualization","title":"Schema Visualization","text":"<pre><code>erDiagram\n    PATIENTS ||--|{ STUDIES : contains\n    STUDIES ||--|{ SERIES : contains\n    SERIES ||--|{ INSTANCES : contains\n\n    PATIENTS {\n        string patient_id PK\n        string patient_name\n    }\n\n    STUDIES {\n        string study_instance_uid PK\n        date study_date\n    }\n\n    SERIES {\n        string series_instance_uid PK\n        string modality\n        string manufacturer\n        string model_name\n    }\n\n    INSTANCES {\n        string sop_instance_uid PK\n        json attributes_json \"Core Metadata\"\n        string pixel_hash \"Integrity Check\"\n    }\n\n    INSTANCES ||--o{ INSTANCE_ATTRIBUTES : \"owns private tags\"\n    INSTANCE_ATTRIBUTES {\n        string instance_uid FK\n        hex group_id\n        hex element_id\n        string value\n    }\n\n    INSTANCES ||--|| SIDECAR_FILE : \"references pixels\"\n    SIDECAR_FILE {\n        binary pixel_bytes\n    }\n\n    INSTANCES ||--o{ PHI_FINDINGS : \"triggers\"\n    PHI_FINDINGS {\n        string field_name\n        string value\n        string remediation\n    }\n\n    INSTANCES ||--o{ AUDIT_LOG : \"generates\"\n    AUDIT_LOG {\n        timestamp time\n        string action\n        string details\n    }</code></pre>"},{"location":"changelog/","title":"Changelog","text":"<p>All notable changes to the \"Gantry\" project will be documented in this file.</p> <p>The format is based on Keep a Changelog, and this project adheres to Semantic Versioning.</p>"},{"location":"changelog/#051-2025-12-31","title":"[0.5.1] - 2025-12-31","text":""},{"location":"changelog/#added","title":"Added","text":"<ul> <li>Python 3.13t+ Support: Full compatibility with Free-threaded Python (no-GIL).</li> <li>Benchmarks: Documented performance achieving ~770k instances/sec for metadata operations.</li> <li>Migration Tools: Added <code>gantry.utils.ctp_parser</code> to convert legacy CTP scripts to Gantry YAML.</li> </ul>"},{"location":"changelog/#changed","title":"Changed","text":"<ul> <li>Dependencies: Merged <code>[images]</code> extra into core install. Gantry now installs <code>pillow</code> and <code>imagecodecs</code> by default.</li> <li>Documentation: Complete rewrite of <code>README.md</code> to reflect v2.0 Architecture.</li> </ul>"},{"location":"changelog/#fixed","title":"Fixed","text":"<ul> <li>Decompression: Robust support for encapsulated Multi-Frame images and JPEG Lossless (Process 14) via <code>imagecodecs</code>.</li> <li>Robustness: Implemented automatic fallback to installed codecs if standard <code>pydicom</code> handler discovery fails (e.g. environment path issues).</li> <li>Handling: Fixed <code>UnboundLocalError</code> regressions in error reporting.</li> <li>Correctness: Fixed bug where encapsulated pixel data was passed incorrectly to decoders.</li> </ul>"},{"location":"changelog/#050-2025-12-18","title":"[0.5.0] - 2025-12-18","text":""},{"location":"changelog/#added_1","title":"Added","text":"<ul> <li>Performance:</li> <li>Split-Persistence: Introduced a binary sidecar (<code>_pixels.bin</code>) for high-speed append-only pixel storage, reducing SQLite metadata size by 99%+.</li> <li>Database Indexing: Added indexes to Foreign Keys (<code>patient_id_fk</code>, etc.) and <code>audit_log</code> for O(1) query performance.</li> <li>Multithreaded Redaction: <code>redact_pixels</code> now uses <code>ThreadPoolExecutor</code> to process Machine Rules in parallel, achieving near-linear speedup on multi-core systems.</li> <li>Optimization:</li> <li>Inverted Redaction Loop: Refactored logic to iterate images once per machine (O(M)) instead of applying every rule to every image (O(NM)).</li> <li>Empty Zone Skipping: Automatically skips processing machines with no configured ROIs.</li> <li>Benchmarks:</li> <li>Verified throughput of 140,000 metadata inserts/sec and 580 MB/s pixel writes in stress tests.</li> <li>UX:</li> <li>Added realtime <code>tqdm</code> progress bars for redaction.</li> </ul>"},{"location":"changelog/#fixed_1","title":"Fixed","text":"<ul> <li>Multiprocessing: Fixed \"Pickling Error\" on Windows/spawn start methods by creating lightweight copies of the object graph for worker communication.</li> <li>Redaction: Fixed crash when <code>get_pixel_data</code> returns <code>None</code> (missing file).</li> <li>Redaction: Fixed \"Completely Outside\" warning logic for RGB images (interpreting Channels as Columns).</li> </ul>"},{"location":"changelog/#053-2026-01-13","title":"[0.5.3] - 2026-01-13","text":""},{"location":"changelog/#fixed_2","title":"Fixed","text":"<ul> <li>Free-Threaded Stability: Fixed a race condition in <code>PersistenceManager</code> during shutdown that caused data loss in no-GIL environments (Python 3.13t+).</li> <li>Export Reliability: Fixed a \"Pickling Error\" regression in <code>run_parallel</code> when using <code>maxtasksperchild</code> with memory leak mitigation.</li> <li>Export Safety: Enforced strict exception raising in export workers; failed decompression now correctly fails the export instead of failing silently.</li> <li>Testing: Resolved <code>MagicMock</code> serialization errors during tests ensuring test suite passes cleanly on all platforms.</li> <li>Debug Cleanup: Removed residual debug output from Sidecar pixel loading and Benchmark stress tests.</li> </ul>"},{"location":"changelog/#changed_1","title":"Changed","text":"<ul> <li>Dependencies: Bumping version for maintenance release.</li> </ul>"},{"location":"changelog/#052-2026-01-08","title":"[0.5.2] - 2026-01-08","text":""},{"location":"changelog/#added_2","title":"Added","text":"<ul> <li>Free-Threaded Stability: Implemented Versioned Dirty Tracking in <code>DicomItem</code> to correctly handle concurrent modifications in no-GIL environments (Python 3.13t+).</li> <li>Memory Optimization: Implemented <code>Instance.unload_pixel_data()</code> and automatic pixel swapping to <code>_pixels.bin</code>. This allows the session to process datasets larger than available RAM by offloading modified pixels to disk.</li> <li>Global Export Parallelism: Export process now utilizes a global pool of workers across all patients, significantly improving throughput for datasets with many small studies.</li> <li>Async Audit Queue: Implemented an asynchronous queue for writing audit logs to SQLite, preventing database locking and contention during highly parallel operations.</li> <li>Redaction Progress UI: Consolidated multiple per-machine progress bars into a single, clean \"Redacting Rules\" indicator.</li> <li>Verbose Logging: Added <code>verbose</code> flag to Redaction Service methods to allow optional debugging of missing pixels/rules.</li> </ul>"},{"location":"changelog/#changed_2","title":"Changed","text":"<ul> <li>Removed Legacy Config: Dropped support for legacy list-based configuration files and internal list-parsing logic. Configuration must now be the standard Unified YAML format.</li> <li>Thread Tuning: Adjusted default parallel worker count to <code>1.5 * CPU_CORES</code> (previously <code>min(32, cpu+4)</code>).</li> <li>Warning Suppression: Redaction warnings (e.g., missing pixel data) are now suppressed by default to reduce console noise.</li> <li>Redaction Execution: Switched <code>redact()</code> to enforce threading (<code>force_threads=True</code>) to correctly handle in-memory state updates and avoid pickling errors with SQLite connections.</li> </ul>"},{"location":"changelog/#fixed_3","title":"Fixed","text":"<ul> <li>Persistence Race Condition: Fixed a critical race condition where modifications made during an asynchronous save operation were lost/overwritten.</li> <li>Memory Leak: Resolved memory accumulation in <code>lock_identities</code> by implementing batch chunking (<code>auto_persist_chunk_size</code>).</li> <li>Progress Reporting: Fixed broken/instant completion progress bars in <code>lock_identities</code>.</li> <li>Logging Regression: Fixed assertion failure in <code>test_full_logging_coverage</code> regarding suppressed log messages.</li> <li>NameError: Fixed a variable scoping issue in <code>RedactionService.process_machine_rules</code>.</li> <li>Parallel Redaction Bugs: Resolved <code>pickle</code> errors and state synchronization issues in parallel redaction by enforcing threading.</li> </ul>"},{"location":"changelog/#041-2025-12-12","title":"[0.4.1] - 2025-12-12","text":""},{"location":"changelog/#added_3","title":"Added","text":"<ul> <li>Configuration Actions: Support for <code>REMOVE</code> and <code>EMPTY</code> actions in <code>privacy_config.json</code> for precise tag handling.</li> <li>Ingest Summary: <code>ingest</code> command now provides a detailed count of imported objects.</li> </ul>"},{"location":"changelog/#fixed_4","title":"Fixed","text":"<ul> <li>Persistence Priority: Fixed \"Split Brain\" issue where remediated <code>Study</code>/<code>Series</code> metadata was overwritten by original file attributes during export.</li> <li>Export Error: Fixed validation strictness to allow export of files with stripped Command Set (Group 0000) tags.</li> <li>API Consistency: Unified <code>scan_for_phi</code> and <code>audit</code> methods.</li> </ul>"},{"location":"changelog/#040-2025-12-11","title":"[0.4.0] - 2025-12-11","text":""},{"location":"changelog/#added_4","title":"Added","text":"<ul> <li>Features:</li> <li>Safe Export: New <code>export(safe=True)</code> mode ensuring no PHI leaves the system.</li> <li>Reversible Anonymization: Securely embed encrypted original identities (<code>gantry.key</code>).</li> <li>Manual Persistence: Changed default behavior to manual <code>.save()</code> for better user control.</li> <li>Background Persistence: Non-blocking saves via <code>PersistenceManager</code>.</li> <li>PHI Analysis Reports: <code>scan_for_phi</code> now returns a rich <code>PhiReport</code> object with Pandas DataFrame support.</li> <li>Parallel Processing: Multi-process support for Import and PHI Scanning.</li> <li>Improvements:</li> <li>Console Output: Suppressed noisy <code>pydicom</code> warnings and improved <code>tqdm</code> progress bars.</li> <li>Batch UX: Better feedback during long-running operations.</li> <li>Test Coverage: specific tests for <code>crypto</code>, <code>config</code>, and <code>safe_export</code>.</li> </ul>"},{"location":"changelog/#fixed_5","title":"Fixed","text":"<ul> <li>Regression: Addressed silent failure in pixel export when source files are missing.</li> <li>Bug: Fixed <code>TypeError</code> in Remediation Date Shifting.</li> <li>Bug: Fixed <code>MultiValue</code> JSON serialization error in persistence.</li> <li>Bug: Fixed <code>ValueError</code> regarding Group 0000 elements during export.</li> </ul>"},{"location":"changelog/#030-2025-12-11","title":"[0.3.0] - 2025-12-11","text":""},{"location":"changelog/#added_5","title":"Added","text":"<ul> <li>Robust Persistence (SQLite): Replaced <code>Pickle</code> with <code>SQLite</code> for session storage (<code>gantry.db</code>). Allows for scale and external querying.</li> <li>Audit Trail: Implemented a comprehensive audit system. Actions such as <code>Redaction</code> and <code>Remediation</code> are now logged to the <code>audit_log</code> table in the database.</li> <li>Automated PHI Remediation:</li> <li>Metadata Anonymization: Automatically detects and anonymizes Patient Names and IDs.</li> <li>Deterministic Date Shifting: Shifts study dates by a consistent offset (based on Patient ID hash) to preserve temporal relationships while obscuring actual dates.</li> <li><code>apply_remediation</code> API: Added top-level API to <code>DicomSession</code> to easily apply fixes found by the privacy inspector.</li> <li>Documentation: Significant updates to <code>README.md</code> and architecture documentation.</li> </ul>"},{"location":"changelog/#changed_3","title":"Changed","text":"<ul> <li>Breaking Change: The internal persistence format has changed from <code>.pkl</code> to <code>.db</code>. Existing sessions from v0.2.0 cannot be loaded and must be re-imported.</li> <li>Dependency Update: Added <code>sqlite3</code> (stdlib) as a core dependency for the store backend.</li> </ul>"},{"location":"changelog/#020-2025-12-10","title":"[0.2.0] - 2025-12-10","text":""},{"location":"changelog/#added_6","title":"Added","text":"<ul> <li>JSON Configuration Validation: <code>ConfigLoader</code> now rejects rules with missing fields or invalid/illegal ROI definitions.</li> <li>ROI Safety Checks: Redaction operations now explicitly check image bounds, clipping ROIs to the image dimensions and warning if they are completely out of bounds.</li> <li>File Deduplication: <code>DicomImporter</code> now detects and skips files that have already been imported into the current session.</li> </ul>"},{"location":"changelog/#fixed_6","title":"Fixed","text":"<ul> <li>Recursive Sequence Import: Nested sequences (e.g., in Structured Reports) are now correctly recursed and indexed.</li> <li>Pixel Depth Export: <code>DicomExporter</code> now correctly preserves 8-bit usage for relevant modalities (e.g., US, SC) instead of hardcoding 12/16-bit depth.</li> </ul>"},{"location":"changelog/#010-2025-12-09","title":"[0.1.0] - 2025-12-09","text":""},{"location":"changelog/#added_7","title":"Added","text":"<ul> <li>Core Architecture: Implemented the semantic object graph (<code>Patient</code> \u2192 <code>Study</code> \u2192 <code>Series</code> \u2192 <code>Instance</code>) to replace flat dictionary handling.</li> <li>Facade Interface: Added <code>gantry.Session</code> class as the primary entry point for user interaction, managing imports, persistence, and inventory.</li> <li>Lazy Loading: Implemented a Proxy Pattern for <code>Instance</code> objects. Metadata is loaded into memory during import, while heavy pixel data is read from disk only upon request.</li> <li>De-Identification Service: Added <code>RedactionService</code> to modify pixel data (burn-in removal) based on specific machine serial numbers.</li> <li>Configuration Management: Added support for <code>redaction_rules.json</code> to define Redaction Regions of Interest (ROIs) externally.</li> <li>Machine Indexing: Created <code>MachinePixelIndex</code> to efficiently group and retrieve instances by their Equipment attributes (Manufacturer, Model, Serial Number).</li> <li>Builder Pattern: Added <code>DicomBuilder</code> (and fluent sub-builders) to allow programmatic construction of complex DICOM hierarchies for testing and synthetic data generation.</li> <li>IOD Validation: Implemented <code>IODValidator</code> to enforce Type 1 and Type 2 attribute compliance for standard SOP Classes (e.g., CT Image Storage) before export.</li> <li>Persistence: Added <code>pickle</code>-based serialization to save and resume session state (<code>DicomStore</code>).</li> <li>Import/Export: Created <code>DicomImporter</code> for fast metadata scanning and <code>DicomExporter</code> for writing valid, standards-compliant <code>.dcm</code> files.</li> </ul>"},{"location":"changelog/#security","title":"Security","text":"<ul> <li>Pixel data redaction is performed in-memory and committed to new files; original files are treated as read-only during the session to prevent accidental data loss.</li> </ul>"},{"location":"configuration/","title":"Configuration Guide","text":"<p>Gantry uses a Unified YAML Configuration (v2.0) to control all aspects of de-identification, including PHI tag rules, date shifting, and pixel redaction.</p> <p>This file allows you to define a reproducible privacy policy that can be shared across your team or version controlled.</p>"},{"location":"configuration/#quick-reference","title":"Quick Reference","text":"Section Description privacy_profile Base set of rules (e.g., \"basic\", \"comprehensive\"). date_jitter Randomly shifts dates to preserve intervals while hiding exact dates. remove_private_tags Removes vendor-specific private tags (odd groups). phi_tags overrides or adds specific tag rules (e.g., <code>PatientName</code>). machines Defines burn-in redaction zones for specific equipment."},{"location":"configuration/#complete-example","title":"Complete Example","text":"<p>Save this as <code>gantry_config.yaml</code>:</p> <pre><code># 1. Privacy Profile (Base Rules)\n# Options: \"basic\", \"comprehensive\", or path to external YAML\nprivacy_profile: \"basic\"\n\n# 2. Date Jitter\n# Shifts all dates by a random amount within this range.\n# The shift is deterministic per-patient (consistent across studies).\ndate_jitter:\n  min_days: -30\n  max_days: -10\n\n# 3. Private Tags\n# Remove all odd-group tags (vendor specific) unless whitelisted?\nremove_private_tags: true\n\n# 4. Custom PHI Tags (Overrides Profile)\nphi_tags:\n  \"0010,0010\": \n    action: \"REMOVE\"\n    name: \"PatientName\"\n\n  \"0010,0020\": \n    action: \"REPLACE\"\n    name: \"PatientID\"\n    value: \"ANONYMIZED\" # Matches default if omitted\n\n  \"0008,0080\":\n    action: \"KEEP\" # Exception: Keep InstitutionName\n\n# 5. Pixel Redaction Rules (Machine Specific)\nmachines:\n  - serial_number: \"US-12345\"\n    model_name: \"Voluson E10\"\n    redaction_zones:\n      # [row_start, row_end, col_start, col_end]\n      - [0, 50, 0, 800]   # Top Banner\n      - [900, 1024, 0, 400] # Bottom Left Details\n</code></pre>"},{"location":"configuration/#detailed-options","title":"Detailed Options","text":""},{"location":"configuration/#1-privacy-profile","title":"1. Privacy Profile","text":"<p>Sets the baseline behavior for thousands of DICOM tags.</p> <pre><code>privacy_profile: \"comprehensive\"\n</code></pre> <ul> <li><code>basic</code>: Implements the DICOM PS3.15 Annex E Basic Profile. Retains some descriptors but removes direct identifiers.</li> <li><code>comprehensive</code>: Aggressive de-identification. Removes almost all non-structural text fields.</li> <li>External File: You can provide a path to another YAML file (e.g., <code>./profiles/my_hospital_standard.yaml</code>) to inherit its rules.</li> </ul>"},{"location":"configuration/#2-date-jitter","title":"2. Date Jitter","text":"<p>Shifts all date attributes (<code>DA</code>, <code>DT</code>) by a random number of days.</p> <ul> <li>Logic: Gantry generates a secret random offset for each <code>PatientID</code>. This offset is consistent for that patient across all their studies and series, preserving temporal relationships (intervals) while hiding the absolute dates.</li> <li> <p>Config:</p> <pre><code>date_jitter:\n  min_days: -10\n  max_days: 10\n</code></pre> </li> </ul>"},{"location":"configuration/#3-private-tags","title":"3. Private Tags","text":"<p>DICOM Private Tags (Odd Group Numbers, e.g., <code>0009,xxxx</code>) often contain hidden PHI strings dumped by the machine.</p> <pre><code>remove_private_tags: true\n</code></pre> <ul> <li><code>true</code>: Removes ALL private tags. (Recommended for safety).</li> <li><code>false</code>: Retains them (Use only if you are sure they are safe or strictly needed for analysis).</li> </ul>"},{"location":"configuration/#4-phi-tags","title":"4. PHI Tags","text":"<p>Define specific rules for individual DICOM tags. Keys must be uppercase hex strings (e.g. <code>\"0010,0010\"</code>).</p> <p>Supported Actions:</p> Action Logic Example Config <code>REPLACE</code> Replaces value with \"ANONYMIZED\" (or custom string). <code>action: \"REPLACE\", value: \"Project-X\"</code> <code>REMOVE</code> Completely deletes the tag from the dataset. <code>action: \"REMOVE\"</code> <code>EMPTY</code> Sets the tag value to an empty string. <code>action: \"EMPTY\"</code> <code>SHIFT</code> Applies the per-patient Date Jitter offset (Dates only). <code>action: \"SHIFT\"</code> <code>KEEP</code> Explicitly retains the original value (Exception to profile). <code>action: \"KEEP\"</code> <p>Example:</p> <pre><code>phi_tags:\n  \"0008,1030\": { \"action\": \"EMPTY\", \"name\": \"StudyDescription\" }\n  \"0010,0030\": { \"action\": \"SHIFT\", \"name\": \"PatientBirthDate\" }\n</code></pre>"},{"location":"configuration/#5-pixel-redaction-machines","title":"5. Pixel Redaction (Machines)","text":"<p>Automatically scrubs burned-in text (pixels) for specific devices. Gantry identifies the machine using the <code>DeviceSerialNumber</code> (0018,1000) tag.</p> <pre><code>machines:\n  - serial_number: \"SN-9999\"\n    model_name: \"Documentation Only\"\n    redaction_zones:\n      - [0, 100, 0, 500]\n</code></pre> <ul> <li><code>serial_number</code> (Required): Exact match for <code>0018,1000</code>.</li> <li><code>redaction_zones</code>: List of regions to zero out.</li> <li>Format: <code>[y1, y2, x1, x2]</code> (Row Start, Row End, Col Start, Col End).</li> <li>Coordinates are 0-indexed.</li> </ul>"},{"location":"configuration/#6-programmatic-configuration-api-20","title":"6. Programmatic Configuration (API 2.0)","text":"<p>In addition to YAML files, you can manage the configuration dynamically using Python code via the <code>session.configuration</code> property.</p>"},{"location":"configuration/#accessing-configuration","title":"Accessing Configuration","text":"<pre><code>import gantry\n\nsession = gantry.Session(data_directory=\"./dicom_data\")\n\n# improved: Access the GantryConfiguration object directly\nconfig = session.configuration\n\nprint(config.rules)    # List active redaction rules\nprint(config.phi_tags) # List active PHI tag overrides\n</code></pre>"},{"location":"configuration/#methods","title":"Methods","text":""},{"location":"configuration/#add_ruleserial_number-manufacturerunknown-modelunknown-zonesnone","title":"<code>add_rule(serial_number, manufacturer=\"Unknown\", model=\"Unknown\", zones=None)</code>","text":"<p>Add a new machine redaction rule dynamically.</p> <pre><code># Add a rule for a specific ultrasound machine\nsession.configuration.add_rule(\n    serial_number=\"US-5555\",\n    manufacturer=\"GE\",\n    model=\"Voluson\",\n    zones=[[0, 50, 0, 800]] # [y1, y2, x1, x2]\n)\n</code></pre>"},{"location":"configuration/#delete_ruleserial_number","title":"<code>delete_rule(serial_number)</code>","text":"<p>Remove a rule by serial number.</p> <pre><code>session.configuration.delete_rule(\"US-5555\")\n</code></pre>"},{"location":"configuration/#set_phi_tagtag-action-replacementnone","title":"<code>set_phi_tag(tag, action, replacement=None)</code>","text":"<p>Update the policy for a specific DICOM tag.</p> <pre><code># Force removal of PatientWeight\nsession.configuration.set_phi_tag(\"0010,1030\", \"REMOVE\")\n\n# Replace StudyDescription with a constant\nsession.configuration.set_phi_tag(\"0008,1030\", \"REPLACE\", replacement=\"RESEARCH STUDY\")\n</code></pre>"},{"location":"configuration/#generating-configuration-templates","title":"Generating Configuration Templates","text":"<p>You can generate a starter <code>gantry_config.yaml</code> based on your current session inventory. This is useful for bootstrapping a new configuration file that includes all detected machines.</p> <pre><code># Inspects data, finds all unique machine serials, and writes a config file\nsession.create_config(\"my_new_policy.yaml\")\n</code></pre>"},{"location":"installation/","title":"Installation","text":"<p>Gantry requires Python 3.9+.</p> <pre><code># Clone the repository\ngit clone https://github.com/kvnlng/Gantry.git\ncd Gantry\n\n# Install with dependencies (including codecs)\npip install -e .\n</code></pre> <p>Note</p> <p>The <code>imagecodecs</code> dependency is included and strongly recommended for handling JPEG Lossless and other compressed Transfer Syntaxes.</p>"},{"location":"installation/#system-requirements","title":"System Requirements","text":"<p>Gantry's parallel processing engine is designed to maximize CPU utilization. However, heavy operations like JPEG 2000 compression require significant memory per worker.</p> <ul> <li>Memory: Gantry is memory-intensive during specific operations (e.g., Pixel Redaction, J2K Export).</li> <li>Minimum: 2GB RAM per vCPU.</li> <li>Recommended (Heavy Workloads): 8GB RAM per vCPU (e.g., for massive multi-frame J2K compression).</li> <li>Concurrency: By default, Gantry uses all available cores (<code>1:1</code> ratio). Use <code>GANTRY_MAX_WORKERS</code> env var to limit this if OOM occurs.</li> </ul>"},{"location":"migration/","title":"Migration Tools","text":""},{"location":"migration/#clinical-trial-processor-ctp","title":"Clinical Trial Processor (CTP)","text":"<p>Gantry includes a utility to convert legacy CTP <code>DicomPixelAnonymizer.script</code> files into Gantry's YAML configuration format.</p> <pre><code># Convert CTP script to Gantry YAML\npython -m gantry.utils.ctp_parser /path/to/anonymizer.script output_rules.yaml\n</code></pre> <p>This parser extracts:</p> <ul> <li>Manufacturer/Model matching criteria.</li> <li>Redaction zones (automatically converting <code>x,y,w,h</code> to <code>r1,r2,c1,c2</code>).</li> </ul>"},{"location":"performance/","title":"Performance","text":"<p>Gantry is designed for massive scale. Recent stress tests verify robust linear scaling on datasets up to 100GB.</p> <p></p>"},{"location":"performance/#100gb-scalability-test","title":"100GB Scalability Test","text":"<ul> <li>Input: 101,000 files (50GB Single-Frame + 50GB Multi-Frame).</li> <li>Import Speed: Uses Sidecar Generation to extract pixel data upfront, ensuring constant-time access during analysis.</li> <li>Export Speed: High-speed streaming write using cached sidecar data.</li> <li>Memory: Peaks at stable levels regardless of dataset size due to aggressive offloading.</li> </ul> <p>The architecture uses O(1) memory streaming, ensuring it never runs out of RAM even when processing terabytes of data.</p>"},{"location":"performance/#memory-management-architecture","title":"Memory Management Architecture","text":"<p>Gantry employs a \"Deep Memory Management\" strategy to handle large-scale datasets on consumer hardware.</p>"},{"location":"performance/#1-process-isolation-redaction","title":"1. Process Isolation (Redaction)","text":"<p>Pixel redaction is the most memory-intensive operation (loading 500MB+ arrays). Gantry uses Process Isolation (<code>ProcessPoolExecutor</code>) to execute these tasks.</p> <ul> <li>Each worker process loads the pixel data, applies redactions, and then exits.</li> <li>This guarantees that the operating system reclaims all memory resources immediately after each task, preventing fragmentation or reference leaks in the main process.</li> </ul>"},{"location":"performance/#2-streaming-ingest","title":"2. Streaming Ingest","text":"<p>The ingestion pipeline uses a streaming generator pattern with a <code>chunksize=1</code>.</p> <ul> <li>Files are processed one by one.</li> <li>Results are yielded immediately to the database.</li> <li>The IPC queue never buffers more than a single item, keeping memory footprint constant (O(1)) regardless of input size.</li> </ul>"},{"location":"performance/#3-zero-copy-persistence","title":"3. Zero-Copy Persistence","text":"<p>To prevent memory spikes during export:</p> <ul> <li>Pixel data is passed directly from NumPy arrays to the storage backend.</li> <li>We utilize buffer interfaces and on-the-fly <code>zlib</code> compression to avoid creating intermediate Python byte strings (which would double memory usage).</li> </ul>"},{"location":"performance/#scalability-benchmarks","title":"Scalability Benchmarks","text":"<p>Recent stress tests (January 2026) verified robust sub-linear scaling capabilities.</p> Phase Files Raw Data Max RSS (Memory) Status Scaling Factor Phase 0 1 ~500 MB ~0.5 GB Success 1x Phase 1 10 ~5 GB ~3.8 GB Success ~7.6x Phase 2 100 ~50 GB ~11.3 GB Success &lt;3x <p>Key Finding: Increasing the dataset size by 10x (10 to 100 files) only resulted in a 3x increase in peak memory usage. This demonstrates that Gantry effectively decouples memory consumption from dataset size.</p>"},{"location":"performance/#micro-benchmarks-metadata-operations","title":"Micro-Benchmarks (Metadata Operations)","text":"Operation Scale Time (Mac M3 Max) Throughput Identity Locking 100,000 Instances ~0.13 s 769k / sec Persist Findings 100,000 Issues ~0.13 s 770k / sec"},{"location":"quickstart/","title":"Quick Start","text":""},{"location":"quickstart/#1-initialize-a-session","title":"1. Initialize a Session","text":"<p>Gantry uses a persistent session to manage your workflow. Unlike scripts that run once and forget, a Session creates a local SQLite database (<code>gantry.db</code>) to index your data. This allows you to pause, resume, and audit your work without re-scanning thousands of files.</p> <pre><code>from gantry import Session\n\n# Initialize a new session (creates 'gantry.db' by default)\nsession = Session(\"my_project.db\")\n</code></pre>"},{"location":"quickstart/#2-ingest-examine","title":"2. Ingest &amp; Examine","text":"<p>Ingestion builds a lightweight metadata index of your DICOM files. Gantry scans your folders recursively, extracting patient/study/series information into the database without moving or modifying your original files. It is resilient to nested directories and non-DICOM clutter.</p> <pre><code>session.ingest(\"/path/to/dicom/data\")\nsession.save() # Persist the index to disk\n\n# Print a summary of the cohort and equipment\nsession.examine()\n</code></pre>"},{"location":"quickstart/#3-configure-audit","title":"3. Configure &amp; Audit","text":"<p>Before changing anything, define your privacy rules. Use <code>create_config</code> to generate a scaffolding based on your inventory, then <code>audit</code> to scan that inventory against your rules. This \"Measure Twice, Cut Once\" approach lets you identify all PHI risks before applying any irreversible changes.</p> <pre><code># Create a default configuration file (v2.0 YAML)\nsession.create_config(\"config.yaml\")\n\n# Load the configuration (rules, tags, jitter)\nsession.load_config(\"config.yaml\")\n\n# Run an audit to find PHI\nreport = session.audit() \nsession.save_analysis(report)\n\nprint(f\"Found {len(report)} potential PHI issues.\")\n</code></pre>"},{"location":"quickstart/#4-backup-identity-optional","title":"4. Backup Identity (Optional)","text":"<p>To enable reversible anonymization, generate a cryptographic key and \"lock\" the original patient identities into a secure, encrypted DICOM tag. This must be done before anonymization.</p> <pre><code># Enable encryption (generates 'gantry.key')\nsession.enable_reversible_anonymization()\n\n# cryptographically lock identities for all patients found in the audit\n# cryptographically lock identities for all patients found in the audit\n# Optional: Specify custom tags to preserve (defaults to Name, ID, DOB, Sex, Accession)\nsession.lock_identities(report, tags_to_lock=[\"0010,0010\", \"0010,0020\", \"0010,0030\"])\nsession.save()\n</code></pre>"},{"location":"quickstart/#5-anonymize-redact-export","title":"5. Anonymize, Redact &amp; Export","text":"<p>Remediation is a multi-stage process performed in-memory:</p> <ol> <li>Anonymize: Strips or replaces metadata tags (PatientID, Names, Dates) based on your config.</li> <li>Redact: Loads pixel data and scrubs burned-in PHI from defined regions.</li> <li>Export: The final \"Gatekeeper\". Writes clean files to a new directory. Setting <code>safe=True</code> ensures the export halts if any verification checks fail (e.g., corrupt images or missing codecs).</li> </ol> <pre><code># Apply metadata remediation (anonymization) using the findings\nsession.anonymize(report)\n\n# Apply pixel redaction rules (requires config to be loaded)\nsession.redact()\n\n# Export only safe (clean) data to a new folder\n# Compression=\"j2k\" optionally compresses output to JPEG 2000\nsession.export(\"/path/to/export_clean\", safe=True, compression=\"j2k\")\n</code></pre> <p>Progress for the save, memory release, and export phases will be displayed:</p> <pre><code>Preparing for export (Auto-Save &amp; Memory Release)...\nReleasing Memory: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5000/5000 [00:02&lt;00:00, 2000.00img/s]\nMemory Cleanup: Released 5000 images from RAM.\nExecuting Redaction Rules...\nRedacting: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 150/150 [00:05&lt;00:00, 28.00img/s]\nExporting session to output_folder (safe=True)...\nExporting:  15%|\u2588\u2588\u258c       | 15/100 [00:05&lt;00:30,  2.80patient/s]\n</code></pre>"},{"location":"quickstart/#6-recover-identity-optional","title":"6. Recover Identity (Optional)","text":"<p>If you have a valid key (<code>gantry.key</code>) and need to retrieve the original identity of an anonymized patient:</p> <pre><code># Load the session containing anonymized data\nsession = Session(\"my_project.db\")\nsession.enable_reversible_anonymization(\"gantry.key\")\n\n# Recover the original PatientName and PatientID\n# Recover the original identity and restore attributes in-memory\n# restore=True (default) automatically updates all instances with original values\nsession.recover_patient_identity(\"ANON_12345\", restore=True)\n\n# Now, accessing p.patient_name or instance attributes returns original data\nprint(f\"Restored: {session.store.patients[0].patient_name}\")\n</code></pre>"},{"location":"roadmap/","title":"Project Roadmap","text":"<p>This document outlines the development plan for Gantry. We welcome contributions from the community to help us achieve these milestones!</p>"},{"location":"roadmap/#current-status-v053-maintenance-release","title":"\ud83d\udccd Current Status: v0.5.3 (Maintenance Release)","text":"<ul> <li>[x] Core Object Model: <code>Patient</code> -&gt; <code>Study</code> -&gt; <code>Series</code> -&gt; <code>Instance</code></li> <li>[x] Split-Persistence Architecture: Binary sidecar (<code>_pixels.bin</code>) for high-speed pixel storage.</li> <li>[x] Database Indexing: O(1) lookups and scalable Joins via SQLite indexes.</li> <li>[x] Multithreaded Redaction: Parallelized pixel redaction using <code>ThreadPoolExecutor</code>.</li> <li>[x] Free-Threaded Stability: Full support for Python 3.14t (no-GIL) via versioned dirty tracking.</li> <li>[x] Deep Memory Management: Automatic pixel offloading (<code>Instance.unload_pixel_data()</code>) to handle datasets exceeding RAM.</li> <li>[x] Async Audit Queue: Non-blocking SQLite persistence for high-throughput auditing.</li> <li>[x] Custom Privacy Profiles: Support for external YAML profiles.</li> <li>[x] Standard Privacy Profiles: Built-in support for DICOM PS3.15 Annex E.</li> <li>[x] Legacy Config Removal: Streamlined codebase by removing list-based config support.</li> </ul>"},{"location":"roadmap/#upcoming-milestones","title":"\ud83d\ude80 Upcoming Milestones","text":""},{"location":"roadmap/#v060-analytics-reporting","title":"v0.6.0 - Analytics &amp; Reporting","text":"<p>Focus: Empowering users to understand their data through deep inspection on the object graph.</p> <ul> <li>[ ] Dataframe Export: Expose a method to flatten <code>Patient -&gt; Study -&gt; Series -&gt; Instance</code> hierarchy into a comprehensive parquet file.</li> <li>[ ] Sidecar Compaction: Utility to vacuum/compact the <code>_pixels.bin</code> file to reclaim space from deleted or redacted images.</li> <li>[ ] Pixel Content Analysis (OCR): Detect burned-in text using OCR (Tesseract) / Cloud Vision to automatically flag sensitive images.</li> <li>[ ] Metadata Querying: Enable SQL-like querying on the dataframe (e.g., \"Find all scans with <code>SliceThickness &lt; 1.0</code> acquired by 'GE' scanners\").</li> <li>[ ] Query-based Export: Allow users to filter exports using criteria (e.g., <code>session.export(query=\"Modality=='CT' and SliceThickness &gt; 5.0\")</code>).</li> <li>[ ] Compliance Reporting: Generate reports verifying dataset compliance against a selected privacy profile.</li> <li>[ ] Export Manifest: Automatic generation of visual (HTML) and machine-readable (CSV/JSON) manifests listing all exported files and their key metadata.</li> <li>[ ] Audit Reporting: Export comprehensive CSV reports of the session inventory, including details on what was redacted or modified.</li> <li>[ ] Structured Reporting (SR) Support: Support for deep parsing and anonymization of DICOM Structured Reports.</li> </ul>"},{"location":"roadmap/#v070-the-connector-networking","title":"v0.7.0 - The Connector (Networking)","text":"<p>Focus: Integrating Gantry into clinical workflows via DIMSE services.</p> <ul> <li>[ ] PACS Integration: Implement C-STORE, C-FIND, C-MOVE using <code>pynetdicom</code> to query and pull studies directly.</li> <li>[ ] Research Export Formats: Native support for exporting to NIfTI and BIDS standards.</li> </ul>"},{"location":"roadmap/#v080-cloud-scale","title":"v0.8.0 - Cloud Scale","text":"<p>Focus: Native support for cloud storage to handle massive datasets.</p> <ul> <li>[ ] Persistence Abstraction: Decouple storage logic to enable cloud backends and future plugins.</li> <li>[ ] Cloud Storage Adapters: Native ingestion/export for S3, Google Cloud Storage, and Azure Blob.</li> </ul>"},{"location":"roadmap/#v100-production-release","title":"v1.0.0 - Production Release","text":"<ul> <li>[ ] API Freeze: Lock down the <code>DicomSession</code> interface.</li> <li>[ ] Documentation: Complete API reference and tutorials on ReadTheDocs or Wiki.</li> <li>[ ] PyPI Release: Publish package to the Python Package Index.</li> </ul>"},{"location":"roadmap/#v110-zero-code-cli","title":"v1.1.0 - Zero Code (CLI)","text":"<p>Focus: Making Gantry accessible to non-programmers and CI pipelines.</p> <ul> <li>[ ] Gantry CLI: A rich command-line interface for auditing and anonymizing datasets.</li> </ul>"},{"location":"roadmap/#future-ideas-backlog","title":"\ud83d\udd2e Future Ideas (Backlog)","text":"<ul> <li>3D Defacing: Algorithmically remove facial features from 3D volumes (MRI/CT).</li> <li>Plugin System: Hooks for custom user scripts during ingest/audit/export loops.</li> <li>GUI Wrapper for <code>DicomSession</code>.</li> <li>Official Docker Image: Optimized container build for Gantry with pre-configured codecs and dependencies.</li> </ul>"},{"location":"api/entities/","title":"Entities API","text":""},{"location":"api/entities/#gantry.entities","title":"<code>gantry.entities</code>","text":""},{"location":"api/entities/#gantry.entities.DicomItem","title":"<code>DicomItem</code>  <code>dataclass</code>","text":"<p>Base class for any entity that holds DICOM attributes and sequences.</p> Source code in <code>gantry/entities.py</code> <pre><code>@dataclass(slots=True)\nclass DicomItem:\n    \"\"\"\n    Base class for any entity that holds DICOM attributes and sequences.\n    \"\"\"\n    # init=False to avoid constructor conflicts during inheritance\n    # init=False to avoid constructor conflicts during inheritance\n    attributes: Dict[str, Any] = field(init=False)\n    sequences: Dict[str, DicomSequence] = field(init=False)\n\n    # Versioning for robust persistence\n    _mod_count: int = field(init=False, default=0)\n    _saved_mod_count: int = field(init=False, default=-1)\n\n    def __post_init__(self):\n        self.attributes = {}\n        self.sequences = {}\n        # Initial state is dirty (1 &gt; 0)\n        self._mod_count = 1\n        self._saved_mod_count = 0\n\n    @property\n    def _dirty(self) -&gt; bool:\n        return self._mod_count &gt; self._saved_mod_count\n\n    @_dirty.setter\n    def _dirty(self, value: bool):\n        # Legacy support: setting True increments version\n        if value:\n            self._mod_count += 1\n        else:\n            # Unsafe clear: assumes current state is saved\n            self._saved_mod_count = self._mod_count\n\n    def mark_saved(self, version_saved: int):\n        \"\"\"Marks specific version as saved. Robust against concurrent edits.\"\"\"\n        if version_saved &gt; self._saved_mod_count:\n            self._saved_mod_count = version_saved\n\n    def set_attr(self, tag: str, value: Any):\n        \"\"\"Sets a generic attribute by its hex tag (e.g., '0010,0010').\"\"\"\n        self.attributes[tag] = value\n        self._mod_count += 1\n\n    def add_sequence_item(self, tag: str, item: 'DicomItem'):\n        \"\"\"Appends a new item to a sequence, creating the sequence if needed.\"\"\"\n        if tag not in self.sequences:\n            self.sequences[tag] = DicomSequence(tag=tag)\n        self.sequences[tag].items.append(item)\n        self._mod_count += 1\n\n    def mark_clean(self):\n        # Legacy: force clean\n        self._saved_mod_count = self._mod_count\n        for seq in self.sequences.values():\n            for item in seq.items:\n                item.mark_clean()\n</code></pre>"},{"location":"api/entities/#gantry.entities.DicomItem.add_sequence_item","title":"<code>add_sequence_item(tag, item)</code>","text":"<p>Appends a new item to a sequence, creating the sequence if needed.</p> Source code in <code>gantry/entities.py</code> <pre><code>def add_sequence_item(self, tag: str, item: 'DicomItem'):\n    \"\"\"Appends a new item to a sequence, creating the sequence if needed.\"\"\"\n    if tag not in self.sequences:\n        self.sequences[tag] = DicomSequence(tag=tag)\n    self.sequences[tag].items.append(item)\n    self._mod_count += 1\n</code></pre>"},{"location":"api/entities/#gantry.entities.DicomItem.mark_saved","title":"<code>mark_saved(version_saved)</code>","text":"<p>Marks specific version as saved. Robust against concurrent edits.</p> Source code in <code>gantry/entities.py</code> <pre><code>def mark_saved(self, version_saved: int):\n    \"\"\"Marks specific version as saved. Robust against concurrent edits.\"\"\"\n    if version_saved &gt; self._saved_mod_count:\n        self._saved_mod_count = version_saved\n</code></pre>"},{"location":"api/entities/#gantry.entities.DicomItem.set_attr","title":"<code>set_attr(tag, value)</code>","text":"<p>Sets a generic attribute by its hex tag (e.g., '0010,0010').</p> Source code in <code>gantry/entities.py</code> <pre><code>def set_attr(self, tag: str, value: Any):\n    \"\"\"Sets a generic attribute by its hex tag (e.g., '0010,0010').\"\"\"\n    self.attributes[tag] = value\n    self._mod_count += 1\n</code></pre>"},{"location":"api/entities/#gantry.entities.DicomSequence","title":"<code>DicomSequence</code>  <code>dataclass</code>","text":"<p>Represents a DICOM Sequence (SQ) containing multiple DicomItems.</p> Source code in <code>gantry/entities.py</code> <pre><code>@dataclass(slots=True)\nclass DicomSequence:\n    \"\"\"\n    Represents a DICOM Sequence (SQ) containing multiple DicomItems.\n    \"\"\"\n    tag: str\n    items: List['DicomItem'] = field(default_factory=list)\n</code></pre>"},{"location":"api/entities/#gantry.entities.Equipment","title":"<code>Equipment</code>  <code>dataclass</code>","text":"<p>Immutable Equipment definition. Frozen=True allows hashing, enabling unique set generation.</p> Source code in <code>gantry/entities.py</code> <pre><code>@dataclass(frozen=True, slots=True)\nclass Equipment:\n    \"\"\"\n    Immutable Equipment definition.\n    Frozen=True allows hashing, enabling unique set generation.\n    \"\"\"\n    manufacturer: str\n    model_name: str\n    device_serial_number: str = \"\"\n</code></pre>"},{"location":"api/entities/#gantry.entities.Instance","title":"<code>Instance</code>  <code>dataclass</code>","text":"<p>               Bases: <code>DicomItem</code></p> <p>Represents a single DICOM image (SOP Instance). Manages lazy loading of pixel data.</p> Source code in <code>gantry/entities.py</code> <pre><code>@dataclass(slots=True)\nclass Instance(DicomItem):\n    \"\"\"\n    Represents a single DICOM image (SOP Instance).\n    Manages lazy loading of pixel data.\n    \"\"\"\n    sop_instance_uid: str = \"\"\n    sop_class_uid: str = \"\"\n    instance_number: int = 0\n\n    # Persistence: Link to original file for lazy loading\n    file_path: Optional[str] = None\n\n    # Transient: Actual pixel data (NOT persisted to pickle)\n    pixel_array: Optional[np.ndarray] = field(default=None, repr=False)\n\n    # Transient: Lazy Loader (Callable that returns np.ndarray)\n    # Used for Sidecar or deferred logic\n    _pixel_loader: Optional[Callable[[], np.ndarray]] = field(default=None, repr=False)\n\n    # Transient: Hash for Integrity Check\n    _pixel_hash: Optional[str] = field(default=None, repr=False)\n\n    # Transient: Track if dates have been shifted in memory\n    date_shifted: bool = field(default=False, init=False)\n\n    # Transient: Index of all text-based nodes for O(1) PHI scanning\n    # List of (DicomItem_Reference, Tag_String)\n    text_index: List[Tuple['DicomItem', str]] = field(default_factory=list, init=False, repr=False)\n\n    def __post_init__(self):\n        # Inlined from DicomItem to avoid super() mismatch issues with slots/reloads\n        self.attributes = {}\n        self.sequences = {}\n\n        # Versioning\n        self._mod_count = 1\n        self._saved_mod_count = 0\n\n        self.set_attr(\"0008,0018\", self.sop_instance_uid)\n        self.set_attr(\"0008,0016\", self.sop_class_uid)\n        self.set_attr(\"0020,0013\", self.instance_number)\n\n\n\n    def regenerate_uid(self):\n        \"\"\"\n        Generates a new, globally unique SOP Instance UID.\n        Call this whenever pixel data is modified.\n        \"\"\"\n        # 1. Generate new UID using pydicom's generator (or your org root)\n        new_uid = generate_uid()\n\n        # 2. Update the Object Property\n        self.sop_instance_uid = new_uid\n\n        # 3. Update the DICOM Attribute Dictionary\n        self.set_attr(\"0008,0018\", new_uid)\n\n        # 4. Detach from physical file\n        # Since this object is now a \"new\" instance in memory, \n        # it no longer matches the file on disk.\n        self.file_path = None \n\n        from .logger import get_logger\n        get_logger().debug(f\"  -&gt; Identity regenerated: {new_uid}\")\n\n    def unload_pixel_data(self):\n        \"\"\"\n        Clears the cached pixel_array from memory to free resources.\n        Only performs the clear if the data can be re-loaded (file_path or _pixel_loader exists).\n        Returns True if unloaded, False if unsafe to unload.\n        \"\"\"\n        if self.pixel_array is None:\n            return True\n\n        if self.file_path or self._pixel_loader:\n            self.pixel_array = None\n            # print(f\"DEBUG: Unloaded pixels for {self.sop_instance_uid}\")\n            return True\n        else:\n            # Data is in memory only (e.g. modified but not saved)\n            print(f\"DEBUG: FAILED TO UNLOAD {self.sop_instance_uid} - No file path or loader!\")\n            return False\n\n    def get_pixel_data(self) -&gt; Optional[np.ndarray]:\n        \"\"\"\n        Returns pixel_array. Loads from disk if not in memory.\n        Returns None if no pixel data is present.\n        \"\"\"\n        if self.pixel_array is not None:\n            return self.pixel_array\n\n        if self._pixel_loader:\n             try:\n                 # Invoke callback (e.g. sidecar read)\n                 arr = self._pixel_loader()\n                 # Use set_pixel_data to ensure attributes (rows, cols) are synced \n                 # This is critical if the loader returns a raw array but attributes were not yet set/restored\n                 self.set_pixel_data(arr)\n                 return self.pixel_array\n             except Exception as e:\n                 raise RuntimeError(f\"Pixel Loader failed for {self.sop_instance_uid}: {e}\")\n\n        if self.file_path and os.path.exists(self.file_path):\n            try:\n                # Read pixel data on demand\n                ds = None\n                try:\n                    ds = pydicom.dcmread(self.file_path)\n\n                    self.set_pixel_data(ds.pixel_array)  # Cache it in memory\n                    return self.pixel_array\n                except (AttributeError, TypeError):\n                    # No pixel data element\n                    return None\n                except Exception as e:\n                    if \"no pixel data\" in str(e).lower():\n                        return None\n                    # Re-raise to be handled by outer except\n                    raise e\n\n            except Exception as e:\n                # Try explicit fallback to gantry.imagecodecs_handler\n                # Pydicom sometimes fails to iterate handlers correctly or swallows errors.\n                try:\n                    import gantry.imagecodecs_handler as h\n                    if ds is not None and h.is_available() and h.supports_transfer_syntax(ds.file_meta.TransferSyntaxUID):\n                        arr = h.get_pixel_data(ds)\n                        self.set_pixel_data(arr)\n                        return self.pixel_array\n                except Exception as fallback_e:\n                    # Fallback failed, proceed to raise original error\n                    pass\n\n                # Try to get Transfer Syntax UID for better debugging\n                ts_uid = \"Unknown\"\n                if ds is not None and hasattr(ds, \"file_meta\"):\n                     ts_uid = getattr(ds.file_meta, \"TransferSyntaxUID\", \"Unknown\")\n\n                if \"missing dependencies\" in str(e) or \"decompress\" in str(e):\n                    # Enhanced debug output\n                    handlers = []\n                    try:\n                        # pydicom is already imported globally\n                        handlers = [str(h) for h in pydicom.config.pixel_data_handlers]\n                    except: pass\n\n                    raise RuntimeError(\n                        f\"Failed to decompress pixel data for {os.path.basename(self.file_path)} \"\n                        f\"(Transfer Syntax: {ts_uid}).\\n\"\n                        f\"Underlying Error: {e}\\n\"\n                        f\"Active pydicom handlers: {handlers}\\n\"\n                        \"Missing image codecs. Please ensure 'pillow', 'pylibjpeg', or 'gdcm' are installed.\"\n                    ) from e\n\n                # If we just caught the re-raised \"no pixel data\" exception, it would be handled above, \n                # but if dcmread fails completely or something else happens:\n                raise RuntimeError(f\"Lazy load failed for {self.file_path}: {e}\")\n\n        raise FileNotFoundError(f\"Pixels missing and file not found: {self.file_path}\")\n\n    def set_pixel_data(self, array: np.ndarray):\n        \"\"\"\n        Sets the pixel array and automatically updates metadata tags\n        (rows, cols, samples, frames, etc.) based on array shape.\n        Handles unpacking of 2D/3D/4D arrays.\n        \"\"\"\n        self.pixel_array = array\n        shape = array.shape\n        ndim = len(shape)\n\n        # Defaults\n        samples = 1\n        frames = 1\n\n        if ndim == 1:\n             # Flattened array (e.g. from Sidecar loader)\n             # Attempt to reshape using existing metadata if available\n             try:\n                 r = int(self.attributes.get(\"0028,0010\", 0))\n                 c = int(self.attributes.get(\"0028,0011\", 0))\n                 s = int(self.attributes.get(\"0028,0002\", 1))\n                 f = int(self.attributes.get(\"0028,0008\", 1))\n\n                 expected_size = r * c * s * f\n                 if expected_size &gt; 0 and array.size &gt;= expected_size:\n                      # Truncate padding if present (DICOM alignment)\n                      if array.size &gt; expected_size:\n                          array = array[:expected_size]\n\n                      # Reshape logic\n                      if f &gt; 1:\n                          array = array.reshape((f, r, c, s)) if s &gt; 1 else array.reshape((f, r, c))\n                      elif s &gt; 1:\n                          array = array.reshape((r, c, s))\n                      else:\n                          array = array.reshape((r, c))\n                      self.pixel_array = array\n                      return # Done, attributes already match\n                 elif expected_size == 0:\n                      # Metadata missing, treat as linear?\n                      pass\n\n             except:\n                 pass\n\n             # Only raise if we couldn't resolve it\n             if len(array.shape) == 1: # Still 1D\n                  rows, cols = 1, shape[0]\n\n        elif ndim == 2:\n            rows, cols = shape\n        elif ndim == 3:\n            if shape[-1] in [3, 4]:\n                rows, cols, samples = shape\n            else:\n                frames, rows, cols = shape\n        elif ndim == 4:\n            frames, rows, cols, samples = shape\n        else:\n            raise ValueError(f\"Unknown shape: {shape}\")\n\n        self.set_attr(\"0028,0010\", rows)\n        self.set_attr(\"0028,0011\", cols)\n        self.set_attr(\"0028,0002\", samples)\n        if frames &gt; 1: self.set_attr(\"0028,0008\", str(frames))\n        if samples &gt;= 3: \n            self.set_attr(\"0028,0004\", \"RGB\")\n            self.set_attr(\"0028,0006\", 0) # Force Interleaved (standard numpy)\n        else:\n            # Preserve existing PhotometricInterpretation (e.g. MONOCHROME1)\n            # Only set default if missing\n            if not self.attributes.get(\"0028,0004\"):\n                self.set_attr(\"0028,0004\", \"MONOCHROME2\")\n\n        # Ensure BitsAllocated matches array data type\n        # SidecarPixelLoader relies on this to determine uint8 vs uint16\n        bits = array.itemsize * 8\n        self.set_attr(\"0028,0100\", bits)\n\n        self._mod_count += 1\n</code></pre>"},{"location":"api/entities/#gantry.entities.Instance.get_pixel_data","title":"<code>get_pixel_data()</code>","text":"<p>Returns pixel_array. Loads from disk if not in memory. Returns None if no pixel data is present.</p> Source code in <code>gantry/entities.py</code> <pre><code>def get_pixel_data(self) -&gt; Optional[np.ndarray]:\n    \"\"\"\n    Returns pixel_array. Loads from disk if not in memory.\n    Returns None if no pixel data is present.\n    \"\"\"\n    if self.pixel_array is not None:\n        return self.pixel_array\n\n    if self._pixel_loader:\n         try:\n             # Invoke callback (e.g. sidecar read)\n             arr = self._pixel_loader()\n             # Use set_pixel_data to ensure attributes (rows, cols) are synced \n             # This is critical if the loader returns a raw array but attributes were not yet set/restored\n             self.set_pixel_data(arr)\n             return self.pixel_array\n         except Exception as e:\n             raise RuntimeError(f\"Pixel Loader failed for {self.sop_instance_uid}: {e}\")\n\n    if self.file_path and os.path.exists(self.file_path):\n        try:\n            # Read pixel data on demand\n            ds = None\n            try:\n                ds = pydicom.dcmread(self.file_path)\n\n                self.set_pixel_data(ds.pixel_array)  # Cache it in memory\n                return self.pixel_array\n            except (AttributeError, TypeError):\n                # No pixel data element\n                return None\n            except Exception as e:\n                if \"no pixel data\" in str(e).lower():\n                    return None\n                # Re-raise to be handled by outer except\n                raise e\n\n        except Exception as e:\n            # Try explicit fallback to gantry.imagecodecs_handler\n            # Pydicom sometimes fails to iterate handlers correctly or swallows errors.\n            try:\n                import gantry.imagecodecs_handler as h\n                if ds is not None and h.is_available() and h.supports_transfer_syntax(ds.file_meta.TransferSyntaxUID):\n                    arr = h.get_pixel_data(ds)\n                    self.set_pixel_data(arr)\n                    return self.pixel_array\n            except Exception as fallback_e:\n                # Fallback failed, proceed to raise original error\n                pass\n\n            # Try to get Transfer Syntax UID for better debugging\n            ts_uid = \"Unknown\"\n            if ds is not None and hasattr(ds, \"file_meta\"):\n                 ts_uid = getattr(ds.file_meta, \"TransferSyntaxUID\", \"Unknown\")\n\n            if \"missing dependencies\" in str(e) or \"decompress\" in str(e):\n                # Enhanced debug output\n                handlers = []\n                try:\n                    # pydicom is already imported globally\n                    handlers = [str(h) for h in pydicom.config.pixel_data_handlers]\n                except: pass\n\n                raise RuntimeError(\n                    f\"Failed to decompress pixel data for {os.path.basename(self.file_path)} \"\n                    f\"(Transfer Syntax: {ts_uid}).\\n\"\n                    f\"Underlying Error: {e}\\n\"\n                    f\"Active pydicom handlers: {handlers}\\n\"\n                    \"Missing image codecs. Please ensure 'pillow', 'pylibjpeg', or 'gdcm' are installed.\"\n                ) from e\n\n            # If we just caught the re-raised \"no pixel data\" exception, it would be handled above, \n            # but if dcmread fails completely or something else happens:\n            raise RuntimeError(f\"Lazy load failed for {self.file_path}: {e}\")\n\n    raise FileNotFoundError(f\"Pixels missing and file not found: {self.file_path}\")\n</code></pre>"},{"location":"api/entities/#gantry.entities.Instance.regenerate_uid","title":"<code>regenerate_uid()</code>","text":"<p>Generates a new, globally unique SOP Instance UID. Call this whenever pixel data is modified.</p> Source code in <code>gantry/entities.py</code> <pre><code>def regenerate_uid(self):\n    \"\"\"\n    Generates a new, globally unique SOP Instance UID.\n    Call this whenever pixel data is modified.\n    \"\"\"\n    # 1. Generate new UID using pydicom's generator (or your org root)\n    new_uid = generate_uid()\n\n    # 2. Update the Object Property\n    self.sop_instance_uid = new_uid\n\n    # 3. Update the DICOM Attribute Dictionary\n    self.set_attr(\"0008,0018\", new_uid)\n\n    # 4. Detach from physical file\n    # Since this object is now a \"new\" instance in memory, \n    # it no longer matches the file on disk.\n    self.file_path = None \n\n    from .logger import get_logger\n    get_logger().debug(f\"  -&gt; Identity regenerated: {new_uid}\")\n</code></pre>"},{"location":"api/entities/#gantry.entities.Instance.set_pixel_data","title":"<code>set_pixel_data(array)</code>","text":"<p>Sets the pixel array and automatically updates metadata tags (rows, cols, samples, frames, etc.) based on array shape. Handles unpacking of 2D/3D/4D arrays.</p> Source code in <code>gantry/entities.py</code> <pre><code>def set_pixel_data(self, array: np.ndarray):\n    \"\"\"\n    Sets the pixel array and automatically updates metadata tags\n    (rows, cols, samples, frames, etc.) based on array shape.\n    Handles unpacking of 2D/3D/4D arrays.\n    \"\"\"\n    self.pixel_array = array\n    shape = array.shape\n    ndim = len(shape)\n\n    # Defaults\n    samples = 1\n    frames = 1\n\n    if ndim == 1:\n         # Flattened array (e.g. from Sidecar loader)\n         # Attempt to reshape using existing metadata if available\n         try:\n             r = int(self.attributes.get(\"0028,0010\", 0))\n             c = int(self.attributes.get(\"0028,0011\", 0))\n             s = int(self.attributes.get(\"0028,0002\", 1))\n             f = int(self.attributes.get(\"0028,0008\", 1))\n\n             expected_size = r * c * s * f\n             if expected_size &gt; 0 and array.size &gt;= expected_size:\n                  # Truncate padding if present (DICOM alignment)\n                  if array.size &gt; expected_size:\n                      array = array[:expected_size]\n\n                  # Reshape logic\n                  if f &gt; 1:\n                      array = array.reshape((f, r, c, s)) if s &gt; 1 else array.reshape((f, r, c))\n                  elif s &gt; 1:\n                      array = array.reshape((r, c, s))\n                  else:\n                      array = array.reshape((r, c))\n                  self.pixel_array = array\n                  return # Done, attributes already match\n             elif expected_size == 0:\n                  # Metadata missing, treat as linear?\n                  pass\n\n         except:\n             pass\n\n         # Only raise if we couldn't resolve it\n         if len(array.shape) == 1: # Still 1D\n              rows, cols = 1, shape[0]\n\n    elif ndim == 2:\n        rows, cols = shape\n    elif ndim == 3:\n        if shape[-1] in [3, 4]:\n            rows, cols, samples = shape\n        else:\n            frames, rows, cols = shape\n    elif ndim == 4:\n        frames, rows, cols, samples = shape\n    else:\n        raise ValueError(f\"Unknown shape: {shape}\")\n\n    self.set_attr(\"0028,0010\", rows)\n    self.set_attr(\"0028,0011\", cols)\n    self.set_attr(\"0028,0002\", samples)\n    if frames &gt; 1: self.set_attr(\"0028,0008\", str(frames))\n    if samples &gt;= 3: \n        self.set_attr(\"0028,0004\", \"RGB\")\n        self.set_attr(\"0028,0006\", 0) # Force Interleaved (standard numpy)\n    else:\n        # Preserve existing PhotometricInterpretation (e.g. MONOCHROME1)\n        # Only set default if missing\n        if not self.attributes.get(\"0028,0004\"):\n            self.set_attr(\"0028,0004\", \"MONOCHROME2\")\n\n    # Ensure BitsAllocated matches array data type\n    # SidecarPixelLoader relies on this to determine uint8 vs uint16\n    bits = array.itemsize * 8\n    self.set_attr(\"0028,0100\", bits)\n\n    self._mod_count += 1\n</code></pre>"},{"location":"api/entities/#gantry.entities.Instance.unload_pixel_data","title":"<code>unload_pixel_data()</code>","text":"<p>Clears the cached pixel_array from memory to free resources. Only performs the clear if the data can be re-loaded (file_path or _pixel_loader exists). Returns True if unloaded, False if unsafe to unload.</p> Source code in <code>gantry/entities.py</code> <pre><code>def unload_pixel_data(self):\n    \"\"\"\n    Clears the cached pixel_array from memory to free resources.\n    Only performs the clear if the data can be re-loaded (file_path or _pixel_loader exists).\n    Returns True if unloaded, False if unsafe to unload.\n    \"\"\"\n    if self.pixel_array is None:\n        return True\n\n    if self.file_path or self._pixel_loader:\n        self.pixel_array = None\n        # print(f\"DEBUG: Unloaded pixels for {self.sop_instance_uid}\")\n        return True\n    else:\n        # Data is in memory only (e.g. modified but not saved)\n        print(f\"DEBUG: FAILED TO UNLOAD {self.sop_instance_uid} - No file path or loader!\")\n        return False\n</code></pre>"},{"location":"api/entities/#gantry.entities.Patient","title":"<code>Patient</code>  <code>dataclass</code>","text":"<p>Root of the object hierarchy. Groups Studies by Patient ID.</p> Source code in <code>gantry/entities.py</code> <pre><code>@dataclass(slots=True)\nclass Patient:\n    \"\"\"\n    Root of the object hierarchy. Groups Studies by Patient ID.\n    \"\"\"\n    patient_id: str\n    patient_name: str\n    studies: List[Study] = field(default_factory=list)\n    _dirty: bool = field(default=True, init=False)\n\n    def __post_init__(self):\n        self._dirty = True\n\n    def mark_clean(self):\n        self._dirty = False\n        for s in self.studies:\n            s.mark_clean()\n</code></pre>"},{"location":"api/entities/#gantry.entities.Series","title":"<code>Series</code>  <code>dataclass</code>","text":"<p>Groups Instances by Series Instance UID. Typically represents a single scan or reconstruction.</p> Source code in <code>gantry/entities.py</code> <pre><code>@dataclass(slots=True)\nclass Series:\n    \"\"\"\n    Groups Instances by Series Instance UID.\n    Typically represents a single scan or reconstruction.\n    \"\"\"\n    series_instance_uid: str\n    modality: str\n    series_number: int\n    equipment: Optional[Equipment] = None\n    instances: List[Instance] = field(default_factory=list)\n    _dirty: bool = field(default=True, init=False)\n\n    def __post_init__(self):\n        self._dirty = True\n\n    def mark_clean(self):\n        self._dirty = False\n        for i in self.instances:\n            i.mark_clean()\n</code></pre>"},{"location":"api/entities/#gantry.entities.Study","title":"<code>Study</code>  <code>dataclass</code>","text":"<p>Groups Series by Study Instance UID. Represents a single patient visit or examination.</p> Source code in <code>gantry/entities.py</code> <pre><code>@dataclass(slots=True)\nclass Study:\n    \"\"\"\n    Groups Series by Study Instance UID.\n    Represents a single patient visit or examination.\n    \"\"\"\n    study_instance_uid: str\n    study_date: Any\n    series: List[Series] = field(default_factory=list)\n    date_shifted: bool = False\n    study_time: Optional[str] = None\n    _dirty: bool = field(default=True, init=False)\n\n    def __post_init__(self):\n        self._dirty = True\n\n    def mark_clean(self):\n        self._dirty = False\n        for s in self.series:\n            s.mark_clean()\n</code></pre>"},{"location":"api/persistence/","title":"Persistence API","text":""},{"location":"api/persistence/#gantry.persistence","title":"<code>gantry.persistence</code>","text":""},{"location":"api/persistence/#gantry.persistence.SqliteStore","title":"<code>SqliteStore</code>","text":"<p>Handles persistence of the Object Graph to a SQLite database. Also manages the Audit Log.</p> Source code in <code>gantry/persistence.py</code> <pre><code>class SqliteStore:\n    \"\"\"\n    Handles persistence of the Object Graph to a SQLite database.\n    Also manages the Audit Log.\n    \"\"\"\n\n    SCHEMA = \"\"\"\n    CREATE TABLE IF NOT EXISTS patients (\n        id INTEGER PRIMARY KEY AUTOINCREMENT,\n        patient_id TEXT NOT NULL,\n        patient_name TEXT,\n        UNIQUE(patient_id)\n    );\n\n    CREATE TABLE IF NOT EXISTS studies (\n        id INTEGER PRIMARY KEY AUTOINCREMENT,\n        patient_id_fk INTEGER,\n        study_instance_uid TEXT NOT NULL,\n        study_date TEXT,\n        FOREIGN KEY(patient_id_fk) REFERENCES patients(id),\n        UNIQUE(study_instance_uid)\n    );\n\n    CREATE TABLE IF NOT EXISTS series (\n        id INTEGER PRIMARY KEY AUTOINCREMENT,\n        study_id_fk INTEGER,\n        series_instance_uid TEXT NOT NULL,\n        modality TEXT,\n        series_number INTEGER,\n        manufacturer TEXT,\n        model_name TEXT,\n        device_serial_number TEXT,\n        FOREIGN KEY(study_id_fk) REFERENCES studies(id),\n        UNIQUE(series_instance_uid)\n    );\n\n    CREATE TABLE IF NOT EXISTS instances (\n        id INTEGER PRIMARY KEY AUTOINCREMENT,\n        series_id_fk INTEGER,\n        sop_instance_uid TEXT NOT NULL,\n        sop_class_uid TEXT,\n        instance_number INTEGER,\n        file_path TEXT,\n        pixel_file_id INTEGER DEFAULT 0,\n        pixel_offset INTEGER,\n        pixel_length INTEGER,\n        pixel_hash TEXT,\n        compress_alg TEXT,\n        attributes_json TEXT, -- Core attributes (Horizontal)\n        FOREIGN KEY(series_id_fk) REFERENCES series(id),\n        UNIQUE(sop_instance_uid)\n    );\n\n    CREATE TABLE IF NOT EXISTS instance_attributes (\n        id INTEGER PRIMARY KEY AUTOINCREMENT,\n        instance_uid TEXT NOT NULL,\n        group_id TEXT NOT NULL,\n        element_id TEXT NOT NULL,\n        atom_index INTEGER DEFAULT 0,\n        value_rep TEXT,\n        value_text TEXT,\n        FOREIGN KEY(instance_uid) REFERENCES instances(sop_instance_uid) ON DELETE CASCADE,\n        UNIQUE(instance_uid, group_id, element_id, atom_index)\n    );\n\n    CREATE TABLE IF NOT EXISTS audit_log (\n        id INTEGER PRIMARY KEY AUTOINCREMENT,\n        timestamp TEXT,\n        action_type TEXT,\n        entity_uid TEXT,\n        details TEXT\n    );\n    CREATE TABLE IF NOT EXISTS phi_findings (\n        id INTEGER PRIMARY KEY AUTOINCREMENT,\n        timestamp TEXT,\n        entity_uid TEXT,\n        entity_type TEXT,\n        field_name TEXT,\n        value TEXT,\n        reason TEXT,\n        patient_id TEXT,\n        remediation_action TEXT,\n        remediation_value TEXT,\n        details_json TEXT\n    );\n\n    -- Indexing for Performance\n    CREATE INDEX IF NOT EXISTS idx_studies_patient_fk ON studies(patient_id_fk);\n    CREATE INDEX IF NOT EXISTS idx_series_study_fk ON series(study_id_fk);\n    CREATE INDEX IF NOT EXISTS idx_instances_series_fk ON instances(series_id_fk);\n    CREATE INDEX IF NOT EXISTS idx_audit_entity ON audit_log(entity_uid);\n    CREATE INDEX IF NOT EXISTS idx_findings_entity ON phi_findings(entity_uid);\n    CREATE INDEX IF NOT EXISTS idx_inst_attr_uid ON instance_attributes(instance_uid);\n    \"\"\"\n\n    def __init__(self, db_path: str):\n        self.db_path = db_path\n        self.logger = get_logger()\n        if db_path == \":memory:\":\n            # Use a temporary file for sidecar if DB is in-memory\n            # SidecarManager currently requires a file path (append-only logic)\n            # Create a temp file that persists until process exit (or manual cleanup)\n            # We use NamedTemporaryFile but close it so SidecarManager can open/lock it.\n            tf = tempfile.NamedTemporaryFile(suffix=\"_pixels.bin\", delete=False)\n            self.sidecar_path = tf.name\n            tf.close()\n            # Shared memory connection for :memory: database to persist across transactions\n            self._memory_conn = sqlite3.connect(\":memory:\", check_same_thread=False)\n            self._memory_conn.row_factory = sqlite3.Row\n            self._memory_lock = threading.Lock()\n        else:\n            self.sidecar_path = os.path.splitext(db_path)[0] + \"_pixels.bin\"\n            self._memory_conn = None\n            self._memory_lock = None\n\n        self.sidecar = SidecarManager(self.sidecar_path)\n        self._init_db()\n\n        # Async Audit Queue\n        self.audit_queue = queue.Queue()\n        self._stop_event = threading.Event()\n        self._audit_thread = threading.Thread(target=self._audit_worker, daemon=True, name=\"AuditWorker\")\n        self._audit_thread.start()\n\n    def __getstate__(self):\n        \"\"\"Exclude threading primitives from pickling.\"\"\"\n        state = self.__dict__.copy()\n        keys_to_remove = ['_memory_lock', '_memory_conn', 'audit_queue', '_stop_event', '_audit_thread']\n        for k in keys_to_remove:\n            state.pop(k, None)\n        return state\n\n    def __setstate__(self, state):\n        \"\"\"Recreate threading primitives on unpickling.\"\"\"\n        self.__dict__.update(state)\n\n        # Restore non-pickleable attributes\n        if self.db_path == \":memory:\":\n             self._memory_lock = threading.Lock()\n             self._memory_conn = None # Connection lost on pickle transfer\n        else:\n             self._memory_lock = None\n             self._memory_conn = None\n\n        self.audit_queue = queue.Queue()\n        self._stop_event = threading.Event()\n        self._audit_thread = threading.Thread(target=self._audit_worker, daemon=True, name=\"AuditWorker\")\n        self._audit_thread.start()\n\n    @contextlib.contextmanager\n    def _get_connection(self):\n        \"\"\"\n        Context manager for database connections.\n        Handles persistent connection for :memory: databases.\n        \"\"\"\n        if self._memory_conn:\n            # For in-memory DB, reuse the single connection.\n            # We must serialize access because sqlite3 connections are not thread-safe \n            # for concurrent writes even with check_same_thread=False.\n            with self._memory_lock:\n                try:\n                    # print(f\"DEBUG: Acquired lock. Yielding conn {id(self._memory_conn)}\") # Reduced spam\n                    yield self._memory_conn\n                    self._memory_conn.commit()\n                    # print(\"DEBUG: Commit successful\")\n                except Exception as e:\n                    print(f\"DEBUG: Rollback due to {e}\")\n                    self._memory_conn.rollback()\n                    raise\n        else:\n            # File-based DB: create fresh connection per transaction\n            conn = sqlite3.connect(self.db_path, timeout=900.0)\n            conn.execute(\"PRAGMA synchronous=NORMAL\")\n            conn.commit()\n            conn.row_factory = sqlite3.Row\n            try:\n                yield conn\n                conn.commit()\n            except Exception as e:\n                conn.rollback()\n                raise\n            finally:\n                conn.close()\n\n    def _init_db(self):\n        with self._get_connection() as conn:\n            conn.execute(\"PRAGMA journal_mode=WAL;\")\n            conn.executescript(self.SCHEMA)\n\n    def _create_pixel_loader(self, offset, length, alg, instance):\n        \"\"\"Helper to create a lazy pixel loader for the sidecar.\"\"\"\n        return SidecarPixelLoader(self.sidecar_path, offset, length, alg, instance)\n\n    def _audit_worker(self):\n        \"\"\"Background thread to batch write audit logs.\"\"\"\n        batch = []\n        while not self._stop_event.is_set():\n            try:\n                # Collect items with timeout\n                try:\n                    item = self.audit_queue.get(timeout=1.0)\n                    batch.append(item)\n\n                    # Drain queue up to limit\n                    while len(batch) &lt; 100:\n                        try:\n                            item = self.audit_queue.get_nowait()\n                            batch.append(item)\n                        except queue.Empty:\n                            break\n\n                except queue.Empty:\n                    pass\n\n                if batch:\n                    self.log_audit_batch(batch)\n                    batch = []\n\n            except Exception as e:\n                # Don't crash thread\n                self.logger.error(f\"Audit Worker Error: {e}\")\n\n        # Flush remaining\n        while not self.audit_queue.empty():\n            try:\n                batch.append(self.audit_queue.get_nowait())\n            except: break\n        if batch:\n            self.log_audit_batch(batch)\n\n    def stop(self):\n        \"\"\"Stops the audit worker and flushes queue.\"\"\"\n        self._stop_event.set()\n        if self._audit_thread.is_alive():\n            self._audit_thread.join(timeout=2.0)\n        self.flush_audit_queue()\n\n    def flush_audit_queue(self):\n        \"\"\"Manually processes all pending items in the audit queue.\"\"\"\n        batch = []\n        while not self.audit_queue.empty():\n            try:\n                batch.append(self.audit_queue.get_nowait())\n            except queue.Empty:\n                break\n\n        if batch:\n            self.log_audit_batch(batch)\n\n    def log_audit(self, action_type: str, entity_uid: str, details: str):\n        \"\"\"Records an action in the audit log (Async).\"\"\"\n        # Push to queue instead of writing directly\n        self.audit_queue.put((action_type, entity_uid, details))\n\n    def get_audit_summary(self) -&gt; Dict[str, int]:\n        \"\"\"\n        Returns an aggregated summary of actions from the audit log.\n        Returns:\n            Dict[str, int]: e.g., {'ANONYMIZE': 500, 'EXPORT': 500}\n        \"\"\"\n    def get_audit_summary(self) -&gt; Dict[str, int]:\n        \"\"\"\n        Returns an aggregated summary of actions from the audit log.\n        Stops and restarts the background audit worker to ensure consistency.\n        Returns:\n            Dict[str, int]: e.g., {'ANONYMIZE': 500, 'EXPORT': 500}\n        \"\"\"\n        # Stop worker to ensure all in-flight batches are written\n        # This joins the thread and flushes the queue.\n        self.stop()\n\n        try:\n            with self._get_connection() as conn:\n                cursor = conn.cursor()\n                try:\n                    cursor.execute(\"SELECT action_type, COUNT(*) FROM audit_log GROUP BY action_type\")\n                    rows = cursor.fetchall()\n                    return {row[0]: row[1] for row in rows}\n                except sqlite3.OperationalError:\n                    return {}\n        finally:\n            # Restart the worker\n            self._stop_event.clear()\n            self._audit_thread = threading.Thread(target=self._audit_worker, daemon=True, name=\"AuditWorker\")\n            self._audit_thread.start()\n\n\n    def get_audit_errors(self) -&gt; List[tuple]:\n        \"\"\"\n        Retrieves all audit logs with type ERROR or WARNING.\n        Returns:\n            List[tuple]: (timestamp, action_type, details)\n        \"\"\"\n        self.flush_audit_queue()\n        try:\n            with self._get_connection() as conn:\n                cursor = conn.cursor()\n                cursor.execute(\"\"\"\n                    SELECT timestamp, action_type, details \n                    FROM audit_log \n                    WHERE action_type IN ('ERROR', 'WARNING')\n                    ORDER BY timestamp ASC\n                \"\"\")\n                return cursor.fetchall()\n        except sqlite3.OperationalError:\n            return []\n\n    def check_unsafe_attributes(self) -&gt; List[tuple]:\n        \"\"\"\n        Scans for instances with potentially unsafe attributes (e.g., BurnedInAnnotation=\"YES\").\n        Returns:\n            List[tuple]: (sop_instance_uid, file_path, details)\n        \"\"\"\n        unsafe = []\n        try:\n            with self._get_connection() as conn:\n                cursor = conn.cursor()\n                # Naive text search in JSON. \n                # matches \"0028,0301\": \"YES\"\n                # We need to be careful about spacing in JSON serialization, but standard json.dumps usually does \": \"\n                # A safer broad check is %0028,0301%YES%\n                cursor.execute(\"\"\"\n                    SELECT sop_instance_uid, file_path \n                    FROM instances \n                    WHERE attributes_json LIKE '%\"0028,0301\": \"YES\"%'\n                \"\"\")\n                rows = cursor.fetchall()\n                for r in rows:\n                    unsafe.append((r[0], r[1], \"BurnedInAnnotation FLAGGED as YES\"))\n        except sqlite3.OperationalError:\n            pass\n        return unsafe\n\n    def log_audit_batch(self, entries: List[tuple]):\n        \"\"\"\n        Batch inserts audit logs. \n        entries: List of (action_type, entity_uid, details)\n        \"\"\"\n        if not entries: return\n\n        timestamp = datetime.now().isoformat()\n        # Prepare data with timestamp: (timestamp, action, uid, details)\n        data = [(timestamp, e[0], e[1], e[2]) for e in entries]\n\n        try:\n            with self._get_connection() as conn:\n                conn.executemany(\n                    \"INSERT INTO audit_log (timestamp, action_type, entity_uid, details) VALUES (?, ?, ?, ?)\",\n                    data\n                )\n                conn.commit()\n        except sqlite3.Error as e:\n            self.logger.error(f\"Failed to batch log audit: {e}\")\n\n    def load_all(self) -&gt; List[Patient]:\n        \"\"\"\n        Reconstructs the entire object graph from the database.\n        Returns a list of Patient objects.\n        \"\"\"\n        patients = []\n        if self.db_path != \":memory:\" and not os.path.exists(self.db_path):\n            return patients\n\n        try:\n            with self._get_connection() as conn:\n                # conn.row_factory = sqlite3.Row  &lt;-- Handled by _get_connection\n                cur = conn.cursor()\n\n                # Optimized: We could do joins, but for clarity/mapping let's do hierarchical fetch.\n                # Or fetch all and Stitch. Stitching in memory is faster for SQLite than N+1 queries.\n\n                # 1. Fetch AlL\n                p_rows = cur.execute(\"SELECT * FROM patients\").fetchall()\n                st_rows = cur.execute(\"SELECT * FROM studies\").fetchall()\n                se_rows = cur.execute(\"SELECT * FROM series\").fetchall()\n                i_rows = cur.execute(\"SELECT * FROM instances\").fetchall()\n\n\n\n                # 2. Build Maps\n                p_map = {}\n                for r in p_rows:\n                    p = Patient(r['patient_id'], r['patient_name'])\n                    p_map[r['id']] = p\n                    patients.append(p)\n\n                st_map = {}\n                for r in st_rows:\n                    st = Study(r['study_instance_uid'], r['study_date'])\n                    st_map[r['id']] = st\n                    if r['patient_id_fk'] in p_map:\n                        p_map[r['patient_id_fk']].studies.append(st)\n\n                se_map = {}\n                for r in se_rows:\n                    se = Series(r['series_instance_uid'], r['modality'], r['series_number'])\n                    if r['manufacturer'] or r['model_name']:\n                        se.equipment = Equipment(r['manufacturer'], r['model_name'], r['device_serial_number'])\n                    se_map[r['id']] = se\n                    if r['study_id_fk'] in st_map:\n                        st_map[r['study_id_fk']].series.append(se)\n\n                for r in i_rows:\n                    inst = Instance(\n                        r['sop_instance_uid'], \n                        r['sop_class_uid'], \n                        r['instance_number'], \n                        file_path=r['file_path']\n                    )\n\n                    # Wire up Sidecar Loader if present\n                    if r['pixel_offset'] is not None and r['pixel_length'] is not None:\n                         # Capture closure vars\n                         offset = r['pixel_offset']\n                         length = r['pixel_length']\n                         alg = r['compress_alg']\n\n                         # We need to reshape after loading. The dimensions are in attributes.\n                         # We can do this inside the lambda wrapper or a helper method.\n                         # But Instance.attributes aren't populated yet! \n                         # Wait, we populate attributes right after this.\n                         # So the lambda calls self.instance methods? No, lambda binds early.\n\n                         inst._pixel_loader = self._create_pixel_loader(r['pixel_offset'], r['pixel_length'], r['compress_alg'], inst)\n\n                    # Restore extra attributes\n                    if r['attributes_json']:\n                        try:\n                            attrs = json.loads(r['attributes_json'], object_hook=gantry_json_object_hook)\n                            self._deserialize_into(inst, attrs)\n                        except: \n                            pass # JSON error\n\n                    if r['series_id_fk'] in se_map:\n                        se_map[r['series_id_fk']].instances.append(inst)\n\n            self.logger.info(f\"Loaded {len(patients)} patients from {self.db_path}\")\n            # Mark all loaded data as clean so we don't save it back immediately\n            for p in patients:\n                p.mark_clean()\n            return patients\n\n        except sqlite3.Error as e:\n            print(f\"DEBUG: Failed to load from DB: {e}\")\n            self.logger.error(f\"Failed to load PDF from DB: {e}\")\n            import traceback\n            traceback.print_exc()\n            return []\n\n    def load_patient(self, patient_uid: str) -&gt; Optional[Patient]:\n        \"\"\"Loads a single patient and their graph from the DB by PatientID.\"\"\"\n        if self.db_path != \":memory:\" and not os.path.exists(self.db_path):\n            return None\n\n        try:\n             with self._get_connection() as conn:\n                # conn.row_factory = sqlite3.Row\n                cur = conn.cursor()\n\n                # Fetch Patient\n                p_row = cur.execute(\"SELECT * FROM patients WHERE patient_id = ?\", (patient_uid,)).fetchone()\n                if not p_row: return None\n\n                p = Patient(p_row['patient_id'], p_row['patient_name'])\n                p_pk = p_row['id']\n\n                # Fetch Studies\n                st_rows = cur.execute(\"SELECT * FROM studies WHERE patient_id_fk = ?\", (p_pk,)).fetchall()\n                for st_r in st_rows:\n                    st = Study(st_r['study_instance_uid'], st_r['study_date'])\n                    st_pk = st_r['id']\n\n                    # Fetch Series\n                    se_rows = cur.execute(\"SELECT * FROM series WHERE study_id_fk = ?\", (st_pk,)).fetchall()\n                    for se_r in se_rows:\n                        se = Series(se_r['series_instance_uid'], se_r['modality'], se_r['series_number'])\n                        if se_r['manufacturer'] or se_r['model_name']:\n                            se.equipment = Equipment(se_r['manufacturer'], se_r['model_name'], se_r['device_serial_number'])\n                        se_pk = se_r['id']\n\n                        # Fetch Instances\n                        i_rows = cur.execute(\"SELECT * FROM instances WHERE series_id_fk = ?\", (se_pk,)).fetchall()\n                        for r in i_rows:\n                            inst = Instance(\n                                r['sop_instance_uid'], \n                                r['sop_class_uid'], \n                                r['instance_number'], \n                                file_path=r['file_path']\n                            )\n                            # Wire up Sidecar (Copy-Paste logic from load_all, keep generic?)\n                            # ideally refactor _hydrate_instance but inline is fine for now\n                            if r['pixel_offset'] is not None and r['pixel_length'] is not None:\n                                offset, length, alg = r['pixel_offset'], r['pixel_length'], r['compress_alg']\n                                inst._pixel_loader = self._create_pixel_loader(r['pixel_offset'], r['pixel_length'], r['compress_alg'], inst)\n\n                            if r['attributes_json']:\n                                try:\n                                    attrs = json.loads(r['attributes_json'], object_hook=gantry_json_object_hook)\n                                    self._deserialize_into(inst, attrs)\n                                except: pass\n\n                            se.instances.append(inst)\n\n                        st.series.append(se)\n                    p.studies.append(st)\n\n                p.mark_clean()\n                return p\n        except sqlite3.Error as e:\n            self.logger.error(f\"Failed to load patient: {e}\")\n            return None\n\n    def _serialize_item(self, item: Instance) -&gt; Dict[str, Any]:\n        \"\"\"\n        Serializes a DicomItem (or Instance) to a dictionary, including attributes and sequences.\n        \"\"\"\n        data = item.attributes.copy()\n        if item.sequences:\n            seq_data = {}\n            for tag, seq in item.sequences.items():\n                items_list = []\n                for seq_item in seq.items:\n                    # Recursive call for sequence items (which are DicomItems)\n                    # We can reuse logic but need to handle DicomItem vs Instance\n                    # Instance specific fields are handled by caller for the root, \n                    # but for seq items they are just DicomItems.\n                    items_list.append(self._serialize_dicom_item(seq_item))\n                seq_data[tag] = items_list\n            data['__sequences__'] = seq_data\n        return data\n\n    def _serialize_dicom_item(self, item) -&gt; Dict[str, Any]:\n        \"\"\"Helper for recursive serialization of generic DicomItems.\"\"\"\n        data = item.attributes.copy()\n        if item.sequences:\n            seq_data = {}\n            for tag, seq in item.sequences.items():\n                items_list = [self._serialize_dicom_item(i) for i in seq.items]\n                seq_data[tag] = items_list\n            data['__sequences__'] = seq_data\n        return data\n\n    def _deserialize_into(self, target_item, data: Dict[str, Any]):\n        \"\"\"\n        Populates target_item with attributes and sequences from data dict.\n        \"\"\"\n        sequences_data = data.pop('__sequences__', None)\n\n        # 1. Attributes\n        target_item.attributes.update(data)\n\n        # 2. Sequences\n        if sequences_data:\n            from .entities import DicomItem\n            for tag, items_list in sequences_data.items():\n                for item_data in items_list:\n                    new_item = DicomItem()\n                    self._deserialize_into(new_item, item_data)\n                    target_item.add_sequence_item(tag, new_item)\n\n    def save_vertical_attributes(self, instance_uid: str, attributes: Dict[Tuple[str, str], Any], conn: sqlite3.Connection = None):\n        \"\"\"\n        Persists extended attributes to the vertical `instance_attributes` table.\n        attributes key format: (group_hex, element_hex) e.g. (\"0010\", \"0010\")\n        Use UPSERT semantics.\n        Optionally accepts an existing connection to share transaction.\n        \"\"\"\n        if not attributes: return\n\n        data_rows = []\n        for (grp, elem), val in attributes.items():\n            vr = \"UN\" # Todo: Pass VR from caller\n            # Check for VM &gt; 1\n            if isinstance(val, list):\n                for idx, atom in enumerate(val):\n                    data_rows.append((instance_uid, grp, elem, idx, vr, str(atom)))\n            else:\n                 data_rows.append((instance_uid, grp, elem, 0, vr, str(val)))\n\n        if not data_rows: return\n\n        try:\n            from contextlib import nullcontext\n            # If conn is passed, use it (and don't close it/commit it here, leave to caller).\n            # If not, create new context (which commits/closes).\n            ctx = self._get_connection() if conn is None else nullcontext(conn)\n\n            with ctx as db:\n                # 1. OPTIMIZATION: Delete existing for these keys first? \n                # Or UPSERT. \n                # \"test_vertical_update_serialization\" requires correctness.\n                # UPSERT based on unique index (uid, grp, elem, atom) works.\n                # But if list shrinks (VM 3 -&gt; VM 1), UPSERT leaves atoms 2,3.\n                # So we MUST DELETE by (uid, grp, elem) before inserting new set for that tag.\n\n                # We can do this in transaction.\n                keys_to_clear = list(attributes.keys())\n                # Batch delete?\n                # \"DELETE FROM instance_attributes WHERE instance_uid=? AND group_id=? AND element_id=?\\\"\n                del_params = [(instance_uid, k[0], k[1]) for k in keys_to_clear]\n                db.executemany(\n                    \"DELETE FROM instance_attributes WHERE instance_uid=? AND group_id=? AND element_id=?\", \n                    del_params\n                )\n\n                db.executemany(\"\"\"\n                    INSERT INTO instance_attributes (instance_uid, group_id, element_id, atom_index, value_rep, value_text)\n                    VALUES (?, ?, ?, ?, ?, ?)\n                \"\"\", data_rows)\n\n        except sqlite3.Error as e:\n            self.logger.error(f\"Failed to save vertical attributes for {instance_uid}: {e}\")\n            raise e\n\n    def load_vertical_attributes(self, instance_uid: str) -&gt; Dict[Tuple[str, str], Any]:\n        \"\"\"\n        Loads extended attributes from vertical table.\n        Returns dict: {(grp, elem): value_or_list}\n        \"\"\"\n        results = {}\n        try:\n            with self._get_connection() as conn:\n                rows = conn.execute(\"\"\"\n                    SELECT group_id, element_id, atom_index, value_text \n                    FROM instance_attributes \n                    WHERE instance_uid=? \n                    ORDER BY group_id, element_id, atom_index\n                \"\"\", (instance_uid,)).fetchall()\n\n                if not rows: return {}\n\n                # Reassemble\n                curr_key = None\n                collect = []\n\n                for r in rows:\n                    key = (r['group_id'], r['element_id'])\n                    val = r['value_text'] # Type conversion? Strings for now.\n\n                    if key != curr_key:\n                        # Flush previous\n                        if curr_key:\n                            results[curr_key] = collect if len(collect) &gt; 1 else collect[0]\n                        curr_key = key\n                        collect = [val]\n                    else:\n                        collect.append(val)\n\n                # Flush last\n                if curr_key:\n                    results[curr_key] = collect if len(collect) &gt; 1 else collect[0]\n\n            return results\n        except sqlite3.Error as e:\n            self.logger.error(f\"Failed to load vertical attributes for {instance_uid}: {e}\")\n            return {}\n\n    def persist_pixel_data(self, instance: Instance):\n        \"\"\"\n        Immediately persists pixel data to the sidecar to allow memory offloading.\n        Does NOT update the full instance record in the main DB (attributes/json), \n        only the pixel linkage. \n        \"\"\"\n        if instance.pixel_array is None:\n            return\n\n        try:\n            # 1. Write to Sidecar\n            # Pass array directly to avoid .tobytes() Memory spike (Zero-Copy 500MB save)\n            b_data = instance.pixel_array\n            # Determine suitable compression? Defaulting to zlib for swap.\n            # Ideally we respect original or config, but for swap zlib is safe/fast enough.\n            c_alg = 'zlib' \n\n            offset, length = self.sidecar.write_frame(b_data, c_alg)\n\n            # Hash Update (CRITICAL for Integrity Checks)\n            import hashlib\n            p_hash = hashlib.sha256(b_data).hexdigest()\n            instance._pixel_hash = p_hash\n\n            # 2. Update Instance Loader\n            # This allows instance.unload_pixel_data() to work safely\n            instance._pixel_loader = self._create_pixel_loader(offset, length, c_alg, instance)\n\n            # 3. Optional: Persist the linkage to DB immediately?\n            # It's safer if we do, so if we crash, we know where the pixels are.\n            # However, if we don't save the attributes/UID changes, the DB is out of sync anyway.\n            # But the primary goal here is MEMORY MANAGEMENT.\n            # So updating the object state in memory (step 2) is sufficient for unload_pixel_data() to return True.\n            # The final session.save() will record the new offset/length into the DB instances table.\n\n        except Exception as e:\n            self.logger.error(f\"Failed to persist pixel swap for {instance.sop_instance_uid}: {e}\")\n            raise e\n\n    def save_all(self, patients: List[Patient]):\n        \"\"\"\n        Persists the current state incrementally.\n        Strategy: UPSERT dirty items.\n        \"\"\"\n        self.logger.info(f\"Saving {len(patients)} patients to {self.db_path} (Incremental)...\")\n\n        pixel_bytes_written = 0\n        pixel_frames_written = 0\n        sidecar_manager = self.sidecar\n\n        try:\n            with self._get_connection() as conn:\n                cur = conn.cursor()\n\n                # Check for schema compatibility (simple check)\n                try:\n                    # We rely on UNIQUE constraints for UPSERT. \n                    # If older DB without constraints, we might fail or duplicate.\n                    pass \n                except: pass\n\n                # Counts for reporting\n                saved_p, saved_st, saved_se, saved_i = 0, 0, 0, 0\n\n                for p in patients:\n                    # Patient Level (Always Check Dirty)\n                    if getattr(p, '_dirty', True):\n                        cur.execute(\"\"\"\n                            INSERT INTO patients (patient_id, patient_name) VALUES (?, ?)\n                            ON CONFLICT(patient_id) DO UPDATE SET patient_name=excluded.patient_name\n                        \"\"\", (p.patient_id, p.patient_name))\n                        saved_p += 1\n\n                    # We need the PK for children\n                    # Since we might have just updated or it might exist, we select it.\n                    # Optimization: Cache PKs? For now, fetch is safe.\n                    p_pk_row = cur.execute(\"SELECT id FROM patients WHERE patient_id=?\", (p.patient_id,)).fetchone()\n                    if not p_pk_row: continue # Should not happen after Insert\n                    p_pk = p_pk_row[0]\n\n                    for st in p.studies:\n                        if getattr(st, '_dirty', True):\n                            # FIX: Convert date objects to string to avoid Python 3.12+ DeprecationWarning for default adapter\n                            s_date = st.study_date\n                            if hasattr(s_date, \"isoformat\"):\n                                s_date = s_date.isoformat()\n                            elif s_date is not None:\n                                s_date = str(s_date)\n\n                            cur.execute(\"\"\"\n                                INSERT INTO studies (patient_id_fk, study_instance_uid, study_date) VALUES (?, ?, ?)\n                                ON CONFLICT(study_instance_uid) DO UPDATE SET \n                                    study_date=excluded.study_date,\n                                    patient_id_fk=excluded.patient_id_fk\n                            \"\"\", (p_pk, st.study_instance_uid, s_date))\n                            saved_st += 1\n\n                        st_pk_row = cur.execute(\"SELECT id FROM studies WHERE study_instance_uid=?\", (st.study_instance_uid,)).fetchone()\n                        if not st_pk_row: continue\n                        st_pk = st_pk_row[0]\n\n                        for se in st.series:\n                            if getattr(se, '_dirty', True):\n                                man = se.equipment.manufacturer if se.equipment else \"\"\n                                mod = se.equipment.model_name if se.equipment else \"\"\n                                sn = se.equipment.device_serial_number if se.equipment else \"\"\n\n                                cur.execute(\"\"\"\n                                    INSERT INTO series (study_id_fk, series_instance_uid, modality, series_number, manufacturer, model_name, device_serial_number)\n                                    VALUES (?, ?, ?, ?, ?, ?, ?)\n                                    ON CONFLICT(series_instance_uid) DO UPDATE SET \n                                        modality=excluded.modality,\n                                        series_number=excluded.series_number,\n                                        manufacturer=excluded.manufacturer,\n                                        model_name=excluded.model_name,\n                                        device_serial_number=excluded.device_serial_number,\n                                        study_id_fk=excluded.study_id_fk\n                                \"\"\", (st_pk, se.series_instance_uid, se.modality, se.series_number, man, mod, sn))\n                                saved_se += 1\n\n                            se_pk_row = cur.execute(\"SELECT id FROM series WHERE series_instance_uid=?\", (se.series_instance_uid,)).fetchone()\n                            if not se_pk_row: continue\n                            se_pk = se_pk_row[0]\n\n                            # --- Deletion Handling (Diff DB vs Memory) ---\n                            # Only perform if we suspect deletions or periodically? \n                            # Plan says: Implement Diff Logic.\n                            # Optimization: If series is NOT dirty, can we assume no deletions?\n                            # Not necessarily. Removing an item doesn't always mark Series dirty unless we hook \"remove\".\n                            # But DicomItem doesn't track removals from list automatically.\n                            # So we must check.\n\n                            db_uids_rows = cur.execute(\"SELECT sop_instance_uid FROM instances WHERE series_id_fk=?\", (se_pk,)).fetchall()\n                            db_uids = {r[0] for r in db_uids_rows}\n                            mem_uids = {i.sop_instance_uid for i in se.instances}\n\n                            to_delete = db_uids - mem_uids\n                            if to_delete:\n                                cur.executemany(\"DELETE FROM instances WHERE sop_instance_uid=?\", [(u,) for u in to_delete])\n                                saved_i += 0 # Or count negative?\n                                # self.logger.debug(f\"Deleted {len(to_delete)} instances from Series {se.series_instance_uid}\")\n\n                            # --- Upsert Dirty ---\n                            dirty_items = []\n                            for i in se.instances:\n                                if getattr(i, '_dirty', True):\n                                    # Capture version if available (robustness against race)\n                                    ver = getattr(i, '_mod_count', 0)\n                                    dirty_items.append((i, ver))\n\n                            if dirty_items:\n                                i_batch = []\n                                vert_updates = [] # Defer vertical updates to satisfy foreign key\n                                for inst, ver in dirty_items:\n                                    full_data = self._serialize_item(inst)\n\n                                    # Split Core vs Vertical (Private Tags -&gt; Vertical Table)\n                                    core_data = {}\n                                    vert_data = {}\n\n                                    for key, val in full_data.items():\n                                        if key == \"__sequences__\":\n                                             core_data[key] = val # Keep sequences in Core JSON for now\n                                             continue\n\n                                        # key is \"GGGG,EEEE\" hex string\n                                        try:\n                                            group = int(key.split(',')[0], 16)\n                                            # Odd Group = Private Tag (usually)\n                                            # Skip Vertical for BYTES (cant be stored as TEXT easily, keep in JSON)\n                                            is_private = (group % 2 != 0) and not isinstance(val, bytes)\n\n                                            if is_private:\n                                                 # Tuple key for vertical method: (grp, elem)\n                                                 k_tuple = tuple(key.split(','))\n                                                 vert_data[k_tuple] = val\n                                            else:\n                                                 core_data[key] = val\n                                        except:\n                                            core_data[key] = val\n\n                                    # Queue Vertical (Saved after Instance Insert)\n                                    if vert_data:\n                                        vert_updates.append((inst.sop_instance_uid, vert_data))\n\n                                    # Serialize Core\n                                    attrs_json = json.dumps(core_data, cls=GantryJSONEncoder)\n\n                                    p_offset, p_length, p_alg, p_hash = None, None, None, None\n\n                                    if inst.pixel_array is not None:\n                                         b_data = inst.pixel_array.tobytes()\n                                         c_alg = 'zlib'\n                                         # Compute Hash\n                                         import hashlib\n                                         p_hash = hashlib.sha256(b_data).hexdigest()\n\n                                         off, leng = sidecar_manager.write_frame(b_data, c_alg)\n                                         p_offset, p_length, p_alg = off, leng, c_alg\n                                         pixel_bytes_written += leng\n                                         pixel_frames_written += 1\n\n                                         # Update loader so we can unload safely later\n                                         inst._pixel_loader = self._create_pixel_loader(off, leng, c_alg, inst)\n                                         inst._pixel_hash = p_hash # Cache on instance\n\n                                    elif isinstance(inst._pixel_loader, SidecarPixelLoader):\n                                         # Already persisted (swapped), preserve metadata\n                                         p_offset = inst._pixel_loader.offset\n                                         p_length = inst._pixel_loader.length\n                                         p_alg = inst._pixel_loader.alg\n                                         p_hash = getattr(inst, '_pixel_hash', None)\n                                    else:\n                                         pass\n\n                                    i_batch.append((\n                                        se_pk, \n                                        inst.sop_instance_uid, \n                                        inst.sop_class_uid, \n                                        inst.instance_number, \n                                        inst.file_path, \n                                        p_offset, \n                                        p_length, \n                                        p_hash,\n                                        p_alg, \n                                        attrs_json\n                                    ))\n\n                                cur.executemany(\"\"\"\n                                    INSERT INTO instances (series_id_fk, sop_instance_uid, sop_class_uid, instance_number, file_path, \n                                                           pixel_offset, pixel_length, pixel_hash, compress_alg, attributes_json)\n                                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n                                    ON CONFLICT(sop_instance_uid) DO UPDATE SET\n                                        series_id_fk=excluded.series_id_fk,\n                                        sop_class_uid=excluded.sop_class_uid,\n                                        instance_number=excluded.instance_number,\n                                        file_path=excluded.file_path,\n                                        attributes_json=excluded.attributes_json,\n                                        pixel_offset=COALESCE(excluded.pixel_offset, instances.pixel_offset),\n                                        pixel_length=COALESCE(excluded.pixel_length, instances.pixel_length),\n                                        pixel_hash=COALESCE(excluded.pixel_hash, instances.pixel_hash),\n                                        compress_alg=COALESCE(excluded.compress_alg, instances.compress_alg)\n                                \"\"\", i_batch)\n\n                                # Process Deferred Vertical Updates (Now that Instances exist)\n                                if vert_updates:\n                                    # self.logger.debug(f\"Saving vertical attributes for {len(vert_updates)} instances\")\n                                    for uid, v_data in vert_updates:\n                                        self.save_vertical_attributes(uid, v_data, conn=conn)\n\n                                saved_i += len(dirty_items)\n\n                                # Mark saved with version (deferred until commit success? \n                                # No, we can attach to list and do it post-commit)\n                                # But we're inside loops. \n                                # Creating a cleanup list:\n                                # (We can store dirty_items in a larger list to clean up post-commit)\n                                # For now, let's mark clean *assuming* commit will succeed.\n                                # If commit fails, we rollback, but objects remain \"clean\" in memory?\n                                # That is a risk. We should do it post-commit.\n                                # But scope is tricky. \n                                # Let's mark clean here but using version. \n                                # If transaction rolls back, DB is old, but memory has _saved_mod_count advanced?\n                                # That means next save won't save it. BAD.\n                                # We must hold off.\n\n                                # Since we commit once at the end:\n                                # We need to collect ALL dirty items and their versions.\n                                # That is expensive memory-wise for massive sets.\n                                # But necessary for correctness.\n                                # Compromise: we iterate again. \n                                # Wait, \"Iterate again\" in 'mark clean' loop below.\n                                # We can't know \"ver\" then.\n\n                                # Let's just update them here. If commit fails, the Exception propagates.\n                                # Use a try/except block around the whole `save_all`? Yes.\n                                # But `_saved_mod_count` is in memory.\n                                # If we update it, and `save_all` crashes, we can't easily undo it.\n                                # BUT `save_all` crashing usually kills the process or stops persistence.\n                                # So `eventual consistency` implies retrying.\n                                # If we marked it saved but it didn't save, we have data loss.\n\n                                # Correct way: List of callbacks?\n                                # Or just:\n                                for inst, ver in dirty_items:\n                                     if hasattr(inst, 'mark_saved'):\n                                          inst.mark_saved(ver)\n                                     else:\n                                          inst._dirty = False\n\n                conn.commit()\n\n                # Post-Commit: \n                # We already marked items as saved/clean incrementally using naive-commit assumption.\n                # If transaction failed, those items are marked clean in memory but not in DB -&gt; Inconsistency.\n                # However, re-implementing rollback for memory objects is out of scope.\n                # The versioning fixes the \"Overwrite valid change\" race, which is the user's issue.\n                pass\n\n                # Restore Logging Logic\n                if saved_p + saved_i &gt; 0:\n                     msg = f\"Save (Inc) complete. P:{saved_p} St:{saved_st} Se:{saved_se} I:{saved_i}.\"\n                     if pixel_frames_written &gt; 0:\n                         mb = pixel_bytes_written / (1024*1024)\n                         msg += f\" Sidecar: {pixel_frames_written} frames ({mb:.2f} MB).\"\n                     self.logger.info(msg)\n\n        except Exception as e:\n            self.logger.error(f\"Save failed: {e}\")\n            if hasattr(conn, \"rollback\"): conn.rollback()\n            raise\n\n    def get_total_instances(self) -&gt; int:\n        \"\"\"Returns the total number of instances currently persisted.\"\"\"\n        try:\n             with self._get_connection() as conn:\n                cur = conn.cursor()\n                row = cur.execute(\"SELECT COUNT(*) FROM instances\").fetchone()\n                return row[0] if row else 0\n        except sqlite3.Error as e:\n            self.logger.error(f\"Failed to count instances: {e}\")\n            return 0\n\n    def get_flattened_instances(self, patient_ids: List[str] = None, instance_uids: List[str] = None):\n        \"\"\"\n        Yields a flat dictionary for every instance in the DB (or filtered by patient_ids/instance_uids).\n        Ideal for streaming exports or analysis without loading the entire graph into RAM.\n        \"\"\"\n        # We use a managed connection that stays open during iteration\n        with self._get_connection() as conn:\n            # conn.row_factory = sqlite3.Row\n            cur = conn.cursor()\n\n            query = \"\"\"\n                SELECT \n                    p.patient_id, p.patient_name,\n                    st.study_instance_uid, st.study_date,\n                    s.series_instance_uid, s.modality, s.series_number, s.manufacturer, s.model_name, s.device_serial_number,\n                    i.sop_instance_uid, i.sop_class_uid, i.instance_number, i.file_path, \n                    i.pixel_offset, i.pixel_length, i.compress_alg, i.attributes_json\n                FROM instances i\n                JOIN series s ON i.series_id_fk = s.id\n                JOIN studies st ON s.study_id_fk = st.id\n                JOIN patients p ON st.patient_id_fk = p.id\n            \"\"\"\n\n            conditions = []\n            params = []\n\n            if patient_ids:\n                placeholders = \",\".join(\"?\" for _ in patient_ids)\n                conditions.append(f\"p.patient_id IN ({placeholders})\")\n                params.extend(patient_ids)\n\n            if instance_uids:\n                placeholders = \",\".join(\"?\" for _ in instance_uids)\n                conditions.append(f\"i.sop_instance_uid IN ({placeholders})\")\n                params.extend(instance_uids)\n\n            if conditions:\n                query += \" WHERE \" + \" AND \".join(conditions)\n\n            # Execute generator\n            cursor = cur.execute(query, params)\n\n            # We can map columns to names\n            cols = [desc[0] for desc in cursor.description]\n\n            for row in cursor:\n                yield dict(zip(cols, row))\n\n\n    def update_attributes(self, instances: List[Patient]):\n        \"\"\"\n        Efficiently updates the attributes_json for a list of instances.\n        \"\"\"\n        if not instances:\n            return\n\n        self.logger.info(f\"Updating attributes for {len(instances)} instances...\")\n        try:\n            with self._get_connection() as conn:\n                cur = conn.cursor()\n\n                # Pre-calculate data for executemany\n                data = []\n                for inst in instances:\n                    # Serialize attributes AND sequences\n                    full_data = self._serialize_item(inst)\n                    attrs_json = json.dumps(full_data, cls=GantryJSONEncoder)\n                    data.append((attrs_json, inst.sop_instance_uid))\n\n                cur.executemany(\"\"\"\n                    UPDATE instances \n                    SET attributes_json = ? \n                    WHERE sop_instance_uid = ?\n                \"\"\", data)\n\n                conn.commit()\n                self.logger.info(\"Update complete.\")\n\n        except sqlite3.Error as e:\n            self.logger.error(f\"Failed to update attributes: {e}\")\n\n    def save_findings(self, findings: List[PhiFinding]):\n        \"\"\"Persists PHI findings to the database.\"\"\"\n        timestamp = datetime.now().isoformat()\n\n        if not findings:\n            return\n\n        self.logger.info(f\"Saving {len(findings)} PHI findings...\")\n\n        try:\n            with self._get_connection() as conn:\n                cur = conn.cursor()\n\n                # Prepare Data Generator for Batch Insert (Memory Efficient)\n                def findings_generator():\n                    for f in findings:\n                        rem_action = None\n                        rem_value = None\n                        if f.remediation_proposal:\n                            rem_action = f.remediation_proposal.action_type\n                            rem_value = str(f.remediation_proposal.new_value)\n\n                        yield (\n                            timestamp, \n                            f.entity_uid, \n                            f.entity_type, \n                            f.field_name, \n                            str(f.value), \n                            f.reason, \n                            f.patient_id, \n                            rem_action, \n                            rem_value, \n                            \"{}\"\n                        )\n\n                cur.executemany(\"\"\"\n                    INSERT INTO phi_findings \n                    (timestamp, entity_uid, entity_type, field_name, value, reason, patient_id, remediation_action, remediation_value, details_json) \n                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n                \"\"\", findings_generator())\n\n                conn.commit()\n                self.logger.info(\"Findings saved.\")\n\n        except sqlite3.Error as e:\n            self.logger.error(f\"Failed to save findings: {e}\")\n\n    def load_findings(self) -&gt; List[PhiFinding]:\n        \"\"\"Loads all findings from the database.\"\"\"\n        findings = []\n        if self.db_path != \":memory:\" and not os.path.exists(self.db_path):\n            return findings\n\n        try:\n             with self._get_connection() as conn:\n                # conn.row_factory = sqlite3.Row\n                cur = conn.cursor()\n                # Check if table exists (backward compatibility for old DBs if init didnt run on them)\n                # But _init_db runs on __init__, so schema should be there.\n\n                rows = cur.execute(\"SELECT * FROM phi_findings ORDER BY id\").fetchall()\n\n                for r in rows:\n                    if r['remediation_action']:\n                        prop = PhiRemediation(r['remediation_action'], r['field_name'], r['remediation_value'], None) \n                    else: \n                        prop = None\n\n                    f = PhiFinding(\n                        entity_uid=r['entity_uid'],\n                        entity_type=r['entity_type'],\n                        field_name=r['field_name'],\n                        value=r['value'],\n                        reason=r['reason'],\n                        patient_id=r['patient_id'],\n                        remediation_proposal=prop\n                    )\n                    findings.append(f)\n\n        except sqlite3.Error as e:\n            self.logger.error(f\"Failed to load findings: {e}\")\n\n        return findings\n</code></pre>"},{"location":"api/persistence/#gantry.persistence.SqliteStore.__getstate__","title":"<code>__getstate__()</code>","text":"<p>Exclude threading primitives from pickling.</p> Source code in <code>gantry/persistence.py</code> <pre><code>def __getstate__(self):\n    \"\"\"Exclude threading primitives from pickling.\"\"\"\n    state = self.__dict__.copy()\n    keys_to_remove = ['_memory_lock', '_memory_conn', 'audit_queue', '_stop_event', '_audit_thread']\n    for k in keys_to_remove:\n        state.pop(k, None)\n    return state\n</code></pre>"},{"location":"api/persistence/#gantry.persistence.SqliteStore.__setstate__","title":"<code>__setstate__(state)</code>","text":"<p>Recreate threading primitives on unpickling.</p> Source code in <code>gantry/persistence.py</code> <pre><code>def __setstate__(self, state):\n    \"\"\"Recreate threading primitives on unpickling.\"\"\"\n    self.__dict__.update(state)\n\n    # Restore non-pickleable attributes\n    if self.db_path == \":memory:\":\n         self._memory_lock = threading.Lock()\n         self._memory_conn = None # Connection lost on pickle transfer\n    else:\n         self._memory_lock = None\n         self._memory_conn = None\n\n    self.audit_queue = queue.Queue()\n    self._stop_event = threading.Event()\n    self._audit_thread = threading.Thread(target=self._audit_worker, daemon=True, name=\"AuditWorker\")\n    self._audit_thread.start()\n</code></pre>"},{"location":"api/persistence/#gantry.persistence.SqliteStore.check_unsafe_attributes","title":"<code>check_unsafe_attributes()</code>","text":"<p>Scans for instances with potentially unsafe attributes (e.g., BurnedInAnnotation=\"YES\"). Returns:     List[tuple]: (sop_instance_uid, file_path, details)</p> Source code in <code>gantry/persistence.py</code> <pre><code>def check_unsafe_attributes(self) -&gt; List[tuple]:\n    \"\"\"\n    Scans for instances with potentially unsafe attributes (e.g., BurnedInAnnotation=\"YES\").\n    Returns:\n        List[tuple]: (sop_instance_uid, file_path, details)\n    \"\"\"\n    unsafe = []\n    try:\n        with self._get_connection() as conn:\n            cursor = conn.cursor()\n            # Naive text search in JSON. \n            # matches \"0028,0301\": \"YES\"\n            # We need to be careful about spacing in JSON serialization, but standard json.dumps usually does \": \"\n            # A safer broad check is %0028,0301%YES%\n            cursor.execute(\"\"\"\n                SELECT sop_instance_uid, file_path \n                FROM instances \n                WHERE attributes_json LIKE '%\"0028,0301\": \"YES\"%'\n            \"\"\")\n            rows = cursor.fetchall()\n            for r in rows:\n                unsafe.append((r[0], r[1], \"BurnedInAnnotation FLAGGED as YES\"))\n    except sqlite3.OperationalError:\n        pass\n    return unsafe\n</code></pre>"},{"location":"api/persistence/#gantry.persistence.SqliteStore.flush_audit_queue","title":"<code>flush_audit_queue()</code>","text":"<p>Manually processes all pending items in the audit queue.</p> Source code in <code>gantry/persistence.py</code> <pre><code>def flush_audit_queue(self):\n    \"\"\"Manually processes all pending items in the audit queue.\"\"\"\n    batch = []\n    while not self.audit_queue.empty():\n        try:\n            batch.append(self.audit_queue.get_nowait())\n        except queue.Empty:\n            break\n\n    if batch:\n        self.log_audit_batch(batch)\n</code></pre>"},{"location":"api/persistence/#gantry.persistence.SqliteStore.get_audit_errors","title":"<code>get_audit_errors()</code>","text":"<p>Retrieves all audit logs with type ERROR or WARNING. Returns:     List[tuple]: (timestamp, action_type, details)</p> Source code in <code>gantry/persistence.py</code> <pre><code>def get_audit_errors(self) -&gt; List[tuple]:\n    \"\"\"\n    Retrieves all audit logs with type ERROR or WARNING.\n    Returns:\n        List[tuple]: (timestamp, action_type, details)\n    \"\"\"\n    self.flush_audit_queue()\n    try:\n        with self._get_connection() as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"\"\"\n                SELECT timestamp, action_type, details \n                FROM audit_log \n                WHERE action_type IN ('ERROR', 'WARNING')\n                ORDER BY timestamp ASC\n            \"\"\")\n            return cursor.fetchall()\n    except sqlite3.OperationalError:\n        return []\n</code></pre>"},{"location":"api/persistence/#gantry.persistence.SqliteStore.get_audit_summary","title":"<code>get_audit_summary()</code>","text":"<p>Returns an aggregated summary of actions from the audit log. Stops and restarts the background audit worker to ensure consistency. Returns:     Dict[str, int]: e.g., {'ANONYMIZE': 500, 'EXPORT': 500}</p> Source code in <code>gantry/persistence.py</code> <pre><code>def get_audit_summary(self) -&gt; Dict[str, int]:\n    \"\"\"\n    Returns an aggregated summary of actions from the audit log.\n    Stops and restarts the background audit worker to ensure consistency.\n    Returns:\n        Dict[str, int]: e.g., {'ANONYMIZE': 500, 'EXPORT': 500}\n    \"\"\"\n    # Stop worker to ensure all in-flight batches are written\n    # This joins the thread and flushes the queue.\n    self.stop()\n\n    try:\n        with self._get_connection() as conn:\n            cursor = conn.cursor()\n            try:\n                cursor.execute(\"SELECT action_type, COUNT(*) FROM audit_log GROUP BY action_type\")\n                rows = cursor.fetchall()\n                return {row[0]: row[1] for row in rows}\n            except sqlite3.OperationalError:\n                return {}\n    finally:\n        # Restart the worker\n        self._stop_event.clear()\n        self._audit_thread = threading.Thread(target=self._audit_worker, daemon=True, name=\"AuditWorker\")\n        self._audit_thread.start()\n</code></pre>"},{"location":"api/persistence/#gantry.persistence.SqliteStore.get_flattened_instances","title":"<code>get_flattened_instances(patient_ids=None, instance_uids=None)</code>","text":"<p>Yields a flat dictionary for every instance in the DB (or filtered by patient_ids/instance_uids). Ideal for streaming exports or analysis without loading the entire graph into RAM.</p> Source code in <code>gantry/persistence.py</code> <pre><code>def get_flattened_instances(self, patient_ids: List[str] = None, instance_uids: List[str] = None):\n    \"\"\"\n    Yields a flat dictionary for every instance in the DB (or filtered by patient_ids/instance_uids).\n    Ideal for streaming exports or analysis without loading the entire graph into RAM.\n    \"\"\"\n    # We use a managed connection that stays open during iteration\n    with self._get_connection() as conn:\n        # conn.row_factory = sqlite3.Row\n        cur = conn.cursor()\n\n        query = \"\"\"\n            SELECT \n                p.patient_id, p.patient_name,\n                st.study_instance_uid, st.study_date,\n                s.series_instance_uid, s.modality, s.series_number, s.manufacturer, s.model_name, s.device_serial_number,\n                i.sop_instance_uid, i.sop_class_uid, i.instance_number, i.file_path, \n                i.pixel_offset, i.pixel_length, i.compress_alg, i.attributes_json\n            FROM instances i\n            JOIN series s ON i.series_id_fk = s.id\n            JOIN studies st ON s.study_id_fk = st.id\n            JOIN patients p ON st.patient_id_fk = p.id\n        \"\"\"\n\n        conditions = []\n        params = []\n\n        if patient_ids:\n            placeholders = \",\".join(\"?\" for _ in patient_ids)\n            conditions.append(f\"p.patient_id IN ({placeholders})\")\n            params.extend(patient_ids)\n\n        if instance_uids:\n            placeholders = \",\".join(\"?\" for _ in instance_uids)\n            conditions.append(f\"i.sop_instance_uid IN ({placeholders})\")\n            params.extend(instance_uids)\n\n        if conditions:\n            query += \" WHERE \" + \" AND \".join(conditions)\n\n        # Execute generator\n        cursor = cur.execute(query, params)\n\n        # We can map columns to names\n        cols = [desc[0] for desc in cursor.description]\n\n        for row in cursor:\n            yield dict(zip(cols, row))\n</code></pre>"},{"location":"api/persistence/#gantry.persistence.SqliteStore.get_total_instances","title":"<code>get_total_instances()</code>","text":"<p>Returns the total number of instances currently persisted.</p> Source code in <code>gantry/persistence.py</code> <pre><code>def get_total_instances(self) -&gt; int:\n    \"\"\"Returns the total number of instances currently persisted.\"\"\"\n    try:\n         with self._get_connection() as conn:\n            cur = conn.cursor()\n            row = cur.execute(\"SELECT COUNT(*) FROM instances\").fetchone()\n            return row[0] if row else 0\n    except sqlite3.Error as e:\n        self.logger.error(f\"Failed to count instances: {e}\")\n        return 0\n</code></pre>"},{"location":"api/persistence/#gantry.persistence.SqliteStore.load_all","title":"<code>load_all()</code>","text":"<p>Reconstructs the entire object graph from the database. Returns a list of Patient objects.</p> Source code in <code>gantry/persistence.py</code> <pre><code>def load_all(self) -&gt; List[Patient]:\n    \"\"\"\n    Reconstructs the entire object graph from the database.\n    Returns a list of Patient objects.\n    \"\"\"\n    patients = []\n    if self.db_path != \":memory:\" and not os.path.exists(self.db_path):\n        return patients\n\n    try:\n        with self._get_connection() as conn:\n            # conn.row_factory = sqlite3.Row  &lt;-- Handled by _get_connection\n            cur = conn.cursor()\n\n            # Optimized: We could do joins, but for clarity/mapping let's do hierarchical fetch.\n            # Or fetch all and Stitch. Stitching in memory is faster for SQLite than N+1 queries.\n\n            # 1. Fetch AlL\n            p_rows = cur.execute(\"SELECT * FROM patients\").fetchall()\n            st_rows = cur.execute(\"SELECT * FROM studies\").fetchall()\n            se_rows = cur.execute(\"SELECT * FROM series\").fetchall()\n            i_rows = cur.execute(\"SELECT * FROM instances\").fetchall()\n\n\n\n            # 2. Build Maps\n            p_map = {}\n            for r in p_rows:\n                p = Patient(r['patient_id'], r['patient_name'])\n                p_map[r['id']] = p\n                patients.append(p)\n\n            st_map = {}\n            for r in st_rows:\n                st = Study(r['study_instance_uid'], r['study_date'])\n                st_map[r['id']] = st\n                if r['patient_id_fk'] in p_map:\n                    p_map[r['patient_id_fk']].studies.append(st)\n\n            se_map = {}\n            for r in se_rows:\n                se = Series(r['series_instance_uid'], r['modality'], r['series_number'])\n                if r['manufacturer'] or r['model_name']:\n                    se.equipment = Equipment(r['manufacturer'], r['model_name'], r['device_serial_number'])\n                se_map[r['id']] = se\n                if r['study_id_fk'] in st_map:\n                    st_map[r['study_id_fk']].series.append(se)\n\n            for r in i_rows:\n                inst = Instance(\n                    r['sop_instance_uid'], \n                    r['sop_class_uid'], \n                    r['instance_number'], \n                    file_path=r['file_path']\n                )\n\n                # Wire up Sidecar Loader if present\n                if r['pixel_offset'] is not None and r['pixel_length'] is not None:\n                     # Capture closure vars\n                     offset = r['pixel_offset']\n                     length = r['pixel_length']\n                     alg = r['compress_alg']\n\n                     # We need to reshape after loading. The dimensions are in attributes.\n                     # We can do this inside the lambda wrapper or a helper method.\n                     # But Instance.attributes aren't populated yet! \n                     # Wait, we populate attributes right after this.\n                     # So the lambda calls self.instance methods? No, lambda binds early.\n\n                     inst._pixel_loader = self._create_pixel_loader(r['pixel_offset'], r['pixel_length'], r['compress_alg'], inst)\n\n                # Restore extra attributes\n                if r['attributes_json']:\n                    try:\n                        attrs = json.loads(r['attributes_json'], object_hook=gantry_json_object_hook)\n                        self._deserialize_into(inst, attrs)\n                    except: \n                        pass # JSON error\n\n                if r['series_id_fk'] in se_map:\n                    se_map[r['series_id_fk']].instances.append(inst)\n\n        self.logger.info(f\"Loaded {len(patients)} patients from {self.db_path}\")\n        # Mark all loaded data as clean so we don't save it back immediately\n        for p in patients:\n            p.mark_clean()\n        return patients\n\n    except sqlite3.Error as e:\n        print(f\"DEBUG: Failed to load from DB: {e}\")\n        self.logger.error(f\"Failed to load PDF from DB: {e}\")\n        import traceback\n        traceback.print_exc()\n        return []\n</code></pre>"},{"location":"api/persistence/#gantry.persistence.SqliteStore.load_findings","title":"<code>load_findings()</code>","text":"<p>Loads all findings from the database.</p> Source code in <code>gantry/persistence.py</code> <pre><code>def load_findings(self) -&gt; List[PhiFinding]:\n    \"\"\"Loads all findings from the database.\"\"\"\n    findings = []\n    if self.db_path != \":memory:\" and not os.path.exists(self.db_path):\n        return findings\n\n    try:\n         with self._get_connection() as conn:\n            # conn.row_factory = sqlite3.Row\n            cur = conn.cursor()\n            # Check if table exists (backward compatibility for old DBs if init didnt run on them)\n            # But _init_db runs on __init__, so schema should be there.\n\n            rows = cur.execute(\"SELECT * FROM phi_findings ORDER BY id\").fetchall()\n\n            for r in rows:\n                if r['remediation_action']:\n                    prop = PhiRemediation(r['remediation_action'], r['field_name'], r['remediation_value'], None) \n                else: \n                    prop = None\n\n                f = PhiFinding(\n                    entity_uid=r['entity_uid'],\n                    entity_type=r['entity_type'],\n                    field_name=r['field_name'],\n                    value=r['value'],\n                    reason=r['reason'],\n                    patient_id=r['patient_id'],\n                    remediation_proposal=prop\n                )\n                findings.append(f)\n\n    except sqlite3.Error as e:\n        self.logger.error(f\"Failed to load findings: {e}\")\n\n    return findings\n</code></pre>"},{"location":"api/persistence/#gantry.persistence.SqliteStore.load_patient","title":"<code>load_patient(patient_uid)</code>","text":"<p>Loads a single patient and their graph from the DB by PatientID.</p> Source code in <code>gantry/persistence.py</code> <pre><code>def load_patient(self, patient_uid: str) -&gt; Optional[Patient]:\n    \"\"\"Loads a single patient and their graph from the DB by PatientID.\"\"\"\n    if self.db_path != \":memory:\" and not os.path.exists(self.db_path):\n        return None\n\n    try:\n         with self._get_connection() as conn:\n            # conn.row_factory = sqlite3.Row\n            cur = conn.cursor()\n\n            # Fetch Patient\n            p_row = cur.execute(\"SELECT * FROM patients WHERE patient_id = ?\", (patient_uid,)).fetchone()\n            if not p_row: return None\n\n            p = Patient(p_row['patient_id'], p_row['patient_name'])\n            p_pk = p_row['id']\n\n            # Fetch Studies\n            st_rows = cur.execute(\"SELECT * FROM studies WHERE patient_id_fk = ?\", (p_pk,)).fetchall()\n            for st_r in st_rows:\n                st = Study(st_r['study_instance_uid'], st_r['study_date'])\n                st_pk = st_r['id']\n\n                # Fetch Series\n                se_rows = cur.execute(\"SELECT * FROM series WHERE study_id_fk = ?\", (st_pk,)).fetchall()\n                for se_r in se_rows:\n                    se = Series(se_r['series_instance_uid'], se_r['modality'], se_r['series_number'])\n                    if se_r['manufacturer'] or se_r['model_name']:\n                        se.equipment = Equipment(se_r['manufacturer'], se_r['model_name'], se_r['device_serial_number'])\n                    se_pk = se_r['id']\n\n                    # Fetch Instances\n                    i_rows = cur.execute(\"SELECT * FROM instances WHERE series_id_fk = ?\", (se_pk,)).fetchall()\n                    for r in i_rows:\n                        inst = Instance(\n                            r['sop_instance_uid'], \n                            r['sop_class_uid'], \n                            r['instance_number'], \n                            file_path=r['file_path']\n                        )\n                        # Wire up Sidecar (Copy-Paste logic from load_all, keep generic?)\n                        # ideally refactor _hydrate_instance but inline is fine for now\n                        if r['pixel_offset'] is not None and r['pixel_length'] is not None:\n                            offset, length, alg = r['pixel_offset'], r['pixel_length'], r['compress_alg']\n                            inst._pixel_loader = self._create_pixel_loader(r['pixel_offset'], r['pixel_length'], r['compress_alg'], inst)\n\n                        if r['attributes_json']:\n                            try:\n                                attrs = json.loads(r['attributes_json'], object_hook=gantry_json_object_hook)\n                                self._deserialize_into(inst, attrs)\n                            except: pass\n\n                        se.instances.append(inst)\n\n                    st.series.append(se)\n                p.studies.append(st)\n\n            p.mark_clean()\n            return p\n    except sqlite3.Error as e:\n        self.logger.error(f\"Failed to load patient: {e}\")\n        return None\n</code></pre>"},{"location":"api/persistence/#gantry.persistence.SqliteStore.load_vertical_attributes","title":"<code>load_vertical_attributes(instance_uid)</code>","text":"<p>Loads extended attributes from vertical table. Returns dict: {(grp, elem): value_or_list}</p> Source code in <code>gantry/persistence.py</code> <pre><code>def load_vertical_attributes(self, instance_uid: str) -&gt; Dict[Tuple[str, str], Any]:\n    \"\"\"\n    Loads extended attributes from vertical table.\n    Returns dict: {(grp, elem): value_or_list}\n    \"\"\"\n    results = {}\n    try:\n        with self._get_connection() as conn:\n            rows = conn.execute(\"\"\"\n                SELECT group_id, element_id, atom_index, value_text \n                FROM instance_attributes \n                WHERE instance_uid=? \n                ORDER BY group_id, element_id, atom_index\n            \"\"\", (instance_uid,)).fetchall()\n\n            if not rows: return {}\n\n            # Reassemble\n            curr_key = None\n            collect = []\n\n            for r in rows:\n                key = (r['group_id'], r['element_id'])\n                val = r['value_text'] # Type conversion? Strings for now.\n\n                if key != curr_key:\n                    # Flush previous\n                    if curr_key:\n                        results[curr_key] = collect if len(collect) &gt; 1 else collect[0]\n                    curr_key = key\n                    collect = [val]\n                else:\n                    collect.append(val)\n\n            # Flush last\n            if curr_key:\n                results[curr_key] = collect if len(collect) &gt; 1 else collect[0]\n\n        return results\n    except sqlite3.Error as e:\n        self.logger.error(f\"Failed to load vertical attributes for {instance_uid}: {e}\")\n        return {}\n</code></pre>"},{"location":"api/persistence/#gantry.persistence.SqliteStore.log_audit","title":"<code>log_audit(action_type, entity_uid, details)</code>","text":"<p>Records an action in the audit log (Async).</p> Source code in <code>gantry/persistence.py</code> <pre><code>def log_audit(self, action_type: str, entity_uid: str, details: str):\n    \"\"\"Records an action in the audit log (Async).\"\"\"\n    # Push to queue instead of writing directly\n    self.audit_queue.put((action_type, entity_uid, details))\n</code></pre>"},{"location":"api/persistence/#gantry.persistence.SqliteStore.log_audit_batch","title":"<code>log_audit_batch(entries)</code>","text":"<p>Batch inserts audit logs.  entries: List of (action_type, entity_uid, details)</p> Source code in <code>gantry/persistence.py</code> <pre><code>def log_audit_batch(self, entries: List[tuple]):\n    \"\"\"\n    Batch inserts audit logs. \n    entries: List of (action_type, entity_uid, details)\n    \"\"\"\n    if not entries: return\n\n    timestamp = datetime.now().isoformat()\n    # Prepare data with timestamp: (timestamp, action, uid, details)\n    data = [(timestamp, e[0], e[1], e[2]) for e in entries]\n\n    try:\n        with self._get_connection() as conn:\n            conn.executemany(\n                \"INSERT INTO audit_log (timestamp, action_type, entity_uid, details) VALUES (?, ?, ?, ?)\",\n                data\n            )\n            conn.commit()\n    except sqlite3.Error as e:\n        self.logger.error(f\"Failed to batch log audit: {e}\")\n</code></pre>"},{"location":"api/persistence/#gantry.persistence.SqliteStore.persist_pixel_data","title":"<code>persist_pixel_data(instance)</code>","text":"<p>Immediately persists pixel data to the sidecar to allow memory offloading. Does NOT update the full instance record in the main DB (attributes/json),  only the pixel linkage.</p> Source code in <code>gantry/persistence.py</code> <pre><code>def persist_pixel_data(self, instance: Instance):\n    \"\"\"\n    Immediately persists pixel data to the sidecar to allow memory offloading.\n    Does NOT update the full instance record in the main DB (attributes/json), \n    only the pixel linkage. \n    \"\"\"\n    if instance.pixel_array is None:\n        return\n\n    try:\n        # 1. Write to Sidecar\n        # Pass array directly to avoid .tobytes() Memory spike (Zero-Copy 500MB save)\n        b_data = instance.pixel_array\n        # Determine suitable compression? Defaulting to zlib for swap.\n        # Ideally we respect original or config, but for swap zlib is safe/fast enough.\n        c_alg = 'zlib' \n\n        offset, length = self.sidecar.write_frame(b_data, c_alg)\n\n        # Hash Update (CRITICAL for Integrity Checks)\n        import hashlib\n        p_hash = hashlib.sha256(b_data).hexdigest()\n        instance._pixel_hash = p_hash\n\n        # 2. Update Instance Loader\n        # This allows instance.unload_pixel_data() to work safely\n        instance._pixel_loader = self._create_pixel_loader(offset, length, c_alg, instance)\n\n        # 3. Optional: Persist the linkage to DB immediately?\n        # It's safer if we do, so if we crash, we know where the pixels are.\n        # However, if we don't save the attributes/UID changes, the DB is out of sync anyway.\n        # But the primary goal here is MEMORY MANAGEMENT.\n        # So updating the object state in memory (step 2) is sufficient for unload_pixel_data() to return True.\n        # The final session.save() will record the new offset/length into the DB instances table.\n\n    except Exception as e:\n        self.logger.error(f\"Failed to persist pixel swap for {instance.sop_instance_uid}: {e}\")\n        raise e\n</code></pre>"},{"location":"api/persistence/#gantry.persistence.SqliteStore.save_all","title":"<code>save_all(patients)</code>","text":"<p>Persists the current state incrementally. Strategy: UPSERT dirty items.</p> Source code in <code>gantry/persistence.py</code> <pre><code>def save_all(self, patients: List[Patient]):\n    \"\"\"\n    Persists the current state incrementally.\n    Strategy: UPSERT dirty items.\n    \"\"\"\n    self.logger.info(f\"Saving {len(patients)} patients to {self.db_path} (Incremental)...\")\n\n    pixel_bytes_written = 0\n    pixel_frames_written = 0\n    sidecar_manager = self.sidecar\n\n    try:\n        with self._get_connection() as conn:\n            cur = conn.cursor()\n\n            # Check for schema compatibility (simple check)\n            try:\n                # We rely on UNIQUE constraints for UPSERT. \n                # If older DB without constraints, we might fail or duplicate.\n                pass \n            except: pass\n\n            # Counts for reporting\n            saved_p, saved_st, saved_se, saved_i = 0, 0, 0, 0\n\n            for p in patients:\n                # Patient Level (Always Check Dirty)\n                if getattr(p, '_dirty', True):\n                    cur.execute(\"\"\"\n                        INSERT INTO patients (patient_id, patient_name) VALUES (?, ?)\n                        ON CONFLICT(patient_id) DO UPDATE SET patient_name=excluded.patient_name\n                    \"\"\", (p.patient_id, p.patient_name))\n                    saved_p += 1\n\n                # We need the PK for children\n                # Since we might have just updated or it might exist, we select it.\n                # Optimization: Cache PKs? For now, fetch is safe.\n                p_pk_row = cur.execute(\"SELECT id FROM patients WHERE patient_id=?\", (p.patient_id,)).fetchone()\n                if not p_pk_row: continue # Should not happen after Insert\n                p_pk = p_pk_row[0]\n\n                for st in p.studies:\n                    if getattr(st, '_dirty', True):\n                        # FIX: Convert date objects to string to avoid Python 3.12+ DeprecationWarning for default adapter\n                        s_date = st.study_date\n                        if hasattr(s_date, \"isoformat\"):\n                            s_date = s_date.isoformat()\n                        elif s_date is not None:\n                            s_date = str(s_date)\n\n                        cur.execute(\"\"\"\n                            INSERT INTO studies (patient_id_fk, study_instance_uid, study_date) VALUES (?, ?, ?)\n                            ON CONFLICT(study_instance_uid) DO UPDATE SET \n                                study_date=excluded.study_date,\n                                patient_id_fk=excluded.patient_id_fk\n                        \"\"\", (p_pk, st.study_instance_uid, s_date))\n                        saved_st += 1\n\n                    st_pk_row = cur.execute(\"SELECT id FROM studies WHERE study_instance_uid=?\", (st.study_instance_uid,)).fetchone()\n                    if not st_pk_row: continue\n                    st_pk = st_pk_row[0]\n\n                    for se in st.series:\n                        if getattr(se, '_dirty', True):\n                            man = se.equipment.manufacturer if se.equipment else \"\"\n                            mod = se.equipment.model_name if se.equipment else \"\"\n                            sn = se.equipment.device_serial_number if se.equipment else \"\"\n\n                            cur.execute(\"\"\"\n                                INSERT INTO series (study_id_fk, series_instance_uid, modality, series_number, manufacturer, model_name, device_serial_number)\n                                VALUES (?, ?, ?, ?, ?, ?, ?)\n                                ON CONFLICT(series_instance_uid) DO UPDATE SET \n                                    modality=excluded.modality,\n                                    series_number=excluded.series_number,\n                                    manufacturer=excluded.manufacturer,\n                                    model_name=excluded.model_name,\n                                    device_serial_number=excluded.device_serial_number,\n                                    study_id_fk=excluded.study_id_fk\n                            \"\"\", (st_pk, se.series_instance_uid, se.modality, se.series_number, man, mod, sn))\n                            saved_se += 1\n\n                        se_pk_row = cur.execute(\"SELECT id FROM series WHERE series_instance_uid=?\", (se.series_instance_uid,)).fetchone()\n                        if not se_pk_row: continue\n                        se_pk = se_pk_row[0]\n\n                        # --- Deletion Handling (Diff DB vs Memory) ---\n                        # Only perform if we suspect deletions or periodically? \n                        # Plan says: Implement Diff Logic.\n                        # Optimization: If series is NOT dirty, can we assume no deletions?\n                        # Not necessarily. Removing an item doesn't always mark Series dirty unless we hook \"remove\".\n                        # But DicomItem doesn't track removals from list automatically.\n                        # So we must check.\n\n                        db_uids_rows = cur.execute(\"SELECT sop_instance_uid FROM instances WHERE series_id_fk=?\", (se_pk,)).fetchall()\n                        db_uids = {r[0] for r in db_uids_rows}\n                        mem_uids = {i.sop_instance_uid for i in se.instances}\n\n                        to_delete = db_uids - mem_uids\n                        if to_delete:\n                            cur.executemany(\"DELETE FROM instances WHERE sop_instance_uid=?\", [(u,) for u in to_delete])\n                            saved_i += 0 # Or count negative?\n                            # self.logger.debug(f\"Deleted {len(to_delete)} instances from Series {se.series_instance_uid}\")\n\n                        # --- Upsert Dirty ---\n                        dirty_items = []\n                        for i in se.instances:\n                            if getattr(i, '_dirty', True):\n                                # Capture version if available (robustness against race)\n                                ver = getattr(i, '_mod_count', 0)\n                                dirty_items.append((i, ver))\n\n                        if dirty_items:\n                            i_batch = []\n                            vert_updates = [] # Defer vertical updates to satisfy foreign key\n                            for inst, ver in dirty_items:\n                                full_data = self._serialize_item(inst)\n\n                                # Split Core vs Vertical (Private Tags -&gt; Vertical Table)\n                                core_data = {}\n                                vert_data = {}\n\n                                for key, val in full_data.items():\n                                    if key == \"__sequences__\":\n                                         core_data[key] = val # Keep sequences in Core JSON for now\n                                         continue\n\n                                    # key is \"GGGG,EEEE\" hex string\n                                    try:\n                                        group = int(key.split(',')[0], 16)\n                                        # Odd Group = Private Tag (usually)\n                                        # Skip Vertical for BYTES (cant be stored as TEXT easily, keep in JSON)\n                                        is_private = (group % 2 != 0) and not isinstance(val, bytes)\n\n                                        if is_private:\n                                             # Tuple key for vertical method: (grp, elem)\n                                             k_tuple = tuple(key.split(','))\n                                             vert_data[k_tuple] = val\n                                        else:\n                                             core_data[key] = val\n                                    except:\n                                        core_data[key] = val\n\n                                # Queue Vertical (Saved after Instance Insert)\n                                if vert_data:\n                                    vert_updates.append((inst.sop_instance_uid, vert_data))\n\n                                # Serialize Core\n                                attrs_json = json.dumps(core_data, cls=GantryJSONEncoder)\n\n                                p_offset, p_length, p_alg, p_hash = None, None, None, None\n\n                                if inst.pixel_array is not None:\n                                     b_data = inst.pixel_array.tobytes()\n                                     c_alg = 'zlib'\n                                     # Compute Hash\n                                     import hashlib\n                                     p_hash = hashlib.sha256(b_data).hexdigest()\n\n                                     off, leng = sidecar_manager.write_frame(b_data, c_alg)\n                                     p_offset, p_length, p_alg = off, leng, c_alg\n                                     pixel_bytes_written += leng\n                                     pixel_frames_written += 1\n\n                                     # Update loader so we can unload safely later\n                                     inst._pixel_loader = self._create_pixel_loader(off, leng, c_alg, inst)\n                                     inst._pixel_hash = p_hash # Cache on instance\n\n                                elif isinstance(inst._pixel_loader, SidecarPixelLoader):\n                                     # Already persisted (swapped), preserve metadata\n                                     p_offset = inst._pixel_loader.offset\n                                     p_length = inst._pixel_loader.length\n                                     p_alg = inst._pixel_loader.alg\n                                     p_hash = getattr(inst, '_pixel_hash', None)\n                                else:\n                                     pass\n\n                                i_batch.append((\n                                    se_pk, \n                                    inst.sop_instance_uid, \n                                    inst.sop_class_uid, \n                                    inst.instance_number, \n                                    inst.file_path, \n                                    p_offset, \n                                    p_length, \n                                    p_hash,\n                                    p_alg, \n                                    attrs_json\n                                ))\n\n                            cur.executemany(\"\"\"\n                                INSERT INTO instances (series_id_fk, sop_instance_uid, sop_class_uid, instance_number, file_path, \n                                                       pixel_offset, pixel_length, pixel_hash, compress_alg, attributes_json)\n                                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n                                ON CONFLICT(sop_instance_uid) DO UPDATE SET\n                                    series_id_fk=excluded.series_id_fk,\n                                    sop_class_uid=excluded.sop_class_uid,\n                                    instance_number=excluded.instance_number,\n                                    file_path=excluded.file_path,\n                                    attributes_json=excluded.attributes_json,\n                                    pixel_offset=COALESCE(excluded.pixel_offset, instances.pixel_offset),\n                                    pixel_length=COALESCE(excluded.pixel_length, instances.pixel_length),\n                                    pixel_hash=COALESCE(excluded.pixel_hash, instances.pixel_hash),\n                                    compress_alg=COALESCE(excluded.compress_alg, instances.compress_alg)\n                            \"\"\", i_batch)\n\n                            # Process Deferred Vertical Updates (Now that Instances exist)\n                            if vert_updates:\n                                # self.logger.debug(f\"Saving vertical attributes for {len(vert_updates)} instances\")\n                                for uid, v_data in vert_updates:\n                                    self.save_vertical_attributes(uid, v_data, conn=conn)\n\n                            saved_i += len(dirty_items)\n\n                            # Mark saved with version (deferred until commit success? \n                            # No, we can attach to list and do it post-commit)\n                            # But we're inside loops. \n                            # Creating a cleanup list:\n                            # (We can store dirty_items in a larger list to clean up post-commit)\n                            # For now, let's mark clean *assuming* commit will succeed.\n                            # If commit fails, we rollback, but objects remain \"clean\" in memory?\n                            # That is a risk. We should do it post-commit.\n                            # But scope is tricky. \n                            # Let's mark clean here but using version. \n                            # If transaction rolls back, DB is old, but memory has _saved_mod_count advanced?\n                            # That means next save won't save it. BAD.\n                            # We must hold off.\n\n                            # Since we commit once at the end:\n                            # We need to collect ALL dirty items and their versions.\n                            # That is expensive memory-wise for massive sets.\n                            # But necessary for correctness.\n                            # Compromise: we iterate again. \n                            # Wait, \"Iterate again\" in 'mark clean' loop below.\n                            # We can't know \"ver\" then.\n\n                            # Let's just update them here. If commit fails, the Exception propagates.\n                            # Use a try/except block around the whole `save_all`? Yes.\n                            # But `_saved_mod_count` is in memory.\n                            # If we update it, and `save_all` crashes, we can't easily undo it.\n                            # BUT `save_all` crashing usually kills the process or stops persistence.\n                            # So `eventual consistency` implies retrying.\n                            # If we marked it saved but it didn't save, we have data loss.\n\n                            # Correct way: List of callbacks?\n                            # Or just:\n                            for inst, ver in dirty_items:\n                                 if hasattr(inst, 'mark_saved'):\n                                      inst.mark_saved(ver)\n                                 else:\n                                      inst._dirty = False\n\n            conn.commit()\n\n            # Post-Commit: \n            # We already marked items as saved/clean incrementally using naive-commit assumption.\n            # If transaction failed, those items are marked clean in memory but not in DB -&gt; Inconsistency.\n            # However, re-implementing rollback for memory objects is out of scope.\n            # The versioning fixes the \"Overwrite valid change\" race, which is the user's issue.\n            pass\n\n            # Restore Logging Logic\n            if saved_p + saved_i &gt; 0:\n                 msg = f\"Save (Inc) complete. P:{saved_p} St:{saved_st} Se:{saved_se} I:{saved_i}.\"\n                 if pixel_frames_written &gt; 0:\n                     mb = pixel_bytes_written / (1024*1024)\n                     msg += f\" Sidecar: {pixel_frames_written} frames ({mb:.2f} MB).\"\n                 self.logger.info(msg)\n\n    except Exception as e:\n        self.logger.error(f\"Save failed: {e}\")\n        if hasattr(conn, \"rollback\"): conn.rollback()\n        raise\n</code></pre>"},{"location":"api/persistence/#gantry.persistence.SqliteStore.save_findings","title":"<code>save_findings(findings)</code>","text":"<p>Persists PHI findings to the database.</p> Source code in <code>gantry/persistence.py</code> <pre><code>def save_findings(self, findings: List[PhiFinding]):\n    \"\"\"Persists PHI findings to the database.\"\"\"\n    timestamp = datetime.now().isoformat()\n\n    if not findings:\n        return\n\n    self.logger.info(f\"Saving {len(findings)} PHI findings...\")\n\n    try:\n        with self._get_connection() as conn:\n            cur = conn.cursor()\n\n            # Prepare Data Generator for Batch Insert (Memory Efficient)\n            def findings_generator():\n                for f in findings:\n                    rem_action = None\n                    rem_value = None\n                    if f.remediation_proposal:\n                        rem_action = f.remediation_proposal.action_type\n                        rem_value = str(f.remediation_proposal.new_value)\n\n                    yield (\n                        timestamp, \n                        f.entity_uid, \n                        f.entity_type, \n                        f.field_name, \n                        str(f.value), \n                        f.reason, \n                        f.patient_id, \n                        rem_action, \n                        rem_value, \n                        \"{}\"\n                    )\n\n            cur.executemany(\"\"\"\n                INSERT INTO phi_findings \n                (timestamp, entity_uid, entity_type, field_name, value, reason, patient_id, remediation_action, remediation_value, details_json) \n                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n            \"\"\", findings_generator())\n\n            conn.commit()\n            self.logger.info(\"Findings saved.\")\n\n    except sqlite3.Error as e:\n        self.logger.error(f\"Failed to save findings: {e}\")\n</code></pre>"},{"location":"api/persistence/#gantry.persistence.SqliteStore.save_vertical_attributes","title":"<code>save_vertical_attributes(instance_uid, attributes, conn=None)</code>","text":"<p>Persists extended attributes to the vertical <code>instance_attributes</code> table. attributes key format: (group_hex, element_hex) e.g. (\"0010\", \"0010\") Use UPSERT semantics. Optionally accepts an existing connection to share transaction.</p> Source code in <code>gantry/persistence.py</code> <pre><code>def save_vertical_attributes(self, instance_uid: str, attributes: Dict[Tuple[str, str], Any], conn: sqlite3.Connection = None):\n    \"\"\"\n    Persists extended attributes to the vertical `instance_attributes` table.\n    attributes key format: (group_hex, element_hex) e.g. (\"0010\", \"0010\")\n    Use UPSERT semantics.\n    Optionally accepts an existing connection to share transaction.\n    \"\"\"\n    if not attributes: return\n\n    data_rows = []\n    for (grp, elem), val in attributes.items():\n        vr = \"UN\" # Todo: Pass VR from caller\n        # Check for VM &gt; 1\n        if isinstance(val, list):\n            for idx, atom in enumerate(val):\n                data_rows.append((instance_uid, grp, elem, idx, vr, str(atom)))\n        else:\n             data_rows.append((instance_uid, grp, elem, 0, vr, str(val)))\n\n    if not data_rows: return\n\n    try:\n        from contextlib import nullcontext\n        # If conn is passed, use it (and don't close it/commit it here, leave to caller).\n        # If not, create new context (which commits/closes).\n        ctx = self._get_connection() if conn is None else nullcontext(conn)\n\n        with ctx as db:\n            # 1. OPTIMIZATION: Delete existing for these keys first? \n            # Or UPSERT. \n            # \"test_vertical_update_serialization\" requires correctness.\n            # UPSERT based on unique index (uid, grp, elem, atom) works.\n            # But if list shrinks (VM 3 -&gt; VM 1), UPSERT leaves atoms 2,3.\n            # So we MUST DELETE by (uid, grp, elem) before inserting new set for that tag.\n\n            # We can do this in transaction.\n            keys_to_clear = list(attributes.keys())\n            # Batch delete?\n            # \"DELETE FROM instance_attributes WHERE instance_uid=? AND group_id=? AND element_id=?\\\"\n            del_params = [(instance_uid, k[0], k[1]) for k in keys_to_clear]\n            db.executemany(\n                \"DELETE FROM instance_attributes WHERE instance_uid=? AND group_id=? AND element_id=?\", \n                del_params\n            )\n\n            db.executemany(\"\"\"\n                INSERT INTO instance_attributes (instance_uid, group_id, element_id, atom_index, value_rep, value_text)\n                VALUES (?, ?, ?, ?, ?, ?)\n            \"\"\", data_rows)\n\n    except sqlite3.Error as e:\n        self.logger.error(f\"Failed to save vertical attributes for {instance_uid}: {e}\")\n        raise e\n</code></pre>"},{"location":"api/persistence/#gantry.persistence.SqliteStore.stop","title":"<code>stop()</code>","text":"<p>Stops the audit worker and flushes queue.</p> Source code in <code>gantry/persistence.py</code> <pre><code>def stop(self):\n    \"\"\"Stops the audit worker and flushes queue.\"\"\"\n    self._stop_event.set()\n    if self._audit_thread.is_alive():\n        self._audit_thread.join(timeout=2.0)\n    self.flush_audit_queue()\n</code></pre>"},{"location":"api/persistence/#gantry.persistence.SqliteStore.update_attributes","title":"<code>update_attributes(instances)</code>","text":"<p>Efficiently updates the attributes_json for a list of instances.</p> Source code in <code>gantry/persistence.py</code> <pre><code>def update_attributes(self, instances: List[Patient]):\n    \"\"\"\n    Efficiently updates the attributes_json for a list of instances.\n    \"\"\"\n    if not instances:\n        return\n\n    self.logger.info(f\"Updating attributes for {len(instances)} instances...\")\n    try:\n        with self._get_connection() as conn:\n            cur = conn.cursor()\n\n            # Pre-calculate data for executemany\n            data = []\n            for inst in instances:\n                # Serialize attributes AND sequences\n                full_data = self._serialize_item(inst)\n                attrs_json = json.dumps(full_data, cls=GantryJSONEncoder)\n                data.append((attrs_json, inst.sop_instance_uid))\n\n            cur.executemany(\"\"\"\n                UPDATE instances \n                SET attributes_json = ? \n                WHERE sop_instance_uid = ?\n            \"\"\", data)\n\n            conn.commit()\n            self.logger.info(\"Update complete.\")\n\n    except sqlite3.Error as e:\n        self.logger.error(f\"Failed to update attributes: {e}\")\n</code></pre>"},{"location":"api/session/","title":"Session API","text":""},{"location":"api/session/#gantry.session.DicomSession","title":"<code>gantry.session.DicomSession</code>","text":"<p>The Main Facade for the Gantry library. Manages the lifecycle of the DicomStore (Load/Import/Redact/Export/Save).</p> Source code in <code>gantry/session.py</code> <pre><code>class DicomSession:\n    \"\"\"\n    The Main Facade for the Gantry library.\n    Manages the lifecycle of the DicomStore (Load/Import/Redact/Export/Save).\n    \"\"\"\n\n    # =========================================================================\n    # 1. LIFECYCLE\n    # =========================================================================\n\n    def __init__(self, persistence_file=\"gantry.db\"):\n        \"\"\"\n        Initialize the DicomSession.\n\n        Args:\n            persistence_file: Path to the SQLite database for session persistence.\n        \"\"\"\n        configure_logger()\n        self.persistence_file = persistence_file\n\n        # Check existence before SqliteStore potentially creates it\n        db_exists = os.path.exists(persistence_file)\n\n        self.store_backend = SqliteStore(persistence_file)\n        self.persistence_manager = PersistenceManager(self.store_backend)\n\n        # Hydrate memory from DB\n        self.store = DicomStore()\n\n        if db_exists:\n            print(f\"Loading session from {persistence_file}...\")\n        else:\n            print(f\"Initializing new session at {persistence_file}...\")\n\n        self.store.patients = self.store_backend.load_all()\n\n        # Initialize Configuration Object\n        from .configuration import GantryConfiguration\n        self.configuration = GantryConfiguration()\n\n\n        # Reversibility\n        self.key_manager = None\n        self.reversibility_service = None\n\n        if os.path.exists(\"gantry.key\"):\n            self.enable_reversible_anonymization(\"gantry.key\")\n\n        # Shared Global Executor for Process Consistency\n        self._executor = concurrent.futures.ProcessPoolExecutor(max_workers=None) # Default: CPU * 1.5\n\n        if db_exists:\n            print(f\"Loaded session from {persistence_file}\")\n\n        get_logger().info(f\"Session started. {len(self.store.patients)} patients loaded.\")\n\n    def close(self):\n        \"\"\"\n        Cleanly shuts down the session, stopping background threads and flushing queues.\n        \"\"\"\n        print(\"Closing session persistence...\")\n        if hasattr(self, 'persistence_manager'):\n            self.persistence_manager.shutdown()\n        if hasattr(self, 'store_backend'):\n            self.store_backend.stop() # Stops audit thread\n\n        if hasattr(self, '_executor'):\n            print(\"Shutting down process pool...\")\n            self._executor.shutdown(wait=True)\n\n    def save(self, sync: bool = False):\n        \"\"\"\n        Persists the current session state to the database.\n        :param sync: If True, blocks until save is complete.\n        \"\"\"\n        if sync and hasattr(self, 'store_backend'):\n             from .logger import get_logger\n             get_logger().info(\"Saving session (Synchronous)...\")\n             self.store_backend.save_all(self.store.patients)\n        elif hasattr(self, 'persistence_manager'):\n             self.persistence_manager.save_async(self.store.patients)\n\n    def _restart_executor(self, max_workers=None):\n        \"\"\"\n        Restarts the internal process pool executor, potentially with fewer workers.\n        Useful for recovering from BrokenProcessPool errors (OOM).\n        \"\"\"\n        get_logger().warning(f\"Restarting ProcessPoolExecutor (max_workers={max_workers})...\")\n        if self._executor:\n            try:\n                # Force kill old processes if they are stuck/broken\n                self._executor.shutdown(wait=False, cancel_futures=True)\n            except:\n                pass\n\n        # Re-init\n        self._executor = concurrent.futures.ProcessPoolExecutor(max_workers=max_workers)\n\n    def release_memory(self):\n        \"\"\"\n        Attempts to release memory by unloading pixel data from all instances.\n        Safe to call: only unloads data that is safely persisted (on disk or sidecar).\n        Useful after running extensive redaction or export operations.\n        \"\"\"\n        get_logger().info(\"Releasing memory (RAM cleanup)...\")\n        count = 0\n        freed = 0\n\n        # Count total instances first for progress bar\n        total_instances = sum(len(se.instances) for p in self.store.patients for st in p.studies for se in st.series)\n\n        if total_instances == 0:\n            return\n\n        from tqdm import tqdm\n        with tqdm(total=total_instances, desc=\"Releasing Memory\", unit=\"img\") as pbar:\n            for p in self.store.patients:\n                for st in p.studies:\n                    for se in st.series:\n                        for inst in se.instances:\n                            count += 1\n                            if inst.unload_pixel_data():\n                                freed += 1\n                            pbar.update(1)\n\n        get_logger().info(f\"Memory release complete. Unloaded pixels for {freed}/{count} instances.\")\n        if freed &gt; 0:\n            print(f\"Memory Cleanup: Released {freed} images from RAM.\")\n\n    def examine(self):\n        \"\"\"Prints a summary of the session contents and equipment.\"\"\"\n        get_logger().info(\"Generating inventory report.\")\n\n        # 1. Object Counts\n        n_p = len(self.store.patients)\n        n_st = sum(len(p.studies) for p in self.store.patients)\n        n_se = sum(len(st.series) for p in self.store.patients for st in p.studies)\n        n_i = sum(len(se.instances) for p in self.store.patients for st in p.studies for se in st.series)\n\n        # 2. Equipment Grouping\n        eq_counts = {} # (man, model) -&gt; count\n\n        for p in self.store.patients:\n            for st in p.studies:\n                for se in st.series:\n                    for inst in se.instances:\n                        if se.equipment:\n                            key = (se.equipment.manufacturer, se.equipment.model_name)\n                            eq_counts[key] = eq_counts.get(key, 0) + 1\n\n        print(f\"\\nInventory Summary:\")\n        print(f\" Patients:  {n_p}\")\n        print(f\" Studies:   {n_st}\")\n        print(f\" Series:    {n_se}\")\n        print(f\" Instances: {n_i}\")\n\n        print(f\"\\nEquipment Inventory:\")\n        if not eq_counts:\n            print(\" No equipment metadata found.\")\n        else:\n            for (man, mod), count in sorted(eq_counts.items()):\n                print(f\" - {man} - {mod} (Count: {count})\")\n\n    # =========================================================================\n    # 2. INGESTION\n    # =========================================================================\n\n    def ingest(self, directory: str):\n        \"\"\"\n        Ingests DICOM files from a directory into the session store.\n        \"\"\"\n        print(f\"Ingesting from '{directory}'...\")\n        from .io_handlers import DicomImporter\n        # Pass Sidecar Manager for eager pixel writing\n        DicomImporter.import_files([directory], self.store, executor=self._executor, sidecar_manager=self.store_backend.sidecar)\n\n        self.save(sync=True)\n\n        # Calculate stats\n        n_p = len(self.store.patients)\n        n_st = sum(len(p.studies) for p in self.store.patients)\n        n_se = sum(len(st.series) for p in self.store.patients for st in p.studies)\n        n_i = sum(len(se.instances) for p in self.store.patients for st in p.studies for se in st.series)\n\n        print(f\"Ingestion complete. Saved session state.\")\n        print(\"Summary:\")\n        print(f\"  - {n_p} Patients\")\n        print(f\"  - {n_st} Studies\")\n        print(f\"  - {n_se} Series\")\n        print(f\"  - {n_i} Instances\")\n\n    # =========================================================================\n    # 3. CONFIGURATION\n    # =========================================================================\n\n    def load_config(self, config_file: str):\n        \"\"\"\n        User Action: 'Load these rules into memory, but DO NOT run them yet.'\n        Useful for validation or previewing what will happen.\n        \"\"\"\n        try:\n            get_logger().info(f\"Loading configuration from {config_file}...\")\n            print(f\"Loading configuration from {config_file}...\")\n\n            # UNIFIED LOAD (v2) - Now loading into GantryConfiguration object\n            tags, rules, jitter, remove_private = ConfigLoader.load_unified_config(config_file)\n\n            # Update the configuration object\n            self.configuration.phi_tags = tags\n            self.configuration.rules = rules\n            self.configuration.date_jitter = jitter\n            self.configuration.remove_private_tags = remove_private\n\n            get_logger().info(f\"Loaded {len(self.configuration.rules)} machine rules and {len(self.configuration.phi_tags)} PHI tags.\")\n            print(f\"Configuration Loaded:\\n - {len(self.configuration.rules)} Machine Redaction Rules\\n - {len(self.configuration.phi_tags)} PHI Tags\")\n            print(f\" - Date Jitter: {self.configuration.date_jitter['min_days']} to {self.configuration.date_jitter['max_days']} days\")\n            print(f\" - Remove Private Tags: {self.configuration.remove_private_tags}\")\n            print(\"Tip: Run .audit() to check PHI, or .redact_pixels() to apply redaction.\")\n        except Exception as e:\n            import traceback\n            get_logger().error(f\"Load failed: {e}\")\n            print(f\"Load failed: {e}\")\n            print(traceback.format_exc())\n            # Reset on failure? OR keep previous? \n            # Original behavior was reset.\n            self.configuration.rules = []\n            self.configuration.phi_tags = {}\n\n    def preview_config(self):\n        \"\"\"\n        User Action: 'Tell me what WOULD happen if I ran these rules.'\n        checks the loaded rules against the current Store inventory.\n        \"\"\"\n        if not self.configuration.rules:\n            get_logger().warning(\"No configuration loaded. Use .load_config() first.\")\n            print(\"No configuration loaded. Use .load_config() first.\")\n            return\n\n        print(\"\\n--- Dry Run / Configuration Preview ---\")\n\n        # We need the index to check matches\n        # We instantiate the service just to query the index, not to modify\n        service = RedactionService(self.store, self.store_backend)\n\n        match_count = 0\n\n        for rule in self.configuration.rules:\n            serial = rule.get(\"serial_number\", \"UNKNOWN\")\n            model = rule.get(\"model_name\", \"Unknown Model\")\n            zones = rule.get(\"redaction_zones\", [])\n\n            # check matches in store\n            targets = service.index.get_by_machine(serial)\n\n            if targets:\n                count = len(targets)\n                match_count += count\n                print(f\"MATCH: '{serial}' ({model})\")\n                print(f\"    - Found {count} images in current session.\")\n                print(f\"    - Actions: Will apply {len(zones)} redaction zones.\")\n            else:\n                print(f\"NO MATCH: '{serial}'. Rule loaded, but no images found.\")\n\n        print(f\"\\nSummary: Execution will modify approximately {match_count} images.\")\n        print(\"---------------------------------------\")\n\n    def create_config(self, output_path: str):\n        \"\"\"\n        Generates a unified configuration file in YAML format.\n        Includes default PHI tags + Auto-detected machine inventory.\n        \"\"\"\n        import yaml\n        import os\n\n        # Helper for Flow-Style Lists (Bracketed)\n        class FlowList(list): pass\n\n        def flow_list_representer(dumper, data):\n            return dumper.represent_sequence('tag:yaml.org,2002:seq', data, flow_style=True)\n\n        yaml.add_representer(FlowList, flow_list_representer)\n\n        if not (output_path.endswith(\".yaml\") or output_path.endswith(\".yml\")):\n            output_path += \".yaml\"\n            print(f\"Note: Appending .yaml extension -&gt; {output_path}\")\n\n        # 1. Identify what we have\n        all_equipment = self.store.get_unique_equipment()\n\n        # Instantiate service to query pixel/tag data efficiently\n        from .services import RedactionService\n        service = RedactionService(self.store)\n\n        # 2. Identify what is already configured (Pixel Rules)\n        configured_serials = {rule.get(\"serial_number\") for rule in self.configuration.rules}\n\n        # Load Knowledge Base for Machines\n        kb_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), \"resources\", \"redaction_rules.json\")\n        kb_machines = []\n        if os.path.exists(kb_path):\n             try:\n                 import json\n                 with open(kb_path, 'r') as f:\n                     kb_data = json.load(f)\n                     kb_machines = kb_data.get(\"machines\", [])\n             except: pass\n\n        # 3. Find missing machines and try to pre-fill\n        missing_configs = []\n        for eq in all_equipment:\n            if eq.device_serial_number and eq.device_serial_number not in configured_serials:\n\n                # Check KB\n                matched_rule = None\n                # Primary: Serial Match\n                for rule in kb_machines:\n                    if rule.get(\"serial_number\") == eq.device_serial_number:\n                        matched_rule = rule\n                        break\n\n                # Check CTP Rules (Knowledge Base 2)\n                ctp_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), \"resources\", \"ctp_rules.yaml\")\n                if not os.path.exists(ctp_path):\n                     # Fallback to JSON if YAML doesn't exist\n                     ctp_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), \"resources\", \"ctp_rules.json\")\n\n                if not matched_rule and os.path.exists(ctp_path):\n                     try:\n                         if ctp_path.endswith('.yaml'):\n                             import yaml\n                             with open(ctp_path, 'r') as f:\n                                 ctp_data = yaml.safe_load(f)\n                         else:\n                             import json\n                             with open(ctp_path, 'r') as f:\n                                 ctp_data = json.load(f)\n\n                         ctp_rules = ctp_data.get(\"rules\", [])\n\n                         for rule in ctp_rules:\n                             # Fuzzy matching on Manufacturer and Model\n                             # CTP rules usually have \"manufacturer\" and \"model_name\"\n                             r_man = rule.get(\"manufacturer\", \"\").lower()\n                             r_mod = rule.get(\"model_name\", \"\").lower()\n\n                             eq_man = (eq.manufacturer or \"\").lower()\n                             eq_mod = (eq.model_name or \"\").lower()\n\n                             # Simple containment check as per CTP style\n                             if r_man and r_man in eq_man and r_mod and r_mod in eq_mod:\n                                  matched_rule = rule.copy()\n                                  matched_rule[\"serial_number\"] = eq.device_serial_number\n\n                                  # Move _ctp_condition to comment if present\n                                  cond = matched_rule.pop(\"_ctp_condition\", None)\n                                  if cond:\n                                      matched_rule[\"comment\"] = f\"Auto-matched from CTP. Condition: {cond}\"\n                                  else:\n                                      matched_rule[\"comment\"] = f\"Auto-matched from CTP Knowledge Base ({rule.get('manufacturer')} {rule.get('model_name')})\"\n\n                                  break\n\n                     except Exception as e:\n                         get_logger().warning(f\"Failed to load CTP rules: {e}\")\n\n                # Secondary: Model Match (Internal KB)\n                if not matched_rule:\n                    for rule in kb_machines:\n                        if rule.get(\"model_name\") == eq.model_name:\n                             # It's a model match, so we should probably copy the zones \n                             matches_man = not rule.get(\"manufacturer\") or (rule.get(\"manufacturer\") == eq.manufacturer)\n                             if matches_man:\n                                 matched_rule = rule.copy()\n                                 matched_rule[\"serial_number\"] = eq.device_serial_number\n                                 matched_rule[\"comment\"] = f\"Auto-matched from Model Knowledge Base ({eq.model_name})\"\n                                 break\n\n                # 3.b Check for Burned In Annotations (Safety Check)\n                # query index for this machine\n                instances = service.index.get_by_machine(eq.device_serial_number)\n                burned_in_count = 0\n                for inst in instances:\n                    val = inst.attributes.get(\"0028,0301\", \"NO\")\n                    if isinstance(val, str) and \"YES\" in val.upper():\n                        burned_in_count += 1\n\n                safety_comment = \"\"\n                if burned_in_count &gt; 0:\n                    safety_comment = f\"WARNING: {burned_in_count} images have 'Burned In Annotation' flag. Verify pixel redaction.\"\n\n                if matched_rule:\n                    # Use the template\n                    rule_copy = matched_rule.copy() # Ensure we don't mutate KB\n                    if safety_comment:\n                        existing = rule_copy.get(\"comment\", \"\")\n                        rule_copy[\"comment\"] = f\"{existing} {safety_comment}\".strip()\n                    missing_configs.append(rule_copy)\n                else:\n                    # Create empty scaffold\n                    new_rule = {\n                        \"manufacturer\": eq.manufacturer or \"Unknown\",\n                        \"model_name\": eq.model_name or \"Unknown\",\n                        \"serial_number\": eq.device_serial_number,\n                        \"redaction_zones\": [] \n                    }\n                    if safety_comment:\n                        new_rule[\"comment\"] = safety_comment\n                    missing_configs.append(new_rule)\n\n        # 4. Load PHI Tags Default (if not loaded)\n        phi_tags = self.configuration.phi_tags\n        if not phi_tags:\n             # Load default config for scaffold\n             try:\n                 from .config_manager import ConfigLoader\n                 phi_tags = ConfigLoader.load_phi_config() \n             except Exception as e:\n                 get_logger().warning(f\"Failed to load research tags: {e}\")\n\n        # 4b. Enhance PHI Tags (Transform to structured defaults)\n        structured_tags = {}\n\n        # Ensure critical tags are present\n        if \"0008,0020\" not in phi_tags: phi_tags[\"0008,0020\"] = \"Study Date\"\n        if \"0010,0040\" not in phi_tags: phi_tags[\"0010,0040\"] = \"Patient Sex\"\n        if \"0010,1010\" not in phi_tags: phi_tags[\"0010,1010\"] = \"Patient Age\" # Helper\n\n        for tag, val in phi_tags.items():\n            name = val if isinstance(val, str) else val.get(\"name\", \"Unknown\")\n            action = \"REMOVE\" # Default safety\n\n            # Apply Research-Friendly Smart Defaults\n            if tag == \"0008,0020\": # Study Date\n                action = \"JITTER\"\n            elif tag == \"0010,0040\": # Sex\n                action = \"KEEP\"\n            elif tag == \"0010,1010\": # Age\n                action = \"KEEP\"\n            elif \"Date\" in name or \"Time\" in name:\n                action = \"REMOVE\" # Times are sensitive\n            elif \"ID\" in name:\n                action = \"REMOVE\" # IDs are sensitive\n\n            # Preserve existing structure if it was already structured\n            if isinstance(val, dict):\n                structured_tags[tag] = val\n            else:\n                 # Minimal Scaffold: Skip tags that are simply REMOVED (covered by Basic profile)\n                 # Unless explicitly requested to show all? For now, match tests.\n                 if action == \"REMOVE\":\n                     continue\n\n                 structured_tags[tag] = {\n                     \"name\": name,\n                     \"action\": action\n                 }\n\n        # 5. Construct Unified Data\n        data = {\n            \"version\": \"2.0\",\n            \"privacy_profile\": \"basic\",\n            # No _instructions dict anymore, we use comments!\n            \"phi_tags\": structured_tags,\n            \"date_jitter\": self.configuration.date_jitter,\n            \"remove_private_tags\": self.configuration.remove_private_tags,\n            \"machines\": missing_configs + self.configuration.rules\n        }\n\n        if not missing_configs and not self.configuration.rules:\n             print(\"No machines detected to scaffold.\")\n\n        # Pre-process data to ensure comments are single-line strings\n        # And ensure redaction_zones use FlowList for bracketed style\n        for m in data.get(\"machines\", []):\n            if \"comment\" in m and isinstance(m[\"comment\"], str):\n                # Replace newlines with spaces/semicolons\n                m[\"comment\"] = m[\"comment\"].replace(\"\\n\", \" \").replace(\"\\r\", \"\")\n                # collapse multiple spaces\n                import re\n                m[\"comment\"] = re.sub(r'\\s+', ' ', m[\"comment\"]).strip()\n\n            if \"redaction_zones\" in m and isinstance(m[\"redaction_zones\"], list):\n                # Wrap inner lists (zones) in FlowList\n                # And assume user wants [[...], [...]] so wrap outer too?\n\n                zones = m[\"redaction_zones\"]\n                new_zones = FlowList()\n                for z in zones:\n                    if isinstance(z, list):\n                        new_zones.append(FlowList(z))\n                    else:\n                        new_zones.append(z)\n                m[\"redaction_zones\"] = new_zones # Assign flow list wrapper\n\n        try:\n            # Generate YAML string\n            # sort_keys=False ensures order is preserved (machines list)\n            # width=float(\"inf\") prevents line wrapping for long strings\n            yaml_content = yaml.dump(data, sort_keys=False, default_flow_style=False, width=float(\"inf\"))\n\n            # Post-process: Convert \"comment: ...\" into \"# ...\"\n            import re\n            lines = yaml_content.splitlines()\n            new_lines = []\n            for line in lines:\n                # Simple match for key-value pair\n                match = re.search(r'^(\\s*)comment:\\s*(.*)$', line)\n                if match:\n                    indent = match.group(1)\n                    content = match.group(2).strip()\n\n                    # Check for surrounding quotes and strip them\n                    if content.startswith(\"'\") and content.endswith(\"'\"):\n                        content = content[1:-1]\n                        content = content.replace(\"''\", \"'\")\n                    elif content.startswith('\"') and content.endswith('\"'):\n                        content = content[1:-1]\n                        content = content.replace('\\\\\"', '\"')\n\n                    new_lines.append(f\"{indent}# {content}\")\n                else:\n                    if line.strip().startswith(\"- \") and len(new_lines) &gt; 0 and new_lines[-1].strip() != \"\":\n                         new_lines.append(\"\")\n\n                    new_lines.append(line)\n\n            # Prepend Header Comments\n            header = \"\"\"# Gantry Privacy Configuration (v2.0)\n# ==========================================\n#\n#\n# privacy_profile: \"basic\"\n#   - Standard profile handling common PHI (Name, ID, etc).\n#   - Set to \"none\" for manual control.\n#\n# phi_tags:\n#   - Define custom overrides here.\n#   - Actions: KEEP, REMOVE, EMPTY, REPLACE, JITTER (SHIFT)\n#\n# date_jitter:\n#   - Range of days to shift dates by (negative = into past).\n#\n# remove_private_tags:\n#   - If true, removes all odd-group tags except Gantry Metadata.\n#\n#\n\"\"\"\n            final_content = header + \"\\n\" + \"\\n\".join(new_lines) + \"\\n\"\n\n            with open(output_path, 'w') as f:\n                f.write(final_content)\n\n            get_logger().info(f\"Scaffolded Unified Config to {output_path} ({len(missing_configs)} new machines)\")\n            print(f\"Scaffolded Unified Config to {output_path}\")\n        except Exception as e:\n            get_logger().error(f\"Failed to write scaffold: {e}\")\n\n    # =========================================================================\n    # 4. AUDIT &amp; ANALYSIS\n    # =========================================================================\n\n    def audit(self, config_path: str = None) -&gt; \"PhiReport\":\n        \"\"\"\n        Scans all patients in the session for potential PHI.\n        Uses cached `active_phi_tags` if config_path matches or is None, otherwise loads fresh.\n        Returns a PhiReport object (iterable, and convertible to DataFrame).\n        Checkpoint 4: Target.\n        \"\"\"\n        from .privacy import PhiReport\n\n        # Default to current config\n        tags_to_use = self.configuration.phi_tags\n\n        if config_path:\n             try:\n                 t, r, dj, rpt = ConfigLoader.load_unified_config(config_path)\n                 tags_to_use = t\n             except:\n                 # Fallback to simple tags load\n                 tags_to_use = ConfigLoader.load_phi_config(config_path)\n\n        # Uses GantryConfiguration derived tags\n        inspector = PhiInspector(config_tags=tags_to_use, remove_private_tags=self.configuration.remove_private_tags)\n        if not inspector.phi_tags:\n            get_logger().warning(\"PHI Scan Warning: No PHI tags defined. Scan will find nothing. Check your config.\")\n\n        get_logger().info(\"Scanning for PHI (Parallel)...\")\n\n        # Hybrid Approach:\n        # Pass lightweight object CLONES to avoid \"Assert left &gt; 0\" IPC error\n        # AND to ensure we audit in-memory (unsaved) changes.\n        worker_args = []\n        for p in self.store.patients:\n            # Strip pixels to reduce size\n            light_p = self._make_lightweight_copy(p)\n            worker_args.append((light_p, tags_to_use, self.configuration.remove_private_tags))\n\n        results = run_parallel(scan_worker, worker_args, desc=\"Scanning PHI\")\n\n        all_findings = []\n        for findings in results:\n            all_findings.extend(findings)\n\n        # Rehydrate Entities!\n        self._rehydrate_findings(all_findings)\n\n        get_logger().info(f\"PHI Scan Complete. Found {len(all_findings)} issues.\")\n\n        return PhiReport(all_findings)\n\n    def get_cohort_report(self, expand_metadata: bool = False) -&gt; 'pd.DataFrame':\n        \"\"\"\n        Returns a Pandas DataFrame containing flattened metadata for the current cohort.\n        Useful for analysis and QA.\n        \"\"\"\n        import pandas as pd\n        rows = []\n        for p in self.store.patients:\n            for s in p.studies:\n                for se in s.series:\n                    manufacturer = se.equipment.manufacturer if se.equipment else \"\"\n                    model = se.equipment.model_name if se.equipment else \"\"\n                    device_serial = se.equipment.device_serial_number if se.equipment else \"\"\n\n                    for inst in se.instances:\n                        # Basic row info\n                        row = {\n                            \"PatientID\": p.patient_id,\n                            \"PatientName\": p.patient_name,\n                            \"StudyInstanceUID\": s.study_instance_uid,\n                            \"StudyDate\": s.study_date,\n                            \"SeriesInstanceUID\": se.series_instance_uid,\n                            \"Modality\": se.modality,\n                            \"SOPInstanceUID\": inst.sop_instance_uid,\n                            \"Manufacturer\": manufacturer,\n                            \"Model\": model,\n                            \"DeviceSerial\": device_serial\n                        }\n\n                        if expand_metadata and hasattr(inst, 'attributes') and inst.attributes:\n                             row.update(inst.attributes)\n\n                        rows.append(row)\n\n        return pd.DataFrame(rows)\n\n    def generate_report(self, output_path: str, format: str = \"markdown\") -&gt; None:\n        \"\"\"\n        Generates a formal Compliance Report for the current session.\n\n        Args:\n            output_path (str): Path to write the report file.\n            format (str): Output format ('markdown' or 'md'). Default is 'markdown'.\n        \"\"\"\n        get_logger().info(f\"Generating Compliance Report ({format}) to {output_path}...\")\n\n        # 1. Gather Statistics\n        n_p = len(self.store.patients)\n        n_st = sum(len(p.studies) for p in self.store.patients)\n        n_se = sum(len(st.series) for p in self.store.patients for st in p.studies)\n        n_i = sum(len(se.instances) for p in self.store.patients for st in p.studies for se in st.series)\n\n        # 2. Gather Audit Logs &amp; Exceptions\n        audit_summary = self.store_backend.get_audit_summary()\n        exceptions = self.store_backend.get_audit_errors()\n\n        # Check for unsafe attributes (BurnedInAnnotation)\n        unsafe_items = self.store_backend.check_unsafe_attributes()\n        if unsafe_items:\n            for uid, fpath, msg in unsafe_items:\n                exceptions.append((datetime.datetime.now().isoformat(), \"COMPLIANCE_CHECK\", f\"{msg} - {uid}\"))\n\n        # 3. Determine Context\n        privacy_profile = \"See Config\" \n        try:\n            from importlib.metadata import version\n            ver = version(\"gantry\")\n        except:\n            ver = \"0.0.0\"\n\n        # 4. Build Report DTO\n        report = ComplianceReport(\n            gantry_version=ver,\n            project_name=os.path.basename(self.persistence_file),\n            privacy_profile=privacy_profile,\n            total_patients=n_p,\n            total_studies=n_st,\n            total_series=n_se,\n            total_instances=n_i,\n            audit_summary=audit_summary,\n            exceptions=exceptions,\n            validation_status=\"PASS\" if audit_summary and not exceptions else \"REVIEW_REQUIRED\"\n        )\n\n        renderer = get_renderer(format)\n        renderer.render(report, output_path)\n\n    def generate_manifest(self, output_path: str, format: str = \"html\") -&gt; None:\n        \"\"\"\n        Generates a visual (HTML) or machine-readable (JSON) manifest of all instances in the session.\n\n        Args:\n            output_path (str): File path to write the manifest.\n            format (str): 'html' or 'json'.\n        \"\"\"\n        get_logger().info(f\"Generating Manifest ({format}) to {output_path}...\")\n\n        items = []\n        for p in self.store.patients:\n            for st in p.studies:\n                for se in st.series:\n                    modality = se.modality\n                    manufacturer = se.equipment.manufacturer if se.equipment else \"\"\n                    model = se.equipment.model_name if se.equipment else \"\"\n\n                    for inst in se.instances:\n                        fpath = getattr(inst, 'file_path', \"N/A\")\n\n                        item = ManifestItem(\n                            patient_id=p.patient_id,\n                            study_instance_uid=st.study_instance_uid,\n                            series_instance_uid=se.series_instance_uid,\n                            sop_instance_uid=inst.sop_instance_uid,\n                            file_path=str(fpath),\n                            modality=modality,\n                            manufacturer=manufacturer,\n                            model_name=model\n                        )\n                        items.append(item)\n\n        manifest = Manifest(\n            generated_at=datetime.datetime.now().isoformat(),\n            items=items,\n            project_name=os.path.basename(self.persistence_file)\n        )\n\n        generate_manifest_file(manifest, output_path, format)\n\n    def save_analysis(self, report):\n        \"\"\"\n        Persists the results of a PHI analysis to the database.\n        report: PhiReport or List[PhiFinding]\n        \"\"\"\n        findings = report\n        if hasattr(report, 'findings'):\n            findings = report.findings\n\n        self.store_backend.save_findings(findings)\n\n    # =========================================================================\n    # 5. PRIVACY &amp; SECURITY\n    # =========================================================================\n\n    def lock_identities(self, patient_id: str, persist: bool = False, _patient_obj: \"Patient\" = None, verbose: bool = True, **kwargs) -&gt; Union[List[\"Instance\"], LockingResult]:\n        \"\"\"\n        Securely embeds the original patient name/ID into a private DICOM tag\n        for all instances belonging to the specified patient.\n        Must be called BEFORE anonymization.\n\n        Args:\n            patient_id: The ID of the patient to preserve, OR a list/report for batch processing.\n            persist: If True, writes changes to DB immediately. If False, returns instances for batch persistence.\n            _patient_obj: Optimization argument to avoid O(N) lookup if patient is already known.\n            verbose: If True, logs debug information. Set to False for batch operations.\n            **kwargs: Additional arguments passed to lock_identities_batch (e.g. auto_persist_chunk_size).\n        \"\"\"\n        if not self.reversibility_service:\n            raise RuntimeError(\"Reversible anonymization not enabled. Call enable_reversible_anonymization() first.\")\n\n        # Dispatch to batch method if a list is provided\n        if isinstance(patient_id, (list, tuple, set)) or hasattr(patient_id, 'findings'):\n            return self.lock_identities_batch(patient_id, **kwargs)\n\n        if verbose:\n            get_logger().debug(f\"Preserving identity for {patient_id}...\")\n\n        modified_instances = []\n\n        if _patient_obj:\n            patient = _patient_obj\n        else:\n            patient = next((p for p in self.store.patients if p.patient_id == patient_id), None)\n\n        if not patient:\n            get_logger().error(f\"Patient {patient_id} not found.\")\n            return LockingResult([])\n\n        # Determine Tags to Lock (Default + Custom)\n        default_tags = [\n            \"0010,0010\", # PatientName\n            \"0010,0020\", # PatientID\n            \"0010,0030\", # PatientBirthDate\n            \"0010,0040\", # PatientSex\n            \"0008,0050\"  # AccessionNumber\n        ]\n\n        tags_to_lock = kwargs.get(\"tags_to_lock\", default_tags)\n\n        # Capture Original Values from First Instance\n        original_attrs = {}\n        first_instance = None\n\n        # Locate first instance efficiently\n        for st in patient.studies:\n            for se in st.series:\n                if se.instances:\n                    first_instance = se.instances[0]\n                    break\n            if first_instance: break\n\n        if first_instance:\n            for tag in tags_to_lock:\n                val = first_instance.attributes.get(tag)\n                if val is not None:\n                     original_attrs[tag] = val\n        else:\n             # Fallback to Patient object properties if no instances (unlikely)\n             if \"0010,0010\" in tags_to_lock: original_attrs[\"0010,0010\"] = patient.patient_name\n             if \"0010,0020\" in tags_to_lock: original_attrs[\"0010,0020\"] = patient.patient_id\n\n        cnt = 0\n\n        # Optimization: Encrypt once per patient\n        token = self.reversibility_service.generate_identity_token(original_attributes=original_attrs)\n\n        # Iterate deep\n        for st in patient.studies:\n            for se in st.series:\n                for inst in se.instances:\n                    self.reversibility_service.embed_identity_token(inst, token)\n                    modified_instances.append(inst)\n                    cnt += 1\n\n        if persist and modified_instances:\n            self.store_backend.update_attributes(modified_instances)\n            get_logger().info(f\"Secured identity (tags: {list(original_attrs.keys())}) in {cnt} instances for {patient_id}.\")\n\n        return LockingResult(modified_instances)\n\n    def lock_identities_batch(self, patient_ids: Union[List[str], \"PhiReport\", List[\"PhiFinding\"]], auto_persist_chunk_size: int = 0) -&gt; Union[List[\"Instance\"], LockingResult]:\n        \"\"\"\n        Batch process multiple patients to lock identities.\n        Returns a list of all modified instances (unless auto_persist_chunk_size is used).\n\n        Args:\n            patient_ids: List of PatientIDs, OR a PhiReport/list of objects with patient_id.\n            auto_persist_chunk_size: If &gt; 0, persists changes and releases memory every N instances.\n                                     IMPORTANT: Returns an empty list if enabled to prevent OOM.\n        \"\"\"\n        if not self.reversibility_service:\n            raise RuntimeError(\"Reversible anonymization not enabled.\")\n\n        # Normalize input to a set of strings\n        normalized_ids = set()\n\n        # Handle PhiReport or list containers\n        iterable_data = patient_ids\n        if hasattr(patient_ids, 'findings'): # PhiReport\n            iterable_data = patient_ids.findings\n\n        for item in iterable_data:\n            if isinstance(item, str):\n                normalized_ids.add(item)\n            elif hasattr(item, 'patient_id') and item.patient_id:\n                 normalized_ids.add(item.patient_id)\n\n        start_ids = list(normalized_ids)\n\n        modified_instances = [] # Only used if auto_persist_chunk_size == 0\n        current_chunk = []      # Used if auto_persist_chunk_size &gt; 0\n\n        count_patients = 0\n        count_instances_chunked = 0\n\n        from tqdm import tqdm\n\n        # Optimization: Create a lookup map for O(1) access\n        patient_map = {p.patient_id: p for p in self.store.patients}\n\n        with tqdm(start_ids, desc=\"Locking Identities\", unit=\"patient\") as pbar:\n            for pid in pbar:\n                p_obj = patient_map.get(pid)\n                if p_obj:\n                    # Use verbose=False to avoid log spam\n                    res = self.lock_identities(pid, persist=False, _patient_obj=p_obj, verbose=False)\n\n                    if auto_persist_chunk_size &gt; 0:\n                        current_chunk.extend(res)\n                        if len(current_chunk) &gt;= auto_persist_chunk_size:\n                            self.store_backend.update_attributes(current_chunk)\n                            count_instances_chunked += len(current_chunk)\n                            current_chunk = [] # Release memory\n                    else:\n                        modified_instances.extend(res)\n\n                    count_patients += 1\n                else:\n                     get_logger().error(f\"Patient {pid} not found (batch processing).\")\n\n        # Final cleanup\n        if auto_persist_chunk_size &gt; 0:\n            if current_chunk:\n                self.store_backend.update_attributes(current_chunk)\n                count_instances_chunked += len(current_chunk)\n\n            get_logger().info(f\"Batch preserved identity for {count_patients} patients ({count_instances_chunked} instances). Persisted incrementally.\")\n            return LockingResult([])\n\n        if modified_instances:\n             msg = f\"Preserved identity for {len(modified_instances)} instances.\"\n             get_logger().info(msg)\n\n        get_logger().info(f\"Batch preserved identity for {count_patients} patients ({len(modified_instances)} instances).\")\n        return LockingResult(modified_instances)\n\n    def recover_patient_identity(self, patient_id: str, restore: bool = True):\n        \"\"\"\n        Attempts to recover original identity from the encrypted token.\n\n        Args:\n            patient_id: The PatientID to recover.\n            restore: If True, applies the recovered attributes back to ALL instances in memory.\n        \"\"\"\n        if not self.reversibility_service:\n            raise RuntimeError(\"Reversibility not enabled.\")\n\n        p = next((x for x in self.store.patients if x.patient_id == patient_id), None)\n        if not p:\n            print(f\"Patient {patient_id} not found.\")\n            return\n\n        # Locate first instance to get the token\n        first_inst = None\n        for st in p.studies:\n            for se in st.series:\n                if se.instances:\n                    first_inst = se.instances[0]\n                    break\n\n        if not first_inst:\n            print(\"No instances found for patient.\")\n            return\n\n        original_attrs = self.reversibility_service.recover_original_data(first_inst)\n\n        if original_attrs:\n            if restore:\n                count = 0\n                for st in p.studies:\n                    for se in st.series:\n                        for inst in se.instances:\n                            for tag, val in original_attrs.items():\n                                inst.set_attr(tag, val)\n                            count += 1\n\n                # Update Patient Object top-level properties if Name/ID changed\n                if \"0010,0010\" in original_attrs:\n                    p.patient_name = original_attrs[\"0010,0010\"]\n                if \"0010,0020\" in original_attrs:\n                    p.patient_id = original_attrs[\"0010,0020\"]\n\n                get_logger().info(f\"Restored identity attributes to {count} instances.\")\n        else:\n            print(\"No encrypted identity token found or decryption failed.\")\n\n    def enable_reversible_anonymization(self, key_path: str = \"gantry.key\"):\n        \"\"\"\n        Initializes the encryption subsystem.\n        \"\"\"\n        self.key_manager = KeyManager(key_path)\n        self.key_manager.load_or_generate_key()\n        self.reversibility_service = ReversibilityService(self.key_manager)\n        get_logger().info(f\"Reversible anonymization enabled. Key: {key_path}\")\n\n    # =========================================================================\n    # 6. REDACTION &amp; REMEDIATION\n    # =========================================================================\n\n    def redact(self, show_progress=True):\n        \"\"\"\n        User Action: 'Apply the currently loaded rules to the pixel data.'\n        \"\"\"\n        if not self.configuration.rules:\n            get_logger().warning(\"No configuration loaded. Use .load_config() first.\")\n            print(\"No configuration loaded. Use .load_config() first.\")\n            return\n\n        service = RedactionService(self.store, self.store_backend)\n\n        try:\n            from concurrent.futures import ThreadPoolExecutor\n            import os\n\n            # Parallel Execution for Speed\n            # Threading works well here because pixel I/O and NumPy ops release GIL.\n            # Shared memory allows in-place modification of instances.\n            # OPTIMIZATION: Limited to 0.5x CPU or Max 8 to prevent OOM with large datasets\n            cpu_count = os.cpu_count() or 1\n            if os.environ.get(\"GANTRY_MAX_WORKERS\"):\n                max_workers = int(os.environ[\"GANTRY_MAX_WORKERS\"])\n            else:\n                max_workers = max(1, min(int(cpu_count * 0.5), 8))\n\n            # Generate granular tasks for better load balancing\n            all_tasks = []\n            get_logger().info(\"Analyzing workload...\")\n            for rule in self.configuration.rules:\n                tasks = service.prepare_redaction_tasks(rule)\n                all_tasks.extend(tasks)\n\n            if not all_tasks:\n                get_logger().warning(\"No matching images found for any loaded rules.\")\n                print(\"No matching images found for any loaded rules.\")\n                return\n\n            print(f\"Queued {len(all_tasks)} redaction tasks across {len(self.configuration.rules)} rules.\")\n            print(f\"Executing using {max_workers} workers (Process Isolation)...\")\n            # 2. Parallel Redaction (Granular)\n            get_logger().info(f\"Starting granular redaction ({len(all_tasks)} tasks, workers={max_workers})...\")\n\n            # Map for quick Result application (SOP -&gt; Instance)\n            instance_map = {t['instance'].sop_instance_uid: t['instance'] for t in all_tasks}\n\n            try:\n                # Use Process Isolation (Standard Pool) - Workers clean up via GC/Exit\n                # We consume generator to apply updates incrementally\n                results_gen = run_parallel(service.execute_redaction_task, all_tasks, desc=\"Redacting Pixels\", \n                                         max_workers=max_workers, \n                                         return_generator=True, chunksize=1, progress=show_progress)\n\n                for mutation in results_gen:\n                     if mutation:\n                          sop = mutation['sop_uid']\n                          if sop in instance_map:\n                               inst = instance_map[sop]\n\n                               # 1. Apply Attributes &amp; Sequences\n                               if mutation.get('attributes'):\n                                   inst.attributes.update(mutation['attributes'])\n                               if mutation.get('sequences'):\n                                   inst.sequences.update(mutation['sequences'])\n\n                               # 2. Apply Pixel Loader (Critical)\n                               # The loader acts as our handle to the sidecar data\n                               loader = mutation.get('pixel_loader')\n                               if loader:\n                                   # Fix Reference: Loader points to Worker's Instance copy.\n                                   # Re-point it to the Main Process Instance.\n                                   loader.instance = inst\n                                   inst._pixel_loader = loader\n\n                               if mutation.get('pixel_hash'):\n                                   inst._pixel_hash = mutation['pixel_hash']\n\n                               inst._dirty = True\n\n            finally:\n                pass\n\n            # Run Safety Checks\n            service.scan_burned_in_annotations()\n\n            print(\"Execution Complete. Remember to call .save() to persist.\")\n            print(\"Execution Complete. Session saved.\")\n\n        except Exception as e:\n            get_logger().error(f\"Execution interrupted: {e}\")\n            print(f\"Execution interrupted: {e}\")\n\n    def redact_by_machine(self, serial_number, roi):\n        \"\"\"\n        Helper to run redaction for a single machine interactively.\n        roi: [y1, y2, x1, x2]\n        \"\"\"\n        # This Helper is tricky. It modifies the ACTIVE configuration temporarily?\n        # Or just runs temporary logic?\n        # Original logic modified active_rules. Let's keep that behavior on our config object.\n        original = list(self.configuration.rules) # Shallow copy\n        try:\n             self.configuration.rules = [{\"serial_number\": serial_number, \"redaction_zones\": [roi]}]\n             self.redact()\n        finally:\n             self.configuration.rules = original\n\n    def anonymize(self, findings: List[PhiFinding] = None):\n        \"\"\"\n        Apply remediation Actions to the PHI Findings.\n        If findings is None, it uses the active PHI tags config to blind/remove all (Blind Execute).\n        This modifies the metadata in memory/DB.\n        \"\"\"\n        from .remediation import RemediationService\n        # Pass date jitter config to constructor\n        # Use persistence_manager.store_backend (SqliteStore) for audit logging\n        remediator = RemediationService(\n            store_backend=self.persistence_manager.store_backend, \n            date_jitter_config=self.configuration.date_jitter\n        )\n\n        count = 0\n        if findings:\n            count = remediator.apply_remediation(findings)\n        else:\n            # Blind execution (apply all rules)\n            # We need to generate findings based on current config first?\n            # Or assume RemediationService can handle blind?\n            # Actually, standard flow assumes findings.\n            tqdm_desc = \"Blind Anonymize\"\n            # Logic for blind anonymization: scan then remediate\n            # Use audit() which uses self.configuration internally now\n            current_findings = self.audit() \n            count = remediator.apply_remediation(current_findings)\n\n        get_logger().info(f\"Anonymized {count} entities.\")\n        print(f\"Anonymized/Remediated {count} tags according to policy.\")\n\n    # =========================================================================\n    # 7. EXPORT\n    # =========================================================================\n\n    def export(self, folder: str, version=None, use_compression=True, \n               check_burned_in=False, check_reversibility=True, patient_ids: List[str] = None, show_progress=True,\n               # Legacy/Test Support arguments\n               compression=None, safe=False, subset=None):\n        \"\"\"\n        Exports the current session to a directory, structured by Patient/Study/Series.\n        \"\"\"\n        import os\n        from .io_handlers import DicomExporter\n\n        # 1. Validation Checks\n        if check_reversibility and self.reversibility_service:\n            # warn if we are exporting encrypted data without warning?\n            # Actually Gantry exports exactly what is in store (which might be encrypted).\n            pass\n\n        target_ids = patient_ids\n        if target_ids is None:\n             target_ids = [p.patient_id for p in self.store.patients]\n\n        # Legacy Argument Mapping\n        if compression is not None:\n            use_compression = compression\n        if safe:\n            check_burned_in = True\n\n        # SAFETY CHECK &amp; FEEDBACK LOOP\n        # If running in safe mode, run a full scan first to give aggregated feedback\n        if check_burned_in:\n            get_logger().info(\"Performing pre-export safety scan...\")\n            findings = self.scan_for_phi()\n            if findings:\n                print(\"\\nSafety Scan Found Issues\")\n                print(\"The following tags were flagged as dirty:\")\n                print(f\"{'Tag':&lt;15} {'Description':&lt;30} {'Count':&lt;10} {'Examples'}\")\n                print(\"-\" * 80)\n\n                from collections import Counter\n                counts = Counter()\n                examples = {}\n                descriptions = {}\n\n                for f in findings:\n                    tag = f.tag or f.field_name\n                    counts[tag] += 1\n                    if tag not in examples: examples[tag] = str(f.value)\n                    descriptions[tag] = f.reason\n\n                for tag, count in counts.items():\n                    ex = examples[tag][:30]\n                    desc = descriptions[tag][:28]\n                    print(f\"{tag:&lt;15} {desc:&lt;30} {count:&lt;10} {ex}\")\n\n                print(\"\\nSuggested Config Update:\")\n                print(\"Add the following rules to your config to resolve these:\")\n                print(\"{\")\n                print('    \"phi_tags\": {')\n                rows = []\n                for tag in counts:\n                     # Attempt to infer name\n                     name = \"patient_name\" if \"0010,0010\" in tag else \"unknown_tag\"\n                     if \"0010,0020\" in tag: name = \"patient_id\"\n                     if \"0008,0020\" in tag: name = \"study_date\"\n\n                     rows.append(f'        \"{tag}\": {{ \"name\": \"{name}\", \"action\": \"REMOVE\" }}, // Found {counts[tag]} times')\n                print(\",\\n\".join(rows))\n                print('    }')\n                print(\"}\")\n\n                print('    }')\n                print(\"}\")\n\n                get_logger().warning(\"Safe Export: PHI findings detected. Proceeding to export ONLY safe instances (Skipping dirty).\")\n                # Build Dirty Filter\n                dirty_uids = set()\n                for f in findings:\n                    if f.entity_uid:\n                        dirty_uids.add(f.entity_uid)\n\n            else:\n                 dirty_uids = set()\n\n        # Subset resolution\n        allowed_uids = None\n        if subset is not None:\n            import pandas as pd\n            df = None\n            if isinstance(subset, str):\n                # Query string\n                full_df = self.get_cohort_report(expand_metadata=True)\n                try:\n                    df = full_df.query(subset)\n                except Exception as e:\n                    get_logger().error(f\"Failed to query subset '{subset}': {e}\")\n                    return\n            elif isinstance(subset, pd.DataFrame):\n                df = subset\n            elif isinstance(subset, list):\n                # Assume list of UIDs (Patient, Series, or Instance)\n                # We need to match against any level. simpler to scan.\n                # For now, let's assume if it matches PatientID, SeriesUID, or SOPUID we keep it.\n                subset_set = set(subset)\n                allowed_uids = subset_set # We will pass this to filter\n\n            if df is not None:\n                # Extract all UIDs relevant\n                allowed_uids = set()\n                # PRECISION EXPORT:\n                # If we have SOPInstanceUID, we ONLY use that to ensure we match the exact rows returned by the query.\n                # Adding PatientID would re-include ALL instances for that patient (defeating granular filters like Modality='CT').\n                if \"SOPInstanceUID\" in df.columns:\n                    allowed_uids.update(df[\"SOPInstanceUID\"].tolist())\n                # Fallbacks if SOPInstanceUID is missing (e.g. custom dataframe)\n                elif \"SeriesInstanceUID\" in df.columns:\n                    allowed_uids.update(df[\"SeriesInstanceUID\"].tolist())\n                elif \"StudyInstanceUID\" in df.columns:\n                    allowed_uids.update(df[\"StudyInstanceUID\"].tolist())\n                elif \"PatientID\" in df.columns:\n                    allowed_uids.update(df[\"PatientID\"].tolist())\n\n        get_logger().info(f\"Exporting session to: {folder}\")\n        print(\"Preparing export plan...\")\n\n        # 2. Memory Management Check\n        # Before starting a massive export (which might load pixels), ensure we save pending changes\n        # and flush memory to avoid OOM if user did a lot of redaction.\n        print(\"Saving pending changes to free memory...\")\n        self.save()\n        self.release_memory()\n\n        # 3. Create Export Plan (Lightweight objects)\n        export_tasks = []\n        total_instances = 0\n\n        count_p = 0\n        count_i = 0\n\n        # We iterate our store to build tasks.\n        # But for parallelism, we want to pass file paths or DB IDs, not full objects.\n        # DicomExporter needs (Instance -&gt; OutputPath) mapping.\n\n        # Pre-calculate paths\n        # Structure: Folder / Patient / Study / Series / Instance\n\n        # Optimization: We can generate the plan using minimal metadata\n        from .io_handlers import ExportContext\n\n        for p in self.store.patients:\n            if p.patient_id not in target_ids:\n                continue\n\n            count_p += 1\n            p_clean = \"Subject_\" + ConfigLoader.clean_filename(p.patient_id or \"UnknownPatient\")\n            p_path = os.path.join(folder, p_clean)\n\n            pat_attrs = {\n                \"0010,0010\": p.patient_name,\n                \"0010,0020\": p.patient_id\n            }\n            if hasattr(p, 'birth_date') and p.birth_date: pat_attrs[\"0010,0030\"] = p.birth_date\n            if hasattr(p, 'sex') and p.sex: pat_attrs[\"0010,0040\"] = p.sex\n\n            for st in p.studies:\n                # Hybrid Naming: Study_YYYYMMDD_Description_UIDSuffix\n                st_desc = \"Study\"\n                # Peek at first series-&gt;instance for description\n                try:\n                    if st.series and st.series[0].instances:\n                        st_desc = st.series[0].instances[0].attributes.get(\"0008,1030\", \"Study\")\n                except: pass\n\n                st_date = str(st.study_date or \"NoDate\")\n                st_uid_suffix = (st.study_instance_uid or \"Unknown\")[-5:]\n\n                st_folder_name = f\"Study_{st_date}_{st_desc}_{st_uid_suffix}\"\n                st_clean = ConfigLoader.clean_filename(st_folder_name)\n                st_path = os.path.join(p_path, st_clean)\n\n                study_attrs = {\n                    \"0020,000D\": st.study_instance_uid,\n                    \"0008,0020\": st.study_date,\n                }\n                if hasattr(st, 'study_time') and st.study_time: study_attrs[\"0008,0030\"] = st.study_time\n                if hasattr(st, 'accession_number'): study_attrs[\"0008,0050\"] = st.accession_number\n\n                for se in st.series:\n                    # Hybrid Naming: Series_NUM_Modality_Description_UIDSuffix\n                    se_desc = \"Series\"\n                    try:\n                        if se.instances:\n                            se_desc = se.instances[0].attributes.get(\"0008,103e\", \"Series\")\n                    except: pass\n\n                    se_num = str(se.series_number)\n                    se_mod = se.modality or \"OT\"\n                    se_uid_suffix = (se.series_instance_uid or \"Unknown\")[-5:]\n\n                    se_folder_name = f\"Series_{se_num}_{se_mod}_{se_desc}_{se_uid_suffix}\"\n                    se_clean = ConfigLoader.clean_filename(se_folder_name)\n                    se_path = os.path.join(st_path, se_clean)\n\n                    series_attrs = {\n                        \"0020,000E\": se.series_instance_uid,\n                        \"0008,0060\": se.modality,\n                        \"0020,0011\": str(se.series_number)\n                    }\n                    if hasattr(se, 'series_description'): series_attrs[\"0008,103E\"] = se.series_description\n\n                    for inst in se.instances:\n                        # Debug\n                        # print(f\"Checking instance {inst.sop_instance_uid}...\")\n\n                        if check_burned_in:\n                            # HIERARCHICAL SAFETY CHECK\n                            # If parent (Patient, Study, Series) is dirty, skip instance.\n                            # Also check instance itself.\n                            is_dirty = False\n                            if p.patient_id in dirty_uids: is_dirty = True\n                            elif st.study_instance_uid in dirty_uids: is_dirty = True\n                            elif se.series_instance_uid in dirty_uids: is_dirty = True\n                            elif inst.sop_instance_uid in dirty_uids: is_dirty = True\n\n                            # Fallback: Per-instance check if not already flagged dirty but inspector failed?\n                            # No, Pre-Check covered everything.\n\n                            if is_dirty:\n                                get_logger().warning(f\"Skipping unsafe instance {inst.sop_instance_uid} (Entity or Parent is Dirty).\")\n                                continue\n\n                        # Legacy Subset Filtering\n                        if allowed_uids is not None:\n                            if (inst.sop_instance_uid not in allowed_uids and \n                                se.series_instance_uid not in allowed_uids and\n                                st.study_instance_uid not in allowed_uids and\n                                p.patient_id not in allowed_uids):\n                                continue\n\n                        count_i += 1\n                        out_path = os.path.join(se_path, ConfigLoader.clean_filename(inst.sop_instance_uid)) + \".dcm\"                      \n\n                        ctx = ExportContext(\n                            instance=inst,\n                            output_path=out_path,\n                            patient_attributes=pat_attrs,\n                            study_attributes=study_attrs,\n                            series_attributes=series_attrs,\n                            compression='j2k' if use_compression else None\n                        )\n\n                        export_tasks.append(ctx)\n                        total_instances += 1\n\n        if not export_tasks:\n            get_logger().warning(\"No instances found to export.\")\n            return\n\n        print(f\"Exporting {total_instances} images from {count_p} patients...\")\n\n        # 4. Execute Export\n        # We use global run_parallel logic or specialized internal batcher?\n        # session.py line 1107 used DicomExporter.export_batch with maxtasksperchild=25\n\n        chunk_size = 500 # Report progress every N\n        show_progress = True\n\n        if total_instances &gt; 0:\n            # MEMORY LEAK MITIGATION:\n            # We use worker recycling (maxtasksperchild=100) via multiprocessing.Pool\n            # This forces workers to restart periodically, clearing any leaked memory (e.g. from C-libs).\n            # We do NOT use the shared self._executor for this, as ProcessPoolExecutor doesn't support recycling.\n            try:\n                # Optimized for stability: maxtasksperchild=25 clears memory frequently\n                # GC Optimization: Disable GC in workers\n                success_count = DicomExporter.export_batch(export_tasks, show_progress=show_progress, total=total_instances, maxtasksperchild=25, disable_gc=True)\n            except Exception as e:\n                get_logger().error(f\"Export Failed! Error: {e}\")\n                raise e\n            finally:\n                # Main process GC trigger\n                import gc\n                gc.collect()\n\n            get_logger().info(f\"Export complete.\")\n        else:\n            get_logger().warning(\"No instances queued for export.\")\n\n        print(\"Done.\")\n\n    def export_dataframe(self, output_path: str = \"export_metadata.csv\", expand_metadata: bool = False):\n        \"\"\"\n        Exports flat validation metadata to CSV or Parquet.\n        \"\"\"\n        import pandas as pd\n        df = self.get_cohort_report(expand_metadata=expand_metadata)\n\n        if output_path.endswith(\".parquet\"):\n            try:\n                # Requires pyarrow and pandas\n                df.to_parquet(output_path, index=False)\n            except Exception as e:\n                get_logger().error(f\"Failed to export parquet: {e}\")\n                raise e\n        else:\n            df.to_csv(output_path, index=False)\n\n        print(f\"Exported metadata to {output_path}\")\n        return df\n\n    def export_to_parquet(self, output_path: str, patient_ids: List[str] = None):\n        \"\"\"\n        [EXPERIMENTAL] Exports flattened metadata to a Parquet file.\n        Requires 'pandas' and 'pyarrow' or 'fastparquet'.\n        \"\"\"\n        try:\n            import pandas as pd\n        except ImportError:\n            get_logger().error(\"export_to_parquet requires 'pandas' installed.\")\n            raise ImportError(\"Please install pandas to use this feature: pip install pandas pyarrow\")\n\n        # 1. Sync DB state\n        get_logger().info(\"Saving state before Parquet export...\")\n        self.save()\n\n        # 2. Stream Data\n        get_logger().info(\"Streaming data from database...\")\n\n        target_ids = patient_ids\n        if target_ids is None:\n             target_ids = [p.patient_id for p in self.store.patients]\n\n        if not target_ids:\n            get_logger().warning(\"No patients to export.\")\n            return\n\n        generator = self.persistence_manager.store_backend.get_flattened_instances(target_ids)\n\n        rows = list(generator)\n\n        if not rows:\n            get_logger().warning(\"No instances found for these patients.\")\n            return\n\n        df = pd.DataFrame(rows)\n\n        # 3. Save\n        get_logger().info(f\"Writing {len(df)} rows to {output_path}...\")\n\n        # Ensure directory exists\n        os.makedirs(os.path.dirname(os.path.abspath(output_path)), exist_ok=True)\n\n        try:\n            df.to_parquet(output_path, index=False)\n            get_logger().info(\"Parquet export successful.\")\n        except ImportError as e:\n             get_logger().error(\"Parquet engine (pyarrow or fastparquet) missing.\")\n             raise e\n        except Exception as e:\n            get_logger().error(f\"Failed to write parquet: {e}\")\n            raise\n\n    # =========================================================================\n    # 8. INTERNAL HELPERS\n    # =========================================================================\n\n    def _rehydrate_findings(self, findings):\n        \"\"\"\n        Updates findings in-place to point to live objects in self.store\n        instead of the unpickled copies from workers.\n        \"\"\"\n        patient_map = {p.patient_id: p for p in self.store.patients}\n        study_map = {}\n        instance_map = {}\n\n        for p in self.store.patients:\n            for s in p.studies:\n                study_map[s.study_instance_uid] = s\n                for se in s.series:\n                    for i in se.instances:\n                        instance_map[i.sop_instance_uid] = i\n\n        for f in findings:\n            if f.entity_type == \"Patient\":\n                if f.entity_uid in patient_map:\n                    f.entity = patient_map[f.entity_uid]\n            elif f.entity_type == \"Study\":\n                if f.entity_uid in study_map:\n                    f.entity = study_map[f.entity_uid]\n            elif f.entity_type == \"Instance\":\n                if f.entity_uid in instance_map:\n                    f.entity = instance_map[f.entity_uid]\n\n    def _make_lightweight_copy(self, patient: \"Patient\") -&gt; \"Patient\":\n        \"\"\"\n        Creates a lightweight clone of the Patient object (and children)\n        stripped of heavy pixel data, for efficient IPC transfer.\n        Also attaches 'file_path' to instances to ensure workers can reload pixels if needed.\n        \"\"\"\n        from .entities import Patient, Study, Series, Instance\n\n        # Clone Patient\n        p_new = Patient(\n            patient_name=patient.patient_name,\n            patient_id=patient.patient_id\n        )\n\n        for s in patient.studies:\n            s_new = Study(\n                study_instance_uid=s.study_instance_uid,\n                study_date=s.study_date\n            )\n            if hasattr(s, \"date_shifted\"):\n                s_new.date_shifted = s.date_shifted\n\n            p_new.studies.append(s_new)\n\n            for se in s.series:\n                se_new = Series(\n                    series_instance_uid=se.series_instance_uid,\n                    modality=se.modality,\n                    series_number=se.series_number\n                )\n                if se.equipment:\n                     se_new.equipment = se.equipment\n                s_new.series.append(se_new)\n\n                for i in se.instances:\n                    # Clone Instance\n                    i_new = Instance(\n                        sop_instance_uid=i.sop_instance_uid,\n                        instance_number=i.instance_number,\n                        sop_class_uid=i.sop_class_uid,\n                        file_path=i.file_path\n                    )\n                    # Key: Ensure attributes are copied so workers can scan tags\n                    if hasattr(i, 'attributes'):\n                        i_new.attributes = i.attributes.copy()\n\n                    if hasattr(i, \"date_shifted\"):\n                        i_new.date_shifted = i.date_shifted\n\n                    se_new.instances.append(i_new)\n\n        return p_new\n\n    # =========================================================================\n    # 9. DEPRECATED\n    # =========================================================================\n\n    def scan_for_phi(self, config_path: str = None) -&gt; \"PhiReport\":\n        \"\"\"\n        Legacy alias for audit().\n        \"\"\"\n        return self.audit(config_path)\n</code></pre>"},{"location":"api/session/#gantry.session.DicomSession.ingest","title":"<code>ingest(directory)</code>","text":"<p>Ingests DICOM files from a directory into the session store.</p> Source code in <code>gantry/session.py</code> <pre><code>def ingest(self, directory: str):\n    \"\"\"\n    Ingests DICOM files from a directory into the session store.\n    \"\"\"\n    print(f\"Ingesting from '{directory}'...\")\n    from .io_handlers import DicomImporter\n    # Pass Sidecar Manager for eager pixel writing\n    DicomImporter.import_files([directory], self.store, executor=self._executor, sidecar_manager=self.store_backend.sidecar)\n\n    self.save(sync=True)\n\n    # Calculate stats\n    n_p = len(self.store.patients)\n    n_st = sum(len(p.studies) for p in self.store.patients)\n    n_se = sum(len(st.series) for p in self.store.patients for st in p.studies)\n    n_i = sum(len(se.instances) for p in self.store.patients for st in p.studies for se in st.series)\n\n    print(f\"Ingestion complete. Saved session state.\")\n    print(\"Summary:\")\n    print(f\"  - {n_p} Patients\")\n    print(f\"  - {n_st} Studies\")\n    print(f\"  - {n_se} Series\")\n    print(f\"  - {n_i} Instances\")\n</code></pre>"},{"location":"api/session/#gantry.session.DicomSession.save","title":"<code>save(sync=False)</code>","text":"<p>Persists the current session state to the database. :param sync: If True, blocks until save is complete.</p> Source code in <code>gantry/session.py</code> <pre><code>def save(self, sync: bool = False):\n    \"\"\"\n    Persists the current session state to the database.\n    :param sync: If True, blocks until save is complete.\n    \"\"\"\n    if sync and hasattr(self, 'store_backend'):\n         from .logger import get_logger\n         get_logger().info(\"Saving session (Synchronous)...\")\n         self.store_backend.save_all(self.store.patients)\n    elif hasattr(self, 'persistence_manager'):\n         self.persistence_manager.save_async(self.store.patients)\n</code></pre>"},{"location":"api/session/#gantry.session.DicomSession.export","title":"<code>export(folder, version=None, use_compression=True, check_burned_in=False, check_reversibility=True, patient_ids=None, show_progress=True, compression=None, safe=False, subset=None)</code>","text":"<p>Exports the current session to a directory, structured by Patient/Study/Series.</p> Source code in <code>gantry/session.py</code> <pre><code>def export(self, folder: str, version=None, use_compression=True, \n           check_burned_in=False, check_reversibility=True, patient_ids: List[str] = None, show_progress=True,\n           # Legacy/Test Support arguments\n           compression=None, safe=False, subset=None):\n    \"\"\"\n    Exports the current session to a directory, structured by Patient/Study/Series.\n    \"\"\"\n    import os\n    from .io_handlers import DicomExporter\n\n    # 1. Validation Checks\n    if check_reversibility and self.reversibility_service:\n        # warn if we are exporting encrypted data without warning?\n        # Actually Gantry exports exactly what is in store (which might be encrypted).\n        pass\n\n    target_ids = patient_ids\n    if target_ids is None:\n         target_ids = [p.patient_id for p in self.store.patients]\n\n    # Legacy Argument Mapping\n    if compression is not None:\n        use_compression = compression\n    if safe:\n        check_burned_in = True\n\n    # SAFETY CHECK &amp; FEEDBACK LOOP\n    # If running in safe mode, run a full scan first to give aggregated feedback\n    if check_burned_in:\n        get_logger().info(\"Performing pre-export safety scan...\")\n        findings = self.scan_for_phi()\n        if findings:\n            print(\"\\nSafety Scan Found Issues\")\n            print(\"The following tags were flagged as dirty:\")\n            print(f\"{'Tag':&lt;15} {'Description':&lt;30} {'Count':&lt;10} {'Examples'}\")\n            print(\"-\" * 80)\n\n            from collections import Counter\n            counts = Counter()\n            examples = {}\n            descriptions = {}\n\n            for f in findings:\n                tag = f.tag or f.field_name\n                counts[tag] += 1\n                if tag not in examples: examples[tag] = str(f.value)\n                descriptions[tag] = f.reason\n\n            for tag, count in counts.items():\n                ex = examples[tag][:30]\n                desc = descriptions[tag][:28]\n                print(f\"{tag:&lt;15} {desc:&lt;30} {count:&lt;10} {ex}\")\n\n            print(\"\\nSuggested Config Update:\")\n            print(\"Add the following rules to your config to resolve these:\")\n            print(\"{\")\n            print('    \"phi_tags\": {')\n            rows = []\n            for tag in counts:\n                 # Attempt to infer name\n                 name = \"patient_name\" if \"0010,0010\" in tag else \"unknown_tag\"\n                 if \"0010,0020\" in tag: name = \"patient_id\"\n                 if \"0008,0020\" in tag: name = \"study_date\"\n\n                 rows.append(f'        \"{tag}\": {{ \"name\": \"{name}\", \"action\": \"REMOVE\" }}, // Found {counts[tag]} times')\n            print(\",\\n\".join(rows))\n            print('    }')\n            print(\"}\")\n\n            print('    }')\n            print(\"}\")\n\n            get_logger().warning(\"Safe Export: PHI findings detected. Proceeding to export ONLY safe instances (Skipping dirty).\")\n            # Build Dirty Filter\n            dirty_uids = set()\n            for f in findings:\n                if f.entity_uid:\n                    dirty_uids.add(f.entity_uid)\n\n        else:\n             dirty_uids = set()\n\n    # Subset resolution\n    allowed_uids = None\n    if subset is not None:\n        import pandas as pd\n        df = None\n        if isinstance(subset, str):\n            # Query string\n            full_df = self.get_cohort_report(expand_metadata=True)\n            try:\n                df = full_df.query(subset)\n            except Exception as e:\n                get_logger().error(f\"Failed to query subset '{subset}': {e}\")\n                return\n        elif isinstance(subset, pd.DataFrame):\n            df = subset\n        elif isinstance(subset, list):\n            # Assume list of UIDs (Patient, Series, or Instance)\n            # We need to match against any level. simpler to scan.\n            # For now, let's assume if it matches PatientID, SeriesUID, or SOPUID we keep it.\n            subset_set = set(subset)\n            allowed_uids = subset_set # We will pass this to filter\n\n        if df is not None:\n            # Extract all UIDs relevant\n            allowed_uids = set()\n            # PRECISION EXPORT:\n            # If we have SOPInstanceUID, we ONLY use that to ensure we match the exact rows returned by the query.\n            # Adding PatientID would re-include ALL instances for that patient (defeating granular filters like Modality='CT').\n            if \"SOPInstanceUID\" in df.columns:\n                allowed_uids.update(df[\"SOPInstanceUID\"].tolist())\n            # Fallbacks if SOPInstanceUID is missing (e.g. custom dataframe)\n            elif \"SeriesInstanceUID\" in df.columns:\n                allowed_uids.update(df[\"SeriesInstanceUID\"].tolist())\n            elif \"StudyInstanceUID\" in df.columns:\n                allowed_uids.update(df[\"StudyInstanceUID\"].tolist())\n            elif \"PatientID\" in df.columns:\n                allowed_uids.update(df[\"PatientID\"].tolist())\n\n    get_logger().info(f\"Exporting session to: {folder}\")\n    print(\"Preparing export plan...\")\n\n    # 2. Memory Management Check\n    # Before starting a massive export (which might load pixels), ensure we save pending changes\n    # and flush memory to avoid OOM if user did a lot of redaction.\n    print(\"Saving pending changes to free memory...\")\n    self.save()\n    self.release_memory()\n\n    # 3. Create Export Plan (Lightweight objects)\n    export_tasks = []\n    total_instances = 0\n\n    count_p = 0\n    count_i = 0\n\n    # We iterate our store to build tasks.\n    # But for parallelism, we want to pass file paths or DB IDs, not full objects.\n    # DicomExporter needs (Instance -&gt; OutputPath) mapping.\n\n    # Pre-calculate paths\n    # Structure: Folder / Patient / Study / Series / Instance\n\n    # Optimization: We can generate the plan using minimal metadata\n    from .io_handlers import ExportContext\n\n    for p in self.store.patients:\n        if p.patient_id not in target_ids:\n            continue\n\n        count_p += 1\n        p_clean = \"Subject_\" + ConfigLoader.clean_filename(p.patient_id or \"UnknownPatient\")\n        p_path = os.path.join(folder, p_clean)\n\n        pat_attrs = {\n            \"0010,0010\": p.patient_name,\n            \"0010,0020\": p.patient_id\n        }\n        if hasattr(p, 'birth_date') and p.birth_date: pat_attrs[\"0010,0030\"] = p.birth_date\n        if hasattr(p, 'sex') and p.sex: pat_attrs[\"0010,0040\"] = p.sex\n\n        for st in p.studies:\n            # Hybrid Naming: Study_YYYYMMDD_Description_UIDSuffix\n            st_desc = \"Study\"\n            # Peek at first series-&gt;instance for description\n            try:\n                if st.series and st.series[0].instances:\n                    st_desc = st.series[0].instances[0].attributes.get(\"0008,1030\", \"Study\")\n            except: pass\n\n            st_date = str(st.study_date or \"NoDate\")\n            st_uid_suffix = (st.study_instance_uid or \"Unknown\")[-5:]\n\n            st_folder_name = f\"Study_{st_date}_{st_desc}_{st_uid_suffix}\"\n            st_clean = ConfigLoader.clean_filename(st_folder_name)\n            st_path = os.path.join(p_path, st_clean)\n\n            study_attrs = {\n                \"0020,000D\": st.study_instance_uid,\n                \"0008,0020\": st.study_date,\n            }\n            if hasattr(st, 'study_time') and st.study_time: study_attrs[\"0008,0030\"] = st.study_time\n            if hasattr(st, 'accession_number'): study_attrs[\"0008,0050\"] = st.accession_number\n\n            for se in st.series:\n                # Hybrid Naming: Series_NUM_Modality_Description_UIDSuffix\n                se_desc = \"Series\"\n                try:\n                    if se.instances:\n                        se_desc = se.instances[0].attributes.get(\"0008,103e\", \"Series\")\n                except: pass\n\n                se_num = str(se.series_number)\n                se_mod = se.modality or \"OT\"\n                se_uid_suffix = (se.series_instance_uid or \"Unknown\")[-5:]\n\n                se_folder_name = f\"Series_{se_num}_{se_mod}_{se_desc}_{se_uid_suffix}\"\n                se_clean = ConfigLoader.clean_filename(se_folder_name)\n                se_path = os.path.join(st_path, se_clean)\n\n                series_attrs = {\n                    \"0020,000E\": se.series_instance_uid,\n                    \"0008,0060\": se.modality,\n                    \"0020,0011\": str(se.series_number)\n                }\n                if hasattr(se, 'series_description'): series_attrs[\"0008,103E\"] = se.series_description\n\n                for inst in se.instances:\n                    # Debug\n                    # print(f\"Checking instance {inst.sop_instance_uid}...\")\n\n                    if check_burned_in:\n                        # HIERARCHICAL SAFETY CHECK\n                        # If parent (Patient, Study, Series) is dirty, skip instance.\n                        # Also check instance itself.\n                        is_dirty = False\n                        if p.patient_id in dirty_uids: is_dirty = True\n                        elif st.study_instance_uid in dirty_uids: is_dirty = True\n                        elif se.series_instance_uid in dirty_uids: is_dirty = True\n                        elif inst.sop_instance_uid in dirty_uids: is_dirty = True\n\n                        # Fallback: Per-instance check if not already flagged dirty but inspector failed?\n                        # No, Pre-Check covered everything.\n\n                        if is_dirty:\n                            get_logger().warning(f\"Skipping unsafe instance {inst.sop_instance_uid} (Entity or Parent is Dirty).\")\n                            continue\n\n                    # Legacy Subset Filtering\n                    if allowed_uids is not None:\n                        if (inst.sop_instance_uid not in allowed_uids and \n                            se.series_instance_uid not in allowed_uids and\n                            st.study_instance_uid not in allowed_uids and\n                            p.patient_id not in allowed_uids):\n                            continue\n\n                    count_i += 1\n                    out_path = os.path.join(se_path, ConfigLoader.clean_filename(inst.sop_instance_uid)) + \".dcm\"                      \n\n                    ctx = ExportContext(\n                        instance=inst,\n                        output_path=out_path,\n                        patient_attributes=pat_attrs,\n                        study_attributes=study_attrs,\n                        series_attributes=series_attrs,\n                        compression='j2k' if use_compression else None\n                    )\n\n                    export_tasks.append(ctx)\n                    total_instances += 1\n\n    if not export_tasks:\n        get_logger().warning(\"No instances found to export.\")\n        return\n\n    print(f\"Exporting {total_instances} images from {count_p} patients...\")\n\n    # 4. Execute Export\n    # We use global run_parallel logic or specialized internal batcher?\n    # session.py line 1107 used DicomExporter.export_batch with maxtasksperchild=25\n\n    chunk_size = 500 # Report progress every N\n    show_progress = True\n\n    if total_instances &gt; 0:\n        # MEMORY LEAK MITIGATION:\n        # We use worker recycling (maxtasksperchild=100) via multiprocessing.Pool\n        # This forces workers to restart periodically, clearing any leaked memory (e.g. from C-libs).\n        # We do NOT use the shared self._executor for this, as ProcessPoolExecutor doesn't support recycling.\n        try:\n            # Optimized for stability: maxtasksperchild=25 clears memory frequently\n            # GC Optimization: Disable GC in workers\n            success_count = DicomExporter.export_batch(export_tasks, show_progress=show_progress, total=total_instances, maxtasksperchild=25, disable_gc=True)\n        except Exception as e:\n            get_logger().error(f\"Export Failed! Error: {e}\")\n            raise e\n        finally:\n            # Main process GC trigger\n            import gc\n            gc.collect()\n\n        get_logger().info(f\"Export complete.\")\n    else:\n        get_logger().warning(\"No instances queued for export.\")\n\n    print(\"Done.\")\n</code></pre>"},{"location":"api/session/#gantry.session.DicomSession.audit","title":"<code>audit(config_path=None)</code>","text":"<p>Scans all patients in the session for potential PHI. Uses cached <code>active_phi_tags</code> if config_path matches or is None, otherwise loads fresh. Returns a PhiReport object (iterable, and convertible to DataFrame). Checkpoint 4: Target.</p> Source code in <code>gantry/session.py</code> <pre><code>def audit(self, config_path: str = None) -&gt; \"PhiReport\":\n    \"\"\"\n    Scans all patients in the session for potential PHI.\n    Uses cached `active_phi_tags` if config_path matches or is None, otherwise loads fresh.\n    Returns a PhiReport object (iterable, and convertible to DataFrame).\n    Checkpoint 4: Target.\n    \"\"\"\n    from .privacy import PhiReport\n\n    # Default to current config\n    tags_to_use = self.configuration.phi_tags\n\n    if config_path:\n         try:\n             t, r, dj, rpt = ConfigLoader.load_unified_config(config_path)\n             tags_to_use = t\n         except:\n             # Fallback to simple tags load\n             tags_to_use = ConfigLoader.load_phi_config(config_path)\n\n    # Uses GantryConfiguration derived tags\n    inspector = PhiInspector(config_tags=tags_to_use, remove_private_tags=self.configuration.remove_private_tags)\n    if not inspector.phi_tags:\n        get_logger().warning(\"PHI Scan Warning: No PHI tags defined. Scan will find nothing. Check your config.\")\n\n    get_logger().info(\"Scanning for PHI (Parallel)...\")\n\n    # Hybrid Approach:\n    # Pass lightweight object CLONES to avoid \"Assert left &gt; 0\" IPC error\n    # AND to ensure we audit in-memory (unsaved) changes.\n    worker_args = []\n    for p in self.store.patients:\n        # Strip pixels to reduce size\n        light_p = self._make_lightweight_copy(p)\n        worker_args.append((light_p, tags_to_use, self.configuration.remove_private_tags))\n\n    results = run_parallel(scan_worker, worker_args, desc=\"Scanning PHI\")\n\n    all_findings = []\n    for findings in results:\n        all_findings.extend(findings)\n\n    # Rehydrate Entities!\n    self._rehydrate_findings(all_findings)\n\n    get_logger().info(f\"PHI Scan Complete. Found {len(all_findings)} issues.\")\n\n    return PhiReport(all_findings)\n</code></pre>"},{"location":"api/session/#gantry.session.DicomSession.redact","title":"<code>redact(show_progress=True)</code>","text":"<p>User Action: 'Apply the currently loaded rules to the pixel data.'</p> Source code in <code>gantry/session.py</code> <pre><code>def redact(self, show_progress=True):\n    \"\"\"\n    User Action: 'Apply the currently loaded rules to the pixel data.'\n    \"\"\"\n    if not self.configuration.rules:\n        get_logger().warning(\"No configuration loaded. Use .load_config() first.\")\n        print(\"No configuration loaded. Use .load_config() first.\")\n        return\n\n    service = RedactionService(self.store, self.store_backend)\n\n    try:\n        from concurrent.futures import ThreadPoolExecutor\n        import os\n\n        # Parallel Execution for Speed\n        # Threading works well here because pixel I/O and NumPy ops release GIL.\n        # Shared memory allows in-place modification of instances.\n        # OPTIMIZATION: Limited to 0.5x CPU or Max 8 to prevent OOM with large datasets\n        cpu_count = os.cpu_count() or 1\n        if os.environ.get(\"GANTRY_MAX_WORKERS\"):\n            max_workers = int(os.environ[\"GANTRY_MAX_WORKERS\"])\n        else:\n            max_workers = max(1, min(int(cpu_count * 0.5), 8))\n\n        # Generate granular tasks for better load balancing\n        all_tasks = []\n        get_logger().info(\"Analyzing workload...\")\n        for rule in self.configuration.rules:\n            tasks = service.prepare_redaction_tasks(rule)\n            all_tasks.extend(tasks)\n\n        if not all_tasks:\n            get_logger().warning(\"No matching images found for any loaded rules.\")\n            print(\"No matching images found for any loaded rules.\")\n            return\n\n        print(f\"Queued {len(all_tasks)} redaction tasks across {len(self.configuration.rules)} rules.\")\n        print(f\"Executing using {max_workers} workers (Process Isolation)...\")\n        # 2. Parallel Redaction (Granular)\n        get_logger().info(f\"Starting granular redaction ({len(all_tasks)} tasks, workers={max_workers})...\")\n\n        # Map for quick Result application (SOP -&gt; Instance)\n        instance_map = {t['instance'].sop_instance_uid: t['instance'] for t in all_tasks}\n\n        try:\n            # Use Process Isolation (Standard Pool) - Workers clean up via GC/Exit\n            # We consume generator to apply updates incrementally\n            results_gen = run_parallel(service.execute_redaction_task, all_tasks, desc=\"Redacting Pixels\", \n                                     max_workers=max_workers, \n                                     return_generator=True, chunksize=1, progress=show_progress)\n\n            for mutation in results_gen:\n                 if mutation:\n                      sop = mutation['sop_uid']\n                      if sop in instance_map:\n                           inst = instance_map[sop]\n\n                           # 1. Apply Attributes &amp; Sequences\n                           if mutation.get('attributes'):\n                               inst.attributes.update(mutation['attributes'])\n                           if mutation.get('sequences'):\n                               inst.sequences.update(mutation['sequences'])\n\n                           # 2. Apply Pixel Loader (Critical)\n                           # The loader acts as our handle to the sidecar data\n                           loader = mutation.get('pixel_loader')\n                           if loader:\n                               # Fix Reference: Loader points to Worker's Instance copy.\n                               # Re-point it to the Main Process Instance.\n                               loader.instance = inst\n                               inst._pixel_loader = loader\n\n                           if mutation.get('pixel_hash'):\n                               inst._pixel_hash = mutation['pixel_hash']\n\n                           inst._dirty = True\n\n        finally:\n            pass\n\n        # Run Safety Checks\n        service.scan_burned_in_annotations()\n\n        print(\"Execution Complete. Remember to call .save() to persist.\")\n        print(\"Execution Complete. Session saved.\")\n\n    except Exception as e:\n        get_logger().error(f\"Execution interrupted: {e}\")\n        print(f\"Execution interrupted: {e}\")\n</code></pre>"},{"location":"api/session/#gantry.session.DicomSession.load_config","title":"<code>load_config(config_file)</code>","text":"<p>User Action: 'Load these rules into memory, but DO NOT run them yet.' Useful for validation or previewing what will happen.</p> Source code in <code>gantry/session.py</code> <pre><code>def load_config(self, config_file: str):\n    \"\"\"\n    User Action: 'Load these rules into memory, but DO NOT run them yet.'\n    Useful for validation or previewing what will happen.\n    \"\"\"\n    try:\n        get_logger().info(f\"Loading configuration from {config_file}...\")\n        print(f\"Loading configuration from {config_file}...\")\n\n        # UNIFIED LOAD (v2) - Now loading into GantryConfiguration object\n        tags, rules, jitter, remove_private = ConfigLoader.load_unified_config(config_file)\n\n        # Update the configuration object\n        self.configuration.phi_tags = tags\n        self.configuration.rules = rules\n        self.configuration.date_jitter = jitter\n        self.configuration.remove_private_tags = remove_private\n\n        get_logger().info(f\"Loaded {len(self.configuration.rules)} machine rules and {len(self.configuration.phi_tags)} PHI tags.\")\n        print(f\"Configuration Loaded:\\n - {len(self.configuration.rules)} Machine Redaction Rules\\n - {len(self.configuration.phi_tags)} PHI Tags\")\n        print(f\" - Date Jitter: {self.configuration.date_jitter['min_days']} to {self.configuration.date_jitter['max_days']} days\")\n        print(f\" - Remove Private Tags: {self.configuration.remove_private_tags}\")\n        print(\"Tip: Run .audit() to check PHI, or .redact_pixels() to apply redaction.\")\n    except Exception as e:\n        import traceback\n        get_logger().error(f\"Load failed: {e}\")\n        print(f\"Load failed: {e}\")\n        print(traceback.format_exc())\n        # Reset on failure? OR keep previous? \n        # Original behavior was reset.\n        self.configuration.rules = []\n        self.configuration.phi_tags = {}\n</code></pre>"}]}
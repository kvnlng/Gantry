{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Gantry","text":"<p>A Python DICOM Object Model and Redaction Toolkit.</p> <p></p> <p>Gantry provides a high-performance, object-oriented interface for managing, analyzing, and de-identifying DICOM datasets. It is designed for large-scale ingestion, precise pixel redaction, and strict PHI compliance.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Object-Oriented API: Work with <code>Patient</code>, <code>Study</code>, <code>Series</code>, and <code>Instance</code> objects directly.</li> <li>Persistent Sessions: All metadata is indexed in a SQLite database, allowing you to pause/resume large jobs and providing an audit trail.</li> <li>Parallel Processing: Multi-process ingestion and export for maximum throughput.</li> <li>Robust Redaction:</li> <li>Metadata: Configurable tag removal, replacement, and shifting.</li> <li>Pixel Data: Machine-specific redaction zones (ROI) to scrub burned-in PHI.</li> <li>Reversibility: Optional cryptographic identity preservation.</li> <li>Codecs: Robust support for JPEG Lossless, JPEG 2000, and other compressed formats via <code>imagecodecs</code>.</li> <li>Free-threaded Python Ready: Fully compatible with Python 3.13t+ (no-GIL) for true parallelism.</li> <li>Deep Memory Management: Automatic pixel offloading allows processing datasets far exceeding available RAM.</li> </ul>"},{"location":"architecture/","title":"Architecture","text":"<p>Gantry acts as a smart indexing layer over your raw DICOM files. It does not modify your original data. Instead, it builds a lightweight metadata index (SQLite) and exposes a clean Python Object Model for manipulation.</p>"},{"location":"architecture/#1-the-session-facade","title":"1. The Session Facade","text":"<p>The <code>Session</code> object is your single entry point. It manages:</p> <ul> <li>Persistence: Auto-saving state to <code>gantry.db</code>.</li> <li>Inventory: Tracking Patients, Studies, and Series.</li> <li>Transactions: Atomic persistence of changes.</li> </ul>"},{"location":"architecture/#2-object-model","title":"2. Object Model","text":"<p>Gantry abstracts DICOM into a semantic hierarchy, removing the pain of manual tag iteration.</p> <pre><code>graph LR\n    Patient --&gt; Study\n    Study --&gt; Series\n    Series --&gt; Instance\n    Instance --&gt; Pixels((Pixel Data))</code></pre> <ul> <li>Patient: Root entity (Name, ID).</li> <li>Study: A distinct visit/exam.</li> <li>Series: A scan or reconstruction (e.g., \"ct_soft_kernel\").</li> <li>Instance: A single DICOM slice. Pixel data is lazy-loaded; the 500MB+ pixel array is only read from disk when you access <code>.pixel_array</code> or export.</li> </ul>"},{"location":"architecture/#3-safety-pipeline-the-8-checkpoints","title":"3. Safety Pipeline (The 8 Checkpoints)","text":"<p>Gantry enforces a strict checkpoint system to ensure data safety:</p> <ol> <li>Ingest: Load raw data into the managed session index.</li> <li>Examine: Inventory the cohort and equipment.</li> <li>Configure: Define privacy tags and redaction rules.</li> <li>Audit (Target): Measure PHI risks against the configuration.</li> <li>Backup: (Optional) Securely lock original identities for reversibility.</li> <li>Anonymize: Apply remediation to metadata (in-memory).</li> <li>Redact: Scrub pixel data for specific machines (in-memory).</li> <li>Verify: Re-audit the session to ensure a clean state.</li> <li>Export: Write clean DICOM files to disk.</li> </ol>"},{"location":"changelog/","title":"Changelog","text":"<p>All notable changes to the \"Gantry\" project will be documented in this file.</p> <p>The format is based on Keep a Changelog, and this project adheres to Semantic Versioning.</p>"},{"location":"changelog/#051-2025-12-31","title":"[0.5.1] - 2025-12-31","text":""},{"location":"changelog/#added","title":"Added","text":"<ul> <li>Python 3.13t+ Support: Full compatibility with Free-threaded Python (no-GIL).</li> <li>Benchmarks: Documented performance achieving ~770k instances/sec for metadata operations.</li> <li>Migration Tools: Added <code>gantry.utils.ctp_parser</code> to convert legacy CTP scripts to Gantry YAML.</li> </ul>"},{"location":"changelog/#changed","title":"Changed","text":"<ul> <li>Dependencies: Merged <code>[images]</code> extra into core install. Gantry now installs <code>pillow</code> and <code>imagecodecs</code> by default.</li> <li>Documentation: Complete rewrite of <code>README.md</code> to reflect v2.0 Architecture.</li> </ul>"},{"location":"changelog/#fixed","title":"Fixed","text":"<ul> <li>Decompression: Robust support for encapsulated Multi-Frame images and JPEG Lossless (Process 14) via <code>imagecodecs</code>.</li> <li>Robustness: Implemented automatic fallback to installed codecs if standard <code>pydicom</code> handler discovery fails (e.g. environment path issues).</li> <li>Handling: Fixed <code>UnboundLocalError</code> regressions in error reporting.</li> <li>Correctness: Fixed bug where encapsulated pixel data was passed incorrectly to decoders.</li> </ul>"},{"location":"changelog/#050-2025-12-18","title":"[0.5.0] - 2025-12-18","text":""},{"location":"changelog/#added_1","title":"Added","text":"<ul> <li>Performance:</li> <li>Split-Persistence: Introduced a binary sidecar (<code>_pixels.bin</code>) for high-speed append-only pixel storage, reducing SQLite metadata size by 99%+.</li> <li>Database Indexing: Added indexes to Foreign Keys (<code>patient_id_fk</code>, etc.) and <code>audit_log</code> for O(1) query performance.</li> <li>Multithreaded Redaction: <code>redact_pixels</code> now uses <code>ThreadPoolExecutor</code> to process Machine Rules in parallel, achieving near-linear speedup on multi-core systems.</li> <li>Optimization:</li> <li>Inverted Redaction Loop: Refactored logic to iterate images once per machine (O(M)) instead of applying every rule to every image (O(NM)).</li> <li>Empty Zone Skipping: Automatically skips processing machines with no configured ROIs.</li> <li>Benchmarks:</li> <li>Verified throughput of 140,000 metadata inserts/sec and 580 MB/s pixel writes in stress tests.</li> <li>UX:</li> <li>Added realtime <code>tqdm</code> progress bars for redaction.</li> </ul>"},{"location":"changelog/#fixed_1","title":"Fixed","text":"<ul> <li>Multiprocessing: Fixed \"Pickling Error\" on Windows/spawn start methods by creating lightweight copies of the object graph for worker communication.</li> <li>Redaction: Fixed crash when <code>get_pixel_data</code> returns <code>None</code> (missing file).</li> <li>Redaction: Fixed \"Completely Outside\" warning logic for RGB images (interpreting Channels as Columns).</li> </ul>"},{"location":"changelog/#053-2026-01-13","title":"[0.5.3] - 2026-01-13","text":""},{"location":"changelog/#fixed_2","title":"Fixed","text":"<ul> <li>Free-Threaded Stability: Fixed a race condition in <code>PersistenceManager</code> during shutdown that caused data loss in no-GIL environments (Python 3.13t+).</li> <li>Export Reliability: Fixed a \"Pickling Error\" regression in <code>run_parallel</code> when using <code>maxtasksperchild</code> with memory leak mitigation.</li> <li>Export Safety: Enforced strict exception raising in export workers; failed decompression now correctly fails the export instead of failing silently.</li> <li>Testing: Resolved <code>MagicMock</code> serialization errors during tests ensuring test suite passes cleanly on all platforms.</li> <li>Debug Cleanup: Removed residual debug output from Sidecar pixel loading and Benchmark stress tests.</li> </ul>"},{"location":"changelog/#changed_1","title":"Changed","text":"<ul> <li>Dependencies: Bumping version for maintenance release.</li> </ul>"},{"location":"changelog/#052-2026-01-08","title":"[0.5.2] - 2026-01-08","text":""},{"location":"changelog/#added_2","title":"Added","text":"<ul> <li>Free-Threaded Stability: Implemented Versioned Dirty Tracking in <code>DicomItem</code> to correctly handle concurrent modifications in no-GIL environments (Python 3.13t+).</li> <li>Memory Optimization: Implemented <code>Instance.unload_pixel_data()</code> and automatic pixel swapping to <code>_pixels.bin</code>. This allows the session to process datasets larger than available RAM by offloading modified pixels to disk.</li> <li>Global Export Parallelism: Export process now utilizes a global pool of workers across all patients, significantly improving throughput for datasets with many small studies.</li> <li>Async Audit Queue: Implemented an asynchronous queue for writing audit logs to SQLite, preventing database locking and contention during highly parallel operations.</li> <li>Redaction Progress UI: Consolidated multiple per-machine progress bars into a single, clean \"Redacting Rules\" indicator.</li> <li>Verbose Logging: Added <code>verbose</code> flag to Redaction Service methods to allow optional debugging of missing pixels/rules.</li> </ul>"},{"location":"changelog/#changed_2","title":"Changed","text":"<ul> <li>Removed Legacy Config: Dropped support for legacy list-based configuration files and internal list-parsing logic. Configuration must now be the standard Unified YAML format.</li> <li>Thread Tuning: Adjusted default parallel worker count to <code>1.5 * CPU_CORES</code> (previously <code>min(32, cpu+4)</code>).</li> <li>Warning Suppression: Redaction warnings (e.g., missing pixel data) are now suppressed by default to reduce console noise.</li> <li>Redaction Execution: Switched <code>redact()</code> to enforce threading (<code>force_threads=True</code>) to correctly handle in-memory state updates and avoid pickling errors with SQLite connections.</li> </ul>"},{"location":"changelog/#fixed_3","title":"Fixed","text":"<ul> <li>Persistence Race Condition: Fixed a critical race condition where modifications made during an asynchronous save operation were lost/overwritten.</li> <li>Memory Leak: Resolved memory accumulation in <code>lock_identities</code> by implementing batch chunking (<code>auto_persist_chunk_size</code>).</li> <li>Progress Reporting: Fixed broken/instant completion progress bars in <code>lock_identities</code>.</li> <li>Logging Regression: Fixed assertion failure in <code>test_full_logging_coverage</code> regarding suppressed log messages.</li> <li>NameError: Fixed a variable scoping issue in <code>RedactionService.process_machine_rules</code>.</li> <li>Parallel Redaction Bugs: Resolved <code>pickle</code> errors and state synchronization issues in parallel redaction by enforcing threading.</li> </ul>"},{"location":"changelog/#041-2025-12-12","title":"[0.4.1] - 2025-12-12","text":""},{"location":"changelog/#added_3","title":"Added","text":"<ul> <li>Configuration Actions: Support for <code>REMOVE</code> and <code>EMPTY</code> actions in <code>privacy_config.json</code> for precise tag handling.</li> <li>Ingest Summary: <code>ingest</code> command now provides a detailed count of imported objects.</li> </ul>"},{"location":"changelog/#fixed_4","title":"Fixed","text":"<ul> <li>Persistence Priority: Fixed \"Split Brain\" issue where remediated <code>Study</code>/<code>Series</code> metadata was overwritten by original file attributes during export.</li> <li>Export Error: Fixed validation strictness to allow export of files with stripped Command Set (Group 0000) tags.</li> <li>API Consistency: Unified <code>scan_for_phi</code> and <code>audit</code> methods.</li> </ul>"},{"location":"changelog/#040-2025-12-11","title":"[0.4.0] - 2025-12-11","text":""},{"location":"changelog/#added_4","title":"Added","text":"<ul> <li>Features:</li> <li>Safe Export: New <code>export(safe=True)</code> mode ensuring no PHI leaves the system.</li> <li>Reversible Anonymization: Securely embed encrypted original identities (<code>gantry.key</code>).</li> <li>Manual Persistence: Changed default behavior to manual <code>.save()</code> for better user control.</li> <li>Background Persistence: Non-blocking saves via <code>PersistenceManager</code>.</li> <li>PHI Analysis Reports: <code>scan_for_phi</code> now returns a rich <code>PhiReport</code> object with Pandas DataFrame support.</li> <li>Parallel Processing: Multi-process support for Import and PHI Scanning.</li> <li>Improvements:</li> <li>Console Output: Suppressed noisy <code>pydicom</code> warnings and improved <code>tqdm</code> progress bars.</li> <li>Batch UX: Better feedback during long-running operations.</li> <li>Test Coverage: specific tests for <code>crypto</code>, <code>config</code>, and <code>safe_export</code>.</li> </ul>"},{"location":"changelog/#fixed_5","title":"Fixed","text":"<ul> <li>Regression: Addressed silent failure in pixel export when source files are missing.</li> <li>Bug: Fixed <code>TypeError</code> in Remediation Date Shifting.</li> <li>Bug: Fixed <code>MultiValue</code> JSON serialization error in persistence.</li> <li>Bug: Fixed <code>ValueError</code> regarding Group 0000 elements during export.</li> </ul>"},{"location":"changelog/#030-2025-12-11","title":"[0.3.0] - 2025-12-11","text":""},{"location":"changelog/#added_5","title":"Added","text":"<ul> <li>Robust Persistence (SQLite): Replaced <code>Pickle</code> with <code>SQLite</code> for session storage (<code>gantry.db</code>). Allows for scale and external querying.</li> <li>Audit Trail: Implemented a comprehensive audit system. Actions such as <code>Redaction</code> and <code>Remediation</code> are now logged to the <code>audit_log</code> table in the database.</li> <li>Automated PHI Remediation:</li> <li>Metadata Anonymization: Automatically detects and anonymizes Patient Names and IDs.</li> <li>Deterministic Date Shifting: Shifts study dates by a consistent offset (based on Patient ID hash) to preserve temporal relationships while obscuring actual dates.</li> <li><code>apply_remediation</code> API: Added top-level API to <code>DicomSession</code> to easily apply fixes found by the privacy inspector.</li> <li>Documentation: Significant updates to <code>README.md</code> and architecture documentation.</li> </ul>"},{"location":"changelog/#changed_3","title":"Changed","text":"<ul> <li>Breaking Change: The internal persistence format has changed from <code>.pkl</code> to <code>.db</code>. Existing sessions from v0.2.0 cannot be loaded and must be re-imported.</li> <li>Dependency Update: Added <code>sqlite3</code> (stdlib) as a core dependency for the store backend.</li> </ul>"},{"location":"changelog/#020-2025-12-10","title":"[0.2.0] - 2025-12-10","text":""},{"location":"changelog/#added_6","title":"Added","text":"<ul> <li>JSON Configuration Validation: <code>ConfigLoader</code> now rejects rules with missing fields or invalid/illegal ROI definitions.</li> <li>ROI Safety Checks: Redaction operations now explicitly check image bounds, clipping ROIs to the image dimensions and warning if they are completely out of bounds.</li> <li>File Deduplication: <code>DicomImporter</code> now detects and skips files that have already been imported into the current session.</li> </ul>"},{"location":"changelog/#fixed_6","title":"Fixed","text":"<ul> <li>Recursive Sequence Import: Nested sequences (e.g., in Structured Reports) are now correctly recursed and indexed.</li> <li>Pixel Depth Export: <code>DicomExporter</code> now correctly preserves 8-bit usage for relevant modalities (e.g., US, SC) instead of hardcoding 12/16-bit depth.</li> </ul>"},{"location":"changelog/#010-2025-12-09","title":"[0.1.0] - 2025-12-09","text":""},{"location":"changelog/#added_7","title":"Added","text":"<ul> <li>Core Architecture: Implemented the semantic object graph (<code>Patient</code> \u2192 <code>Study</code> \u2192 <code>Series</code> \u2192 <code>Instance</code>) to replace flat dictionary handling.</li> <li>Facade Interface: Added <code>gantry.Session</code> class as the primary entry point for user interaction, managing imports, persistence, and inventory.</li> <li>Lazy Loading: Implemented a Proxy Pattern for <code>Instance</code> objects. Metadata is loaded into memory during import, while heavy pixel data is read from disk only upon request.</li> <li>De-Identification Service: Added <code>RedactionService</code> to modify pixel data (burn-in removal) based on specific machine serial numbers.</li> <li>Configuration Management: Added support for <code>redaction_rules.json</code> to define Redaction Regions of Interest (ROIs) externally.</li> <li>Machine Indexing: Created <code>MachinePixelIndex</code> to efficiently group and retrieve instances by their Equipment attributes (Manufacturer, Model, Serial Number).</li> <li>Builder Pattern: Added <code>DicomBuilder</code> (and fluent sub-builders) to allow programmatic construction of complex DICOM hierarchies for testing and synthetic data generation.</li> <li>IOD Validation: Implemented <code>IODValidator</code> to enforce Type 1 and Type 2 attribute compliance for standard SOP Classes (e.g., CT Image Storage) before export.</li> <li>Persistence: Added <code>pickle</code>-based serialization to save and resume session state (<code>DicomStore</code>).</li> <li>Import/Export: Created <code>DicomImporter</code> for fast metadata scanning and <code>DicomExporter</code> for writing valid, standards-compliant <code>.dcm</code> files.</li> </ul>"},{"location":"changelog/#security","title":"Security","text":"<ul> <li>Pixel data redaction is performed in-memory and committed to new files; original files are treated as read-only during the session to prevent accidental data loss.</li> </ul>"},{"location":"configuration/","title":"Configuration","text":"<p>Gantry uses a Unified YAML Configuration to control all aspects of de-identification.</p>"},{"location":"configuration/#example-configyaml","title":"Example <code>config.yaml</code>","text":"<pre><code># 1. Privacy Profile (Optional)\n# Defines the baseline set of tags to remove/clean.\n# Options:\n#   - \"basic\": DICOM PS3.15 Annex E Basic Profile (Partial De-Id)\n#   - \"comprehensive\": Full De-Identification (Most conservative)\n#   - \"/path/to/profile.yaml\": Load a custom set of rules from an external file\nprivacy_profile: \"basic\"\n\n# 2. Date Jitter\n# Shift all dates by a random amount within this range (consistent per Patient).\ndate_jitter:\n  min_days: -30\n  max_days: -10\n\n# 3. Private Tags\n# Whether to remove all private dicom tags (odd groups).\nremove_private_tags: true\n\n# 4. Custom PHI Tags (Overrides Profile)\nphi_tags:\n  \"0010,0010\": { \"action\": \"REMOVE\", \"name\": \"PatientName\" }\n  \"0010,0020\": { \"action\": \"REPLACE\", \"name\": \"PatientID\", \"value\": \"ANON_{id}\" }\n\n# 5. Pixel Redaction Rules (Machine Specific)\nmachines:\n  - serial_number: \"DEV12345\"\n    model_name: \"UltraSound Pro\"\n    redaction_zones:\n      - [0, 50, 0, 800] # ROI: [row_start, row_end, col_start, col_end]\n</code></pre>"},{"location":"configuration/#advanced-features","title":"Advanced Features","text":""},{"location":"configuration/#pixel-redaction","title":"Pixel Redaction","text":"<p>Gantry can scrub burned-in PHI from pixels based on matching the equipment's <code>DeviceSerialNumber</code>. Define <code>redaction_zones</code> in your config to automatically verify and scrub these regions during export/anonymization.</p>"},{"location":"configuration/#reversible-anonymization","title":"Reversible Anonymization","text":"<p>To maintain a secure link back to the original identity:</p> <pre><code># Enable encryption (generates 'gantry.key')\nsession.enable_reversible_anonymization()\n\n# Lock identities BEFORE anonymization to store encrypted original data\nsession.lock_identities(\"PATIENT_123\")\n</code></pre> <p>Users can later recover the identity if they possess the correct key:</p> <pre><code>session.recover_patient_identity(\"ANON_123\")\n</code></pre>"},{"location":"configuration/#strict-codec-export-safety","title":"Strict Codec &amp; Export Safety","text":"<p>Gantry performs strict validation during export. If a compressed image cannot be decompressed (e.g., due to missing codecs or corruption), the export will fail rather than passing through unverified data. This ensures 100% PHI safety.</p> <p>Supported Transfer Syntaxes:</p> <ul> <li>JPEG Lossless (Process 14, SV1)</li> <li>JPEG 2000 (Lossless &amp; Lossy)</li> <li>JPEG-LS</li> <li>RLE Lossless</li> <li>Standard JPEG Baseline/Extended</li> </ul>"},{"location":"installation/","title":"Installation","text":"<p>Gantry requires Python 3.9+.</p> <pre><code># Clone the repository\ngit clone https://github.com/kvnlng/Gantry.git\ncd Gantry\n\n# Install with dependencies (including codecs)\npip install -e .\n</code></pre> <p>Note</p> <p>The <code>imagecodecs</code> dependency is included and strongly recommended for handling JPEG Lossless and other compressed Transfer Syntaxes.</p>"},{"location":"installation/#system-requirements","title":"System Requirements","text":"<p>Gantry's parallel processing engine is designed to maximize CPU utilization. However, heavy operations like JPEG 2000 compression require significant memory per worker.</p> <ul> <li>Memory: Gantry is memory-intensive during specific operations (e.g., Pixel Redaction, J2K Export).</li> <li>Minimum: 2GB RAM per vCPU.</li> <li>Recommended (Heavy Workloads): 8GB RAM per vCPU (e.g., for massive multi-frame J2K compression).</li> <li>Concurrency: By default, Gantry uses all available cores (<code>1:1</code> ratio). Use <code>GANTRY_MAX_WORKERS</code> env var to limit this if OOM occurs.</li> </ul>"},{"location":"migration/","title":"Migration Tools","text":""},{"location":"migration/#clinical-trial-processor-ctp","title":"Clinical Trial Processor (CTP)","text":"<p>Gantry includes a utility to convert legacy CTP <code>DicomPixelAnonymizer.script</code> files into Gantry's YAML configuration format.</p> <pre><code># Convert CTP script to Gantry YAML\npython -m gantry.utils.ctp_parser /path/to/anonymizer.script output_rules.yaml\n</code></pre> <p>This parser extracts:</p> <ul> <li>Manufacturer/Model matching criteria.</li> <li>Redaction zones (automatically converting <code>x,y,w,h</code> to <code>r1,r2,c1,c2</code>).</li> </ul>"},{"location":"performance/","title":"Performance","text":"<p>Gantry is designed for massive scale. Recent stress tests verify robust linear scaling on datasets up to 100GB.</p> <p></p>"},{"location":"performance/#100gb-scalability-test","title":"100GB Scalability Test","text":"<ul> <li>Input: 101,000 files (50GB Single-Frame + 50GB Multi-Frame).</li> <li>Import Speed: ~14 seconds (Index-only ingestion).</li> <li>Export Speed: ~79 seconds (Streaming Write).</li> <li>Memory: Peaks at 5.4GB, stable regardless of dataset size.</li> </ul> <p>The architecture uses O(1) memory streaming, ensuring it never runs out of RAM even when processing terabytes of data.</p>"},{"location":"performance/#micro-benchmarks-metadata-operations","title":"Micro-Benchmarks (Metadata Operations)","text":"Operation Scale Time (Mac M3 Max) Throughput Identity Locking 100,000 Instances ~0.13 s 769k / sec Persist Findings 100,000 Issues ~0.13 s 770k / sec"},{"location":"quickstart/","title":"Quick Start","text":""},{"location":"quickstart/#1-initialize-a-session","title":"1. Initialize a Session","text":"<p>Gantry uses a persistent session to manage your workflow. Unlike scripts that run once and forget, a Session creates a local SQLite database (<code>gantry.db</code>) to index your data. This allows you to pause, resume, and audit your work without re-scanning thousands of files.</p> <pre><code>from gantry import Session\n\n# Initialize a new session (creates 'gantry.db' by default)\nsession = Session(\"my_project.db\")\n</code></pre>"},{"location":"quickstart/#2-ingest-examine","title":"2. Ingest &amp; Examine","text":"<p>Ingestion builds a lightweight metadata index of your DICOM files. Gantry scans your folders recursively, extracting patient/study/series information into the database without moving or modifying your original files. It is resilient to nested directories and non-DICOM clutter.</p> <pre><code>session.ingest(\"/path/to/dicom/data\")\nsession.save() # Persist the index to disk\n\n# Print a summary of the cohort and equipment\nsession.examine()\n</code></pre>"},{"location":"quickstart/#3-configure-audit","title":"3. Configure &amp; Audit","text":"<p>Before changing anything, define your privacy rules. Use <code>create_config</code> to generate a scaffolding based on your inventory, then <code>audit</code> to scan that inventory against your rules. This \"Measure Twice, Cut Once\" approach lets you identify all PHI risks before applying any irreversible changes.</p> <pre><code># Create a default configuration file (v2.0 YAML)\nsession.create_config(\"config.yaml\")\n\n# Load the configuration (rules, tags, jitter)\nsession.load_config(\"config.yaml\")\n\n# Run an audit to find PHI\nreport = session.audit() \nsession.save_analysis(report)\n\nprint(f\"Found {len(report)} potential PHI issues.\")\n</code></pre>"},{"location":"quickstart/#4-backup-identity-optional","title":"4. Backup Identity (Optional)","text":"<p>To enable reversible anonymization, generate a cryptographic key and \"lock\" the original patient identities into a secure, encrypted DICOM tag. This must be done before anonymization.</p> <pre><code># Enable encryption (generates 'gantry.key')\nsession.enable_reversible_anonymization()\n\n# cryptographically lock identities for all patients found in the audit\nsession.lock_identities(report)\nsession.save()\n</code></pre>"},{"location":"quickstart/#5-anonymize-redact-export","title":"5. Anonymize, Redact &amp; Export","text":"<p>Remediation is a multi-stage process performed in-memory:</p> <ol> <li>Anonymize: Strips or replaces metadata tags (PatientID, Names, Dates) based on your config.</li> <li>Redact: Loads pixel data and scrubs burned-in PHI from defined regions.</li> <li>Export: The final \"Gatekeeper\". Writes clean files to a new directory. Setting <code>safe=True</code> ensures the export halts if any verification checks fail (e.g., corrupt images or missing codecs).</li> </ol> <pre><code># Apply metadata remediation (anonymization) using the findings\nsession.anonymize(report)\n\n# Apply pixel redaction rules (requires config to be loaded)\nsession.redact()\n\n# Export only safe (clean) data to a new folder\n# Compression=\"j2k\" optionally compresses output to JPEG 2000\nsession.export(\"/path/to/export_clean\", safe=True, compression=\"j2k\")\n</code></pre> <p>Progress for the save, memory release, and export phases will be displayed:</p> <pre><code>Preparing for export (Auto-Save &amp; Memory Release)...\nReleasing Memory: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5000/5000 [00:02&lt;00:00, 2000.00img/s]\nMemory Cleanup: Released 5000 images from RAM.\nExecuting Redaction Rules...\nRedacting: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 150/150 [00:05&lt;00:00, 28.00img/s]\nExporting session to output_folder (safe=True)...\nExporting:  15%|\u2588\u2588\u258c       | 15/100 [00:05&lt;00:30,  2.80patient/s]\n</code></pre>"},{"location":"quickstart/#6-recover-identity-optional","title":"6. Recover Identity (Optional)","text":"<p>If you have a valid key (<code>gantry.key</code>) and need to retrieve the original identity of an anonymized patient:</p> <pre><code># Load the session containing anonymized data\nsession = Session(\"my_project.db\")\nsession.enable_reversible_anonymization(\"gantry.key\")\n\n# Recover the original PatientName and PatientID\noriginal_id = session.recover_patient_identity(\"ANON_12345\")\nprint(f\"Original Identity: {original_id}\")\n</code></pre>"},{"location":"roadmap/","title":"Project Roadmap","text":"<p>This document outlines the development plan for Gantry. We welcome contributions from the community to help us achieve these milestones!</p>"},{"location":"roadmap/#current-status-v053-maintenance-release","title":"\ud83d\udccd Current Status: v0.5.3 (Maintenance Release)","text":"<ul> <li>[x] Core Object Model: <code>Patient</code> -&gt; <code>Study</code> -&gt; <code>Series</code> -&gt; <code>Instance</code></li> <li>[x] Split-Persistence Architecture: Binary sidecar (<code>_pixels.bin</code>) for high-speed pixel storage.</li> <li>[x] Database Indexing: O(1) lookups and scalable Joins via SQLite indexes.</li> <li>[x] Multithreaded Redaction: Parallelized pixel redaction using <code>ThreadPoolExecutor</code>.</li> <li>[x] Free-Threaded Stability: Full support for Python 3.14t (no-GIL) via versioned dirty tracking.</li> <li>[x] Deep Memory Management: Automatic pixel offloading (<code>Instance.unload_pixel_data()</code>) to handle datasets exceeding RAM.</li> <li>[x] Async Audit Queue: Non-blocking SQLite persistence for high-throughput auditing.</li> <li>[x] Custom Privacy Profiles: Support for external YAML profiles.</li> <li>[x] Standard Privacy Profiles: Built-in support for DICOM PS3.15 Annex E.</li> <li>[x] Legacy Config Removal: Streamlined codebase by removing list-based config support.</li> </ul>"},{"location":"roadmap/#upcoming-milestones","title":"\ud83d\ude80 Upcoming Milestones","text":""},{"location":"roadmap/#v060-analytics-reporting","title":"v0.6.0 - Analytics &amp; Reporting","text":"<p>Focus: Empowering users to understand their data through deep inspection on the object graph.</p> <ul> <li>[ ] Dataframe Export: Expose a method to flatten <code>Patient -&gt; Study -&gt; Series -&gt; Instance</code> hierarchy into a comprehensive parquet file.</li> <li>[ ] Sidecar Compaction: Utility to vacuum/compact the <code>_pixels.bin</code> file to reclaim space from deleted or redacted images.</li> <li>[ ] Pixel Content Analysis (OCR): Detect burned-in text using OCR (Tesseract) / Cloud Vision to automatically flag sensitive images.</li> <li>[ ] Metadata Querying: Enable SQL-like querying on the dataframe (e.g., \"Find all scans with <code>SliceThickness &lt; 1.0</code> acquired by 'GE' scanners\").</li> <li>[ ] Query-based Export: Allow users to filter exports using criteria (e.g., <code>session.export(query=\"Modality=='CT' and SliceThickness &gt; 5.0\")</code>).</li> <li>[ ] Compliance Reporting: Generate reports verifying dataset compliance against a selected privacy profile.</li> <li>[ ] Export Manifest: Automatic generation of visual (HTML) and machine-readable (CSV/JSON) manifests listing all exported files and their key metadata.</li> <li>[ ] Audit Reporting: Export comprehensive CSV reports of the session inventory, including details on what was redacted or modified.</li> <li>[ ] Structured Reporting (SR) Support: Support for deep parsing and anonymization of DICOM Structured Reports.</li> </ul>"},{"location":"roadmap/#v070-the-connector-networking","title":"v0.7.0 - The Connector (Networking)","text":"<p>Focus: Integrating Gantry into clinical workflows via DIMSE services.</p> <ul> <li>[ ] PACS Integration: Implement C-STORE, C-FIND, C-MOVE using <code>pynetdicom</code> to query and pull studies directly.</li> <li>[ ] Research Export Formats: Native support for exporting to NIfTI and BIDS standards.</li> </ul>"},{"location":"roadmap/#v080-cloud-scale","title":"v0.8.0 - Cloud Scale","text":"<p>Focus: Native support for cloud storage to handle massive datasets.</p> <ul> <li>[ ] Persistence Abstraction: Decouple storage logic to enable cloud backends and future plugins.</li> <li>[ ] Cloud Storage Adapters: Native ingestion/export for S3, Google Cloud Storage, and Azure Blob.</li> </ul>"},{"location":"roadmap/#v100-production-release","title":"v1.0.0 - Production Release","text":"<ul> <li>[ ] API Freeze: Lock down the <code>DicomSession</code> interface.</li> <li>[ ] Documentation: Complete API reference and tutorials on ReadTheDocs or Wiki.</li> <li>[ ] PyPI Release: Publish package to the Python Package Index.</li> </ul>"},{"location":"roadmap/#v110-zero-code-cli","title":"v1.1.0 - Zero Code (CLI)","text":"<p>Focus: Making Gantry accessible to non-programmers and CI pipelines.</p> <ul> <li>[ ] Gantry CLI: A rich command-line interface for auditing and anonymizing datasets.</li> </ul>"},{"location":"roadmap/#future-ideas-backlog","title":"\ud83d\udd2e Future Ideas (Backlog)","text":"<ul> <li>3D Defacing: Algorithmically remove facial features from 3D volumes (MRI/CT).</li> <li>Plugin System: Hooks for custom user scripts during ingest/audit/export loops.</li> <li>GUI Wrapper for <code>DicomSession</code>.</li> <li>Official Docker Image: Optimized container build for Gantry with pre-configured codecs and dependencies.</li> </ul>"},{"location":"api/entities/","title":"Entities API","text":""},{"location":"api/entities/#gantry.entities","title":"<code>gantry.entities</code>","text":""},{"location":"api/entities/#gantry.entities.DicomItem","title":"<code>DicomItem</code>  <code>dataclass</code>","text":"<p>Base class for any entity that holds DICOM attributes and sequences.</p> Source code in <code>gantry/entities.py</code> <pre><code>@dataclass(slots=True)\nclass DicomItem:\n    \"\"\"\n    Base class for any entity that holds DICOM attributes and sequences.\n    \"\"\"\n    # init=False to avoid constructor conflicts during inheritance\n    # init=False to avoid constructor conflicts during inheritance\n    attributes: Dict[str, Any] = field(init=False)\n    sequences: Dict[str, DicomSequence] = field(init=False)\n\n    # Versioning for robust persistence\n    _mod_count: int = field(init=False, default=0)\n    _saved_mod_count: int = field(init=False, default=-1)\n\n    def __post_init__(self):\n        self.attributes = {}\n        self.sequences = {}\n        # Initial state is dirty (1 &gt; 0)\n        self._mod_count = 1\n        self._saved_mod_count = 0\n\n    @property\n    def _dirty(self) -&gt; bool:\n        return self._mod_count &gt; self._saved_mod_count\n\n    @_dirty.setter\n    def _dirty(self, value: bool):\n        # Legacy support: setting True increments version\n        if value:\n            self._mod_count += 1\n        else:\n            # Unsafe clear: assumes current state is saved\n            self._saved_mod_count = self._mod_count\n\n    def mark_saved(self, version_saved: int):\n        \"\"\"Marks specific version as saved. Robust against concurrent edits.\"\"\"\n        if version_saved &gt; self._saved_mod_count:\n            self._saved_mod_count = version_saved\n\n    def set_attr(self, tag: str, value: Any):\n        \"\"\"Sets a generic attribute by its hex tag (e.g., '0010,0010').\"\"\"\n        self.attributes[tag] = value\n        self._mod_count += 1\n\n    def add_sequence_item(self, tag: str, item: 'DicomItem'):\n        \"\"\"Appends a new item to a sequence, creating the sequence if needed.\"\"\"\n        if tag not in self.sequences:\n            self.sequences[tag] = DicomSequence(tag=tag)\n        self.sequences[tag].items.append(item)\n        self._mod_count += 1\n\n    def mark_clean(self):\n        # Legacy: force clean\n        self._saved_mod_count = self._mod_count\n        for seq in self.sequences.values():\n            for item in seq.items:\n                item.mark_clean()\n</code></pre>"},{"location":"api/entities/#gantry.entities.DicomItem.add_sequence_item","title":"<code>add_sequence_item(tag, item)</code>","text":"<p>Appends a new item to a sequence, creating the sequence if needed.</p> Source code in <code>gantry/entities.py</code> <pre><code>def add_sequence_item(self, tag: str, item: 'DicomItem'):\n    \"\"\"Appends a new item to a sequence, creating the sequence if needed.\"\"\"\n    if tag not in self.sequences:\n        self.sequences[tag] = DicomSequence(tag=tag)\n    self.sequences[tag].items.append(item)\n    self._mod_count += 1\n</code></pre>"},{"location":"api/entities/#gantry.entities.DicomItem.mark_saved","title":"<code>mark_saved(version_saved)</code>","text":"<p>Marks specific version as saved. Robust against concurrent edits.</p> Source code in <code>gantry/entities.py</code> <pre><code>def mark_saved(self, version_saved: int):\n    \"\"\"Marks specific version as saved. Robust against concurrent edits.\"\"\"\n    if version_saved &gt; self._saved_mod_count:\n        self._saved_mod_count = version_saved\n</code></pre>"},{"location":"api/entities/#gantry.entities.DicomItem.set_attr","title":"<code>set_attr(tag, value)</code>","text":"<p>Sets a generic attribute by its hex tag (e.g., '0010,0010').</p> Source code in <code>gantry/entities.py</code> <pre><code>def set_attr(self, tag: str, value: Any):\n    \"\"\"Sets a generic attribute by its hex tag (e.g., '0010,0010').\"\"\"\n    self.attributes[tag] = value\n    self._mod_count += 1\n</code></pre>"},{"location":"api/entities/#gantry.entities.DicomSequence","title":"<code>DicomSequence</code>  <code>dataclass</code>","text":"<p>Represents a DICOM Sequence (SQ) containing multiple DicomItems.</p> Source code in <code>gantry/entities.py</code> <pre><code>@dataclass(slots=True)\nclass DicomSequence:\n    \"\"\"\n    Represents a DICOM Sequence (SQ) containing multiple DicomItems.\n    \"\"\"\n    tag: str\n    items: List['DicomItem'] = field(default_factory=list)\n</code></pre>"},{"location":"api/entities/#gantry.entities.Equipment","title":"<code>Equipment</code>  <code>dataclass</code>","text":"<p>Immutable Equipment definition. Frozen=True allows hashing, enabling unique set generation.</p> Source code in <code>gantry/entities.py</code> <pre><code>@dataclass(frozen=True, slots=True)\nclass Equipment:\n    \"\"\"\n    Immutable Equipment definition.\n    Frozen=True allows hashing, enabling unique set generation.\n    \"\"\"\n    manufacturer: str\n    model_name: str\n    device_serial_number: str = \"\"\n</code></pre>"},{"location":"api/entities/#gantry.entities.Instance","title":"<code>Instance</code>  <code>dataclass</code>","text":"<p>               Bases: <code>DicomItem</code></p> <p>Represents a single DICOM image (SOP Instance). Manages lazy loading of pixel data.</p> Source code in <code>gantry/entities.py</code> <pre><code>@dataclass(slots=True)\nclass Instance(DicomItem):\n    \"\"\"\n    Represents a single DICOM image (SOP Instance).\n    Manages lazy loading of pixel data.\n    \"\"\"\n    sop_instance_uid: str = \"\"\n    sop_class_uid: str = \"\"\n    instance_number: int = 0\n\n    # Persistence: Link to original file for lazy loading\n    file_path: Optional[str] = None\n\n    # Transient: Actual pixel data (NOT persisted to pickle)\n    pixel_array: Optional[np.ndarray] = field(default=None, repr=False)\n\n    # Transient: Lazy Loader (Callable that returns np.ndarray)\n    # Used for Sidecar or deferred logic\n    _pixel_loader: Optional[Callable[[], np.ndarray]] = field(default=None, repr=False)\n\n    # Transient: Track if dates have been shifted in memory\n    date_shifted: bool = field(default=False, init=False)\n\n    def __post_init__(self):\n        # Inlined from DicomItem to avoid super() mismatch issues with slots/reloads\n        self.attributes = {}\n        self.sequences = {}\n\n        # Versioning\n        self._mod_count = 1\n        self._saved_mod_count = 0\n\n        self.set_attr(\"0008,0018\", self.sop_instance_uid)\n        self.set_attr(\"0008,0016\", self.sop_class_uid)\n        self.set_attr(\"0020,0013\", self.instance_number)\n\n\n\n    def regenerate_uid(self):\n        \"\"\"\n        Generates a new, globally unique SOP Instance UID.\n        Call this whenever pixel data is modified.\n        \"\"\"\n        # 1. Generate new UID using pydicom's generator (or your org root)\n        new_uid = generate_uid()\n\n        # 2. Update the Object Property\n        self.sop_instance_uid = new_uid\n\n        # 3. Update the DICOM Attribute Dictionary\n        self.set_attr(\"0008,0018\", new_uid)\n\n        # 4. Detach from physical file\n        # Since this object is now a \"new\" instance in memory, \n        # it no longer matches the file on disk.\n        self.file_path = None \n\n        from .logger import get_logger\n        get_logger().debug(f\"  -&gt; Identity regenerated: {new_uid}\")\n\n    def unload_pixel_data(self):\n        \"\"\"\n        Clears the cached pixel_array from memory to free resources.\n        Only performs the clear if the data can be re-loaded (file_path or _pixel_loader exists).\n        Returns True if unloaded, False if unsafe to unload.\n        \"\"\"\n        if self.pixel_array is None:\n            return True\n\n        if self.file_path or self._pixel_loader:\n            self.pixel_array = None\n            return True\n        else:\n            # Data is in memory only (e.g. modified but not saved)\n            return False\n\n    def get_pixel_data(self) -&gt; Optional[np.ndarray]:\n        \"\"\"\n        Returns pixel_array. Loads from disk if not in memory.\n        Returns None if no pixel data is present.\n        \"\"\"\n        if self.pixel_array is not None:\n            return self.pixel_array\n\n        if self._pixel_loader:\n             try:\n                 # Invoke callback (e.g. sidecar read)\n                 arr = self._pixel_loader()\n                 # Use set_pixel_data to ensure attributes (rows, cols) are synced \n                 # This is critical if the loader returns a raw array but attributes were not yet set/restored\n                 self.set_pixel_data(arr)\n                 return self.pixel_array\n             except Exception as e:\n                 raise RuntimeError(f\"Pixel Loader failed for {self.sop_instance_uid}: {e}\")\n\n        if self.file_path and os.path.exists(self.file_path):\n            try:\n                # Read pixel data on demand\n                ds = None\n                try:\n                    ds = pydicom.dcmread(self.file_path)\n\n                    self.set_pixel_data(ds.pixel_array)  # Cache it in memory\n                    return self.pixel_array\n                except (AttributeError, TypeError):\n                    # No pixel data element\n                    return None\n                except Exception as e:\n                    if \"no pixel data\" in str(e).lower():\n                        return None\n                    # Re-raise to be handled by outer except\n                    raise e\n\n            except Exception as e:\n                # Try explicit fallback to gantry.imagecodecs_handler\n                # Pydicom sometimes fails to iterate handlers correctly or swallows errors.\n                try:\n                    import gantry.imagecodecs_handler as h\n                    if ds is not None and h.is_available() and h.supports_transfer_syntax(ds.file_meta.TransferSyntaxUID):\n                        arr = h.get_pixel_data(ds)\n                        self.set_pixel_data(arr)\n                        return self.pixel_array\n                except Exception as fallback_e:\n                    # Fallback failed, proceed to raise original error\n                    pass\n\n                # Try to get Transfer Syntax UID for better debugging\n                ts_uid = \"Unknown\"\n                if ds is not None and hasattr(ds, \"file_meta\"):\n                     ts_uid = getattr(ds.file_meta, \"TransferSyntaxUID\", \"Unknown\")\n\n                if \"missing dependencies\" in str(e) or \"decompress\" in str(e):\n                    # Enhanced debug output\n                    handlers = []\n                    try:\n                        # pydicom is already imported globally\n                        handlers = [str(h) for h in pydicom.config.pixel_data_handlers]\n                    except: pass\n\n                    raise RuntimeError(\n                        f\"Failed to decompress pixel data for {os.path.basename(self.file_path)} \"\n                        f\"(Transfer Syntax: {ts_uid}).\\n\"\n                        f\"Underlying Error: {e}\\n\"\n                        f\"Active pydicom handlers: {handlers}\\n\"\n                        \"Missing image codecs. Please ensure 'pillow', 'pylibjpeg', or 'gdcm' are installed.\"\n                    ) from e\n\n                # If we just caught the re-raised \"no pixel data\" exception, it would be handled above, \n                # but if dcmread fails completely or something else happens:\n                raise RuntimeError(f\"Lazy load failed for {self.file_path}: {e}\")\n\n        raise FileNotFoundError(f\"Pixels missing and file not found: {self.file_path}\")\n\n    def set_pixel_data(self, array: np.ndarray):\n        \"\"\"\n        Sets the pixel array and automatically updates metadata tags\n        (rows, cols, samples, frames, etc.) based on array shape.\n        Handles unpacking of 2D/3D/4D arrays.\n        \"\"\"\n        self.pixel_array = array\n        shape = array.shape\n        ndim = len(shape)\n\n        # Defaults\n        samples = 1\n        frames = 1\n\n        if ndim == 2:\n            rows, cols = shape\n        elif ndim == 3:\n            if shape[-1] in [3, 4]:\n                rows, cols, samples = shape\n            else:\n                frames, rows, cols = shape\n        elif ndim == 4:\n            frames, rows, cols, samples = shape\n        else:\n            raise ValueError(f\"Unknown shape: {shape}\")\n\n        self.set_attr(\"0028,0010\", rows)\n        self.set_attr(\"0028,0011\", cols)\n        self.set_attr(\"0028,0002\", samples)\n        if frames &gt; 1: self.set_attr(\"0028,0008\", str(frames))\n        if samples &gt;= 3: self.set_attr(\"0028,0004\", \"RGB\")\n\n        # Ensure BitsAllocated matches array data type\n        # SidecarPixelLoader relies on this to determine uint8 vs uint16\n        bits = array.itemsize * 8\n        self.set_attr(\"0028,0100\", bits)\n\n        self._mod_count += 1\n</code></pre>"},{"location":"api/entities/#gantry.entities.Instance.get_pixel_data","title":"<code>get_pixel_data()</code>","text":"<p>Returns pixel_array. Loads from disk if not in memory. Returns None if no pixel data is present.</p> Source code in <code>gantry/entities.py</code> <pre><code>def get_pixel_data(self) -&gt; Optional[np.ndarray]:\n    \"\"\"\n    Returns pixel_array. Loads from disk if not in memory.\n    Returns None if no pixel data is present.\n    \"\"\"\n    if self.pixel_array is not None:\n        return self.pixel_array\n\n    if self._pixel_loader:\n         try:\n             # Invoke callback (e.g. sidecar read)\n             arr = self._pixel_loader()\n             # Use set_pixel_data to ensure attributes (rows, cols) are synced \n             # This is critical if the loader returns a raw array but attributes were not yet set/restored\n             self.set_pixel_data(arr)\n             return self.pixel_array\n         except Exception as e:\n             raise RuntimeError(f\"Pixel Loader failed for {self.sop_instance_uid}: {e}\")\n\n    if self.file_path and os.path.exists(self.file_path):\n        try:\n            # Read pixel data on demand\n            ds = None\n            try:\n                ds = pydicom.dcmread(self.file_path)\n\n                self.set_pixel_data(ds.pixel_array)  # Cache it in memory\n                return self.pixel_array\n            except (AttributeError, TypeError):\n                # No pixel data element\n                return None\n            except Exception as e:\n                if \"no pixel data\" in str(e).lower():\n                    return None\n                # Re-raise to be handled by outer except\n                raise e\n\n        except Exception as e:\n            # Try explicit fallback to gantry.imagecodecs_handler\n            # Pydicom sometimes fails to iterate handlers correctly or swallows errors.\n            try:\n                import gantry.imagecodecs_handler as h\n                if ds is not None and h.is_available() and h.supports_transfer_syntax(ds.file_meta.TransferSyntaxUID):\n                    arr = h.get_pixel_data(ds)\n                    self.set_pixel_data(arr)\n                    return self.pixel_array\n            except Exception as fallback_e:\n                # Fallback failed, proceed to raise original error\n                pass\n\n            # Try to get Transfer Syntax UID for better debugging\n            ts_uid = \"Unknown\"\n            if ds is not None and hasattr(ds, \"file_meta\"):\n                 ts_uid = getattr(ds.file_meta, \"TransferSyntaxUID\", \"Unknown\")\n\n            if \"missing dependencies\" in str(e) or \"decompress\" in str(e):\n                # Enhanced debug output\n                handlers = []\n                try:\n                    # pydicom is already imported globally\n                    handlers = [str(h) for h in pydicom.config.pixel_data_handlers]\n                except: pass\n\n                raise RuntimeError(\n                    f\"Failed to decompress pixel data for {os.path.basename(self.file_path)} \"\n                    f\"(Transfer Syntax: {ts_uid}).\\n\"\n                    f\"Underlying Error: {e}\\n\"\n                    f\"Active pydicom handlers: {handlers}\\n\"\n                    \"Missing image codecs. Please ensure 'pillow', 'pylibjpeg', or 'gdcm' are installed.\"\n                ) from e\n\n            # If we just caught the re-raised \"no pixel data\" exception, it would be handled above, \n            # but if dcmread fails completely or something else happens:\n            raise RuntimeError(f\"Lazy load failed for {self.file_path}: {e}\")\n\n    raise FileNotFoundError(f\"Pixels missing and file not found: {self.file_path}\")\n</code></pre>"},{"location":"api/entities/#gantry.entities.Instance.regenerate_uid","title":"<code>regenerate_uid()</code>","text":"<p>Generates a new, globally unique SOP Instance UID. Call this whenever pixel data is modified.</p> Source code in <code>gantry/entities.py</code> <pre><code>def regenerate_uid(self):\n    \"\"\"\n    Generates a new, globally unique SOP Instance UID.\n    Call this whenever pixel data is modified.\n    \"\"\"\n    # 1. Generate new UID using pydicom's generator (or your org root)\n    new_uid = generate_uid()\n\n    # 2. Update the Object Property\n    self.sop_instance_uid = new_uid\n\n    # 3. Update the DICOM Attribute Dictionary\n    self.set_attr(\"0008,0018\", new_uid)\n\n    # 4. Detach from physical file\n    # Since this object is now a \"new\" instance in memory, \n    # it no longer matches the file on disk.\n    self.file_path = None \n\n    from .logger import get_logger\n    get_logger().debug(f\"  -&gt; Identity regenerated: {new_uid}\")\n</code></pre>"},{"location":"api/entities/#gantry.entities.Instance.set_pixel_data","title":"<code>set_pixel_data(array)</code>","text":"<p>Sets the pixel array and automatically updates metadata tags (rows, cols, samples, frames, etc.) based on array shape. Handles unpacking of 2D/3D/4D arrays.</p> Source code in <code>gantry/entities.py</code> <pre><code>def set_pixel_data(self, array: np.ndarray):\n    \"\"\"\n    Sets the pixel array and automatically updates metadata tags\n    (rows, cols, samples, frames, etc.) based on array shape.\n    Handles unpacking of 2D/3D/4D arrays.\n    \"\"\"\n    self.pixel_array = array\n    shape = array.shape\n    ndim = len(shape)\n\n    # Defaults\n    samples = 1\n    frames = 1\n\n    if ndim == 2:\n        rows, cols = shape\n    elif ndim == 3:\n        if shape[-1] in [3, 4]:\n            rows, cols, samples = shape\n        else:\n            frames, rows, cols = shape\n    elif ndim == 4:\n        frames, rows, cols, samples = shape\n    else:\n        raise ValueError(f\"Unknown shape: {shape}\")\n\n    self.set_attr(\"0028,0010\", rows)\n    self.set_attr(\"0028,0011\", cols)\n    self.set_attr(\"0028,0002\", samples)\n    if frames &gt; 1: self.set_attr(\"0028,0008\", str(frames))\n    if samples &gt;= 3: self.set_attr(\"0028,0004\", \"RGB\")\n\n    # Ensure BitsAllocated matches array data type\n    # SidecarPixelLoader relies on this to determine uint8 vs uint16\n    bits = array.itemsize * 8\n    self.set_attr(\"0028,0100\", bits)\n\n    self._mod_count += 1\n</code></pre>"},{"location":"api/entities/#gantry.entities.Instance.unload_pixel_data","title":"<code>unload_pixel_data()</code>","text":"<p>Clears the cached pixel_array from memory to free resources. Only performs the clear if the data can be re-loaded (file_path or _pixel_loader exists). Returns True if unloaded, False if unsafe to unload.</p> Source code in <code>gantry/entities.py</code> <pre><code>def unload_pixel_data(self):\n    \"\"\"\n    Clears the cached pixel_array from memory to free resources.\n    Only performs the clear if the data can be re-loaded (file_path or _pixel_loader exists).\n    Returns True if unloaded, False if unsafe to unload.\n    \"\"\"\n    if self.pixel_array is None:\n        return True\n\n    if self.file_path or self._pixel_loader:\n        self.pixel_array = None\n        return True\n    else:\n        # Data is in memory only (e.g. modified but not saved)\n        return False\n</code></pre>"},{"location":"api/entities/#gantry.entities.Patient","title":"<code>Patient</code>  <code>dataclass</code>","text":"<p>Root of the object hierarchy. Groups Studies by Patient ID.</p> Source code in <code>gantry/entities.py</code> <pre><code>@dataclass(slots=True)\nclass Patient:\n    \"\"\"\n    Root of the object hierarchy. Groups Studies by Patient ID.\n    \"\"\"\n    patient_id: str\n    patient_name: str\n    studies: List[Study] = field(default_factory=list)\n    _dirty: bool = field(default=True, init=False)\n\n    def __post_init__(self):\n        self._dirty = True\n\n    def mark_clean(self):\n        self._dirty = False\n        for s in self.studies:\n            s.mark_clean()\n</code></pre>"},{"location":"api/entities/#gantry.entities.Series","title":"<code>Series</code>  <code>dataclass</code>","text":"<p>Groups Instances by Series Instance UID. Typically represents a single scan or reconstruction.</p> Source code in <code>gantry/entities.py</code> <pre><code>@dataclass(slots=True)\nclass Series:\n    \"\"\"\n    Groups Instances by Series Instance UID.\n    Typically represents a single scan or reconstruction.\n    \"\"\"\n    series_instance_uid: str\n    modality: str\n    series_number: int\n    equipment: Optional[Equipment] = None\n    instances: List[Instance] = field(default_factory=list)\n    _dirty: bool = field(default=True, init=False)\n\n    def __post_init__(self):\n        self._dirty = True\n\n    def mark_clean(self):\n        self._dirty = False\n        for i in self.instances:\n            i.mark_clean()\n</code></pre>"},{"location":"api/entities/#gantry.entities.Study","title":"<code>Study</code>  <code>dataclass</code>","text":"<p>Groups Series by Study Instance UID. Represents a single patient visit or examination.</p> Source code in <code>gantry/entities.py</code> <pre><code>@dataclass(slots=True)\nclass Study:\n    \"\"\"\n    Groups Series by Study Instance UID.\n    Represents a single patient visit or examination.\n    \"\"\"\n    study_instance_uid: str\n    study_date: Any\n    series: List[Series] = field(default_factory=list)\n    date_shifted: bool = False\n    _dirty: bool = field(default=True, init=False)\n\n    def __post_init__(self):\n        self._dirty = True\n\n    def mark_clean(self):\n        self._dirty = False\n        for s in self.series:\n            s.mark_clean()\n</code></pre>"},{"location":"api/persistence/","title":"Persistence API","text":""},{"location":"api/persistence/#gantry.persistence","title":"<code>gantry.persistence</code>","text":""},{"location":"api/persistence/#gantry.persistence.SidecarPixelLoader","title":"<code>SidecarPixelLoader</code>","text":"<p>Functor for lazy loading of pixel data from sidecar. Must be a top-level class to be picklable.</p> Source code in <code>gantry/persistence.py</code> <pre><code>class SidecarPixelLoader:\n    \"\"\"\n    Functor for lazy loading of pixel data from sidecar.\n    Must be a top-level class to be picklable.\n    \"\"\"\n    def __init__(self, sidecar_path, offset, length, alg, instance):\n        self.sidecar_path = sidecar_path\n        self.offset = offset\n        self.length = length\n        self.alg = alg\n        self.instance = instance\n\n    def __call__(self):\n        from .sidecar import SidecarManager\n        mgr = SidecarManager(self.sidecar_path)\n\n        raw = mgr.read_frame(self.offset, self.length, self.alg)\n\n        # Reconstruct based on attributes\n        bits = self.instance.attributes.get(\"0028,0100\", 8)\n        dt = np.uint16 if bits &gt; 8 else np.uint8\n\n        arr = np.frombuffer(raw, dtype=dt)\n\n        rows = self.instance.attributes.get(\"0028,0010\", 0)\n        cols = self.instance.attributes.get(\"0028,0011\", 0)\n        samples = self.instance.attributes.get(\"0028,0002\", 1)\n        frames = int(self.instance.attributes.get(\"0028,0008\", 0) or 0)\n\n        if frames &gt; 1:\n            target_shape = (frames, rows, cols, samples)\n            if samples == 1: target_shape = (frames, rows, cols)\n        elif samples &gt; 1:\n            target_shape = (rows, cols, samples)\n        else:\n            target_shape = (rows, cols)\n\n        try:\n            return arr.reshape(target_shape)\n        except:\n            return arr\n</code></pre>"},{"location":"api/persistence/#gantry.persistence.SqliteStore","title":"<code>SqliteStore</code>","text":"<p>Handles persistence of the Object Graph to a SQLite database. Also manages the Audit Log.</p> Source code in <code>gantry/persistence.py</code> <pre><code>class SqliteStore:\n    \"\"\"\n    Handles persistence of the Object Graph to a SQLite database.\n    Also manages the Audit Log.\n    \"\"\"\n\n    SCHEMA = \"\"\"\n    CREATE TABLE IF NOT EXISTS patients (\n        id INTEGER PRIMARY KEY AUTOINCREMENT,\n        patient_id TEXT NOT NULL,\n        patient_name TEXT,\n        UNIQUE(patient_id)\n    );\n\n    CREATE TABLE IF NOT EXISTS studies (\n        id INTEGER PRIMARY KEY AUTOINCREMENT,\n        patient_id_fk INTEGER,\n        study_instance_uid TEXT NOT NULL,\n        study_date TEXT,\n        FOREIGN KEY(patient_id_fk) REFERENCES patients(id),\n        UNIQUE(study_instance_uid)\n    );\n\n    CREATE TABLE IF NOT EXISTS series (\n        id INTEGER PRIMARY KEY AUTOINCREMENT,\n        study_id_fk INTEGER,\n        series_instance_uid TEXT NOT NULL,\n        modality TEXT,\n        series_number INTEGER,\n        manufacturer TEXT,\n        model_name TEXT,\n        device_serial_number TEXT,\n        FOREIGN KEY(study_id_fk) REFERENCES studies(id),\n        UNIQUE(series_instance_uid)\n    );\n\n    CREATE TABLE IF NOT EXISTS instances (\n        id INTEGER PRIMARY KEY AUTOINCREMENT,\n        series_id_fk INTEGER,\n        sop_instance_uid TEXT NOT NULL,\n        sop_class_uid TEXT,\n        instance_number INTEGER,\n        file_path TEXT,\n        pixel_file_id INTEGER DEFAULT 0,\n        pixel_offset INTEGER,\n        pixel_length INTEGER,\n        compress_alg TEXT,\n        attributes_json TEXT, -- Store extra attributes as JSON for now\n        FOREIGN KEY(series_id_fk) REFERENCES series(id),\n        UNIQUE(sop_instance_uid)\n    );\n\n    CREATE TABLE IF NOT EXISTS audit_log (\n        id INTEGER PRIMARY KEY AUTOINCREMENT,\n        timestamp TEXT,\n        action_type TEXT,\n        entity_uid TEXT,\n        details TEXT\n    );\n    CREATE TABLE IF NOT EXISTS phi_findings (\n        id INTEGER PRIMARY KEY AUTOINCREMENT,\n        timestamp TEXT,\n        entity_uid TEXT,\n        entity_type TEXT,\n        field_name TEXT,\n        value TEXT,\n        reason TEXT,\n        patient_id TEXT,\n        remediation_action TEXT,\n        remediation_value TEXT,\n        details_json TEXT\n    );\n\n    -- Indexing for Performance\n    CREATE INDEX IF NOT EXISTS idx_studies_patient_fk ON studies(patient_id_fk);\n    CREATE INDEX IF NOT EXISTS idx_series_study_fk ON series(study_id_fk);\n    CREATE INDEX IF NOT EXISTS idx_instances_series_fk ON instances(series_id_fk);\n    CREATE INDEX IF NOT EXISTS idx_audit_entity ON audit_log(entity_uid);\n    CREATE INDEX IF NOT EXISTS idx_findings_entity ON phi_findings(entity_uid);\n    \"\"\"\n\n    def __init__(self, db_path: str):\n        self.db_path = db_path\n        self.logger = get_logger()\n        if db_path == \":memory:\":\n            # Use a temporary file for sidecar if DB is in-memory\n            # SidecarManager currently requires a file path (append-only logic)\n            # Create a temp file that persists until process exit (or manual cleanup)\n            # We use NamedTemporaryFile but close it so SidecarManager can open/lock it.\n            tf = tempfile.NamedTemporaryFile(suffix=\"_pixels.bin\", delete=False)\n            self.sidecar_path = tf.name\n            tf.close()\n            # Shared memory connection for :memory: database to persist across transactions\n            self._memory_conn = sqlite3.connect(\":memory:\", check_same_thread=False)\n            self._memory_conn.row_factory = sqlite3.Row\n            self._memory_lock = threading.Lock()\n        else:\n            self.sidecar_path = os.path.splitext(db_path)[0] + \"_pixels.bin\"\n            self._memory_conn = None\n            self._memory_lock = None\n\n        self.sidecar = SidecarManager(self.sidecar_path)\n        self._init_db()\n\n        # Async Audit Queue\n        self.audit_queue = queue.Queue()\n        self._stop_event = threading.Event()\n        self._audit_thread = threading.Thread(target=self._audit_worker, daemon=True, name=\"AuditWorker\")\n        self._audit_thread.start()\n\n    @contextlib.contextmanager\n    def _get_connection(self):\n        \"\"\"\n        Context manager for database connections.\n        Handles persistent connection for :memory: databases.\n        \"\"\"\n        if self._memory_conn:\n            # For in-memory DB, reuse the single connection.\n            # We must serialize access because sqlite3 connections are not thread-safe \n            # for concurrent writes even with check_same_thread=False.\n            with self._memory_lock:\n                try:\n                    # print(f\"DEBUG: Acquired lock. Yielding conn {id(self._memory_conn)}\") # Reduced spam\n                    yield self._memory_conn\n                    self._memory_conn.commit()\n                    # print(\"DEBUG: Commit successful\")\n                except Exception as e:\n                    print(f\"DEBUG: Rollback due to {e}\")\n                    self._memory_conn.rollback()\n                    raise\n        else:\n            # File-based DB: create fresh connection per transaction\n            conn = sqlite3.connect(self.db_path, timeout=900.0)\n            conn.row_factory = sqlite3.Row\n            try:\n                yield conn\n                conn.commit()\n            except Exception as e:\n                conn.rollback()\n                raise\n            finally:\n                conn.close()\n\n    def _init_db(self):\n        with self._get_connection() as conn:\n            conn.execute(\"PRAGMA journal_mode=WAL;\")\n            conn.executescript(self.SCHEMA)\n\n    def _create_pixel_loader(self, offset, length, alg, instance):\n        \"\"\"Helper to create a lazy pixel loader for the sidecar.\"\"\"\n        return SidecarPixelLoader(self.sidecar_path, offset, length, alg, instance)\n\n    def _audit_worker(self):\n        \"\"\"Background thread to batch write audit logs.\"\"\"\n        batch = []\n        while not self._stop_event.is_set():\n            try:\n                # Collect items with timeout\n                try:\n                    item = self.audit_queue.get(timeout=1.0)\n                    batch.append(item)\n\n                    # Drain queue up to limit\n                    while len(batch) &lt; 100:\n                        try:\n                            item = self.audit_queue.get_nowait()\n                            batch.append(item)\n                        except queue.Empty:\n                            break\n\n                except queue.Empty:\n                    pass\n\n                if batch:\n                    self.log_audit_batch(batch)\n                    batch = []\n\n            except Exception as e:\n                # Don't crash thread\n                self.logger.error(f\"Audit Worker Error: {e}\")\n\n        # Flush remaining\n        while not self.audit_queue.empty():\n            try:\n                batch.append(self.audit_queue.get_nowait())\n            except: break\n        if batch:\n            self.log_audit_batch(batch)\n\n    def stop(self):\n        \"\"\"Stops the audit worker and flushes queue.\"\"\"\n        self._stop_event.set()\n        if self._audit_thread.is_alive():\n            self._audit_thread.join(timeout=2.0)\n\n    def log_audit(self, action_type: str, entity_uid: str, details: str):\n        \"\"\"Records an action in the audit log (Async).\"\"\"\n        # Push to queue instead of writing directly\n        self.audit_queue.put((action_type, entity_uid, details))\n\n    def log_audit_batch(self, entries: List[tuple]):\n        \"\"\"\n        Batch inserts audit logs. \n        entries: List of (action_type, entity_uid, details)\n        \"\"\"\n        if not entries: return\n\n        timestamp = datetime.now().isoformat()\n        # Prepare data with timestamp: (timestamp, action, uid, details)\n        data = [(timestamp, e[0], e[1], e[2]) for e in entries]\n\n        try:\n            with self._get_connection() as conn:\n                conn.executemany(\n                    \"INSERT INTO audit_log (timestamp, action_type, entity_uid, details) VALUES (?, ?, ?, ?)\",\n                    data\n                )\n                conn.commit()\n        except sqlite3.Error as e:\n            self.logger.error(f\"Failed to batch log audit: {e}\")\n\n    def load_all(self) -&gt; List[Patient]:\n        \"\"\"\n        Reconstructs the entire object graph from the database.\n        Returns a list of Patient objects.\n        \"\"\"\n        patients = []\n        if self.db_path != \":memory:\" and not os.path.exists(self.db_path):\n            return patients\n\n        try:\n            with self._get_connection() as conn:\n                # conn.row_factory = sqlite3.Row  &lt;-- Handled by _get_connection\n                cur = conn.cursor()\n\n                # Optimized: We could do joins, but for clarity/mapping let's do hierarchical fetch.\n                # Or fetch all and Stitch. Stitching in memory is faster for SQLite than N+1 queries.\n\n                # 1. Fetch AlL\n                p_rows = cur.execute(\"SELECT * FROM patients\").fetchall()\n                st_rows = cur.execute(\"SELECT * FROM studies\").fetchall()\n                se_rows = cur.execute(\"SELECT * FROM series\").fetchall()\n                i_rows = cur.execute(\"SELECT * FROM instances\").fetchall()\n\n\n\n                # 2. Build Maps\n                p_map = {}\n                for r in p_rows:\n                    p = Patient(r['patient_id'], r['patient_name'])\n                    p_map[r['id']] = p\n                    patients.append(p)\n\n                st_map = {}\n                for r in st_rows:\n                    st = Study(r['study_instance_uid'], r['study_date'])\n                    st_map[r['id']] = st\n                    if r['patient_id_fk'] in p_map:\n                        p_map[r['patient_id_fk']].studies.append(st)\n\n                se_map = {}\n                for r in se_rows:\n                    se = Series(r['series_instance_uid'], r['modality'], r['series_number'])\n                    if r['manufacturer'] or r['model_name']:\n                        se.equipment = Equipment(r['manufacturer'], r['model_name'], r['device_serial_number'])\n                    se_map[r['id']] = se\n                    if r['study_id_fk'] in st_map:\n                        st_map[r['study_id_fk']].series.append(se)\n\n                for r in i_rows:\n                    inst = Instance(\n                        r['sop_instance_uid'], \n                        r['sop_class_uid'], \n                        r['instance_number'], \n                        file_path=r['file_path']\n                    )\n\n                    # Wire up Sidecar Loader if present\n                    if r['pixel_offset'] is not None and r['pixel_length'] is not None:\n                         # Capture closure vars\n                         offset = r['pixel_offset']\n                         length = r['pixel_length']\n                         alg = r['compress_alg']\n\n                         # We need to reshape after loading. The dimensions are in attributes.\n                         # We can do this inside the lambda wrapper or a helper method.\n                         # But Instance.attributes aren't populated yet! \n                         # Wait, we populate attributes right after this.\n                         # So the lambda calls self.instance methods? No, lambda binds early.\n\n                         inst._pixel_loader = self._create_pixel_loader(r['pixel_offset'], r['pixel_length'], r['compress_alg'], inst)\n\n                    # Restore extra attributes\n                    if r['attributes_json']:\n                        try:\n                            attrs = json.loads(r['attributes_json'], object_hook=gantry_json_object_hook)\n                            self._deserialize_into(inst, attrs)\n                        except: \n                            pass # JSON error\n\n                    if r['series_id_fk'] in se_map:\n                        se_map[r['series_id_fk']].instances.append(inst)\n\n            self.logger.info(f\"Loaded {len(patients)} patients from {self.db_path}\")\n            # Mark all loaded data as clean so we don't save it back immediately\n            for p in patients:\n                p.mark_clean()\n            return patients\n\n        except sqlite3.Error as e:\n            print(f\"DEBUG: Failed to load from DB: {e}\")\n            self.logger.error(f\"Failed to load PDF from DB: {e}\")\n            import traceback\n            traceback.print_exc()\n            return []\n\n    def load_patient(self, patient_uid: str) -&gt; Optional[Patient]:\n        \"\"\"Loads a single patient and their graph from the DB by PatientID.\"\"\"\n        if self.db_path != \":memory:\" and not os.path.exists(self.db_path):\n            return None\n\n        try:\n             with self._get_connection() as conn:\n                # conn.row_factory = sqlite3.Row\n                cur = conn.cursor()\n\n                # Fetch Patient\n                p_row = cur.execute(\"SELECT * FROM patients WHERE patient_id = ?\", (patient_uid,)).fetchone()\n                if not p_row: return None\n\n                p = Patient(p_row['patient_id'], p_row['patient_name'])\n                p_pk = p_row['id']\n\n                # Fetch Studies\n                st_rows = cur.execute(\"SELECT * FROM studies WHERE patient_id_fk = ?\", (p_pk,)).fetchall()\n                for st_r in st_rows:\n                    st = Study(st_r['study_instance_uid'], st_r['study_date'])\n                    st_pk = st_r['id']\n\n                    # Fetch Series\n                    se_rows = cur.execute(\"SELECT * FROM series WHERE study_id_fk = ?\", (st_pk,)).fetchall()\n                    for se_r in se_rows:\n                        se = Series(se_r['series_instance_uid'], se_r['modality'], se_r['series_number'])\n                        if se_r['manufacturer'] or se_r['model_name']:\n                            se.equipment = Equipment(se_r['manufacturer'], se_r['model_name'], se_r['device_serial_number'])\n                        se_pk = se_r['id']\n\n                        # Fetch Instances\n                        i_rows = cur.execute(\"SELECT * FROM instances WHERE series_id_fk = ?\", (se_pk,)).fetchall()\n                        for r in i_rows:\n                            inst = Instance(\n                                r['sop_instance_uid'], \n                                r['sop_class_uid'], \n                                r['instance_number'], \n                                file_path=r['file_path']\n                            )\n                            # Wire up Sidecar (Copy-Paste logic from load_all, keep generic?)\n                            # ideally refactor _hydrate_instance but inline is fine for now\n                            if r['pixel_offset'] is not None and r['pixel_length'] is not None:\n                                offset, length, alg = r['pixel_offset'], r['pixel_length'], r['compress_alg']\n                                inst._pixel_loader = self._create_pixel_loader(r['pixel_offset'], r['pixel_length'], r['compress_alg'], inst)\n\n                            if r['attributes_json']:\n                                try:\n                                    attrs = json.loads(r['attributes_json'], object_hook=gantry_json_object_hook)\n                                    self._deserialize_into(inst, attrs)\n                                except: pass\n\n                            se.instances.append(inst)\n\n                        st.series.append(se)\n                    p.studies.append(st)\n\n                p.mark_clean()\n                return p\n        except sqlite3.Error as e:\n            self.logger.error(f\"Failed to load patient: {e}\")\n            return None\n\n    def _serialize_item(self, item: Instance) -&gt; Dict[str, Any]:\n        \"\"\"\n        Serializes a DicomItem (or Instance) to a dictionary, including attributes and sequences.\n        \"\"\"\n        data = item.attributes.copy()\n        if item.sequences:\n            seq_data = {}\n            for tag, seq in item.sequences.items():\n                items_list = []\n                for seq_item in seq.items:\n                    # Recursive call for sequence items (which are DicomItems)\n                    # We can reuse logic but need to handle DicomItem vs Instance\n                    # Instance specific fields are handled by caller for the root, \n                    # but for seq items they are just DicomItems.\n                    items_list.append(self._serialize_dicom_item(seq_item))\n                seq_data[tag] = items_list\n            data['__sequences__'] = seq_data\n        return data\n\n    def _serialize_dicom_item(self, item) -&gt; Dict[str, Any]:\n        \"\"\"Helper for recursive serialization of generic DicomItems.\"\"\"\n        data = item.attributes.copy()\n        if item.sequences:\n            seq_data = {}\n            for tag, seq in item.sequences.items():\n                items_list = [self._serialize_dicom_item(i) for i in seq.items]\n                seq_data[tag] = items_list\n            data['__sequences__'] = seq_data\n        return data\n\n    def _deserialize_into(self, target_item, data: Dict[str, Any]):\n        \"\"\"\n        Populates target_item with attributes and sequences from data dict.\n        \"\"\"\n        sequences_data = data.pop('__sequences__', None)\n\n        # 1. Attributes\n        target_item.attributes.update(data)\n\n        # 2. Sequences\n        if sequences_data:\n            from .entities import DicomItem\n            for tag, items_list in sequences_data.items():\n                for item_data in items_list:\n                    new_item = DicomItem()\n                    self._deserialize_into(new_item, item_data)\n                    target_item.add_sequence_item(tag, new_item)\n\n    def persist_pixel_data(self, instance: Instance):\n        \"\"\"\n        Immediately persists pixel data to the sidecar to allow memory offloading.\n        Does NOT update the full instance record in the main DB (attributes/json), \n        only the pixel linkage. \n        \"\"\"\n        if instance.pixel_array is None:\n            return\n\n        try:\n            # 1. Write to Sidecar\n            b_data = instance.pixel_array.tobytes()\n            # Determine suitable compression? Defaulting to zlib for swap.\n            # Ideally we respect original or config, but for swap zlib is safe/fast enough.\n            c_alg = 'zlib' \n\n            offset, length = self.sidecar.write_frame(b_data, c_alg)\n\n            # 2. Update Instance Loader\n            # This allows instance.unload_pixel_data() to work safely\n            instance._pixel_loader = self._create_pixel_loader(offset, length, c_alg, instance)\n\n            # 3. Optional: Persist the linkage to DB immediately?\n            # It's safer if we do, so if we crash, we know where the pixels are.\n            # However, if we don't save the attributes/UID changes, the DB is out of sync anyway.\n            # But the primary goal here is MEMORY MANAGEMENT.\n            # So updating the object state in memory (step 2) is sufficient for unload_pixel_data() to return True.\n            # The final session.save() will record the new offset/length into the DB instances table.\n\n        except Exception as e:\n            self.logger.error(f\"Failed to persist pixel swap for {instance.sop_instance_uid}: {e}\")\n            raise e\n\n    def save_all(self, patients: List[Patient]):\n        \"\"\"\n        Persists the current state incrementally.\n        Strategy: UPSERT dirty items.\n        \"\"\"\n        self.logger.info(f\"Saving {len(patients)} patients to {self.db_path} (Incremental)...\")\n\n        pixel_bytes_written = 0\n        pixel_frames_written = 0\n        sidecar_manager = self.sidecar\n\n        try:\n            with self._get_connection() as conn:\n                cur = conn.cursor()\n\n                # Check for schema compatibility (simple check)\n                try:\n                    # We rely on UNIQUE constraints for UPSERT. \n                    # If older DB without constraints, we might fail or duplicate.\n                    pass \n                except: pass\n\n                # Counts for reporting\n                saved_p, saved_st, saved_se, saved_i = 0, 0, 0, 0\n\n                for p in patients:\n                    # Patient Level (Always Check Dirty)\n                    if getattr(p, '_dirty', True):\n                        cur.execute(\"\"\"\n                            INSERT INTO patients (patient_id, patient_name) VALUES (?, ?)\n                            ON CONFLICT(patient_id) DO UPDATE SET patient_name=excluded.patient_name\n                        \"\"\", (p.patient_id, p.patient_name))\n                        saved_p += 1\n\n                    # We need the PK for children\n                    # Since we might have just updated or it might exist, we select it.\n                    # Optimization: Cache PKs? For now, fetch is safe.\n                    p_pk_row = cur.execute(\"SELECT id FROM patients WHERE patient_id=?\", (p.patient_id,)).fetchone()\n                    if not p_pk_row: continue # Should not happen after Insert\n                    p_pk = p_pk_row[0]\n\n                    for st in p.studies:\n                        if getattr(st, '_dirty', True):\n                            cur.execute(\"\"\"\n                                INSERT INTO studies (patient_id_fk, study_instance_uid, study_date) VALUES (?, ?, ?)\n                                ON CONFLICT(study_instance_uid) DO UPDATE SET \n                                    study_date=excluded.study_date,\n                                    patient_id_fk=excluded.patient_id_fk\n                            \"\"\", (p_pk, st.study_instance_uid, st.study_date))\n                            saved_st += 1\n\n                        st_pk_row = cur.execute(\"SELECT id FROM studies WHERE study_instance_uid=?\", (st.study_instance_uid,)).fetchone()\n                        if not st_pk_row: continue\n                        st_pk = st_pk_row[0]\n\n                        for se in st.series:\n                            if getattr(se, '_dirty', True):\n                                man = se.equipment.manufacturer if se.equipment else \"\"\n                                mod = se.equipment.model_name if se.equipment else \"\"\n                                sn = se.equipment.device_serial_number if se.equipment else \"\"\n\n                                cur.execute(\"\"\"\n                                    INSERT INTO series (study_id_fk, series_instance_uid, modality, series_number, manufacturer, model_name, device_serial_number)\n                                    VALUES (?, ?, ?, ?, ?, ?, ?)\n                                    ON CONFLICT(series_instance_uid) DO UPDATE SET \n                                        modality=excluded.modality,\n                                        series_number=excluded.series_number,\n                                        manufacturer=excluded.manufacturer,\n                                        model_name=excluded.model_name,\n                                        device_serial_number=excluded.device_serial_number,\n                                        study_id_fk=excluded.study_id_fk\n                                \"\"\", (st_pk, se.series_instance_uid, se.modality, se.series_number, man, mod, sn))\n                                saved_se += 1\n\n                            se_pk_row = cur.execute(\"SELECT id FROM series WHERE series_instance_uid=?\", (se.series_instance_uid,)).fetchone()\n                            if not se_pk_row: continue\n                            se_pk = se_pk_row[0]\n\n                            # --- Deletion Handling (Diff DB vs Memory) ---\n                            # Only perform if we suspect deletions or periodically? \n                            # Plan says: Implement Diff Logic.\n                            # Optimization: If series is NOT dirty, can we assume no deletions?\n                            # Not necessarily. Removing an item doesn't always mark Series dirty unless we hook \"remove\".\n                            # But DicomItem doesn't track removals from list automatically.\n                            # So we must check.\n\n                            db_uids_rows = cur.execute(\"SELECT sop_instance_uid FROM instances WHERE series_id_fk=?\", (se_pk,)).fetchall()\n                            db_uids = {r[0] for r in db_uids_rows}\n                            mem_uids = {i.sop_instance_uid for i in se.instances}\n\n                            to_delete = db_uids - mem_uids\n                            if to_delete:\n                                cur.executemany(\"DELETE FROM instances WHERE sop_instance_uid=?\", [(u,) for u in to_delete])\n                                saved_i += 0 # Or count negative?\n                                # self.logger.debug(f\"Deleted {len(to_delete)} instances from Series {se.series_instance_uid}\")\n\n                            # --- Upsert Dirty ---\n                            dirty_items = []\n                            for i in se.instances:\n                                if getattr(i, '_dirty', True):\n                                    # Capture version if available (robustness against race)\n                                    ver = getattr(i, '_mod_count', 0)\n                                    dirty_items.append((i, ver))\n\n                            if dirty_items:\n                                i_batch = []\n                                for inst, ver in dirty_items:\n                                    full_data = self._serialize_item(inst)\n                                    attrs_json = json.dumps(full_data, cls=GantryJSONEncoder)\n\n                                    p_offset, p_length, p_alg = None, None, None\n\n                                    if inst.pixel_array is not None:\n                                         b_data = inst.pixel_array.tobytes()\n                                         c_alg = 'zlib'\n                                         off, leng = sidecar_manager.write_frame(b_data, c_alg)\n                                         p_offset, p_length, p_alg = off, leng, c_alg\n                                         pixel_bytes_written += leng\n                                         pixel_frames_written += 1\n\n                                         # Update loader so we can unload safely later\n                                         inst._pixel_loader = self._create_pixel_loader(off, leng, c_alg, inst)\n                                    elif isinstance(inst._pixel_loader, SidecarPixelLoader):\n                                         # Already persisted (swapped), preserve metadata\n                                         p_offset = inst._pixel_loader.offset\n                                         p_length = inst._pixel_loader.length\n                                         p_alg = inst._pixel_loader.alg\n\n                                    i_batch.append((\n                                        se_pk, \n                                        inst.sop_instance_uid, \n                                        inst.sop_class_uid, \n                                        inst.instance_number, \n                                        inst.file_path, \n                                        p_offset, \n                                        p_length, \n                                        p_alg, \n                                        attrs_json\n                                    ))\n\n                                cur.executemany(\"\"\"\n                                    INSERT INTO instances (series_id_fk, sop_instance_uid, sop_class_uid, instance_number, file_path, \n                                                           pixel_offset, pixel_length, compress_alg, attributes_json)\n                                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)\n                                    ON CONFLICT(sop_instance_uid) DO UPDATE SET\n                                        series_id_fk=excluded.series_id_fk,\n                                        sop_class_uid=excluded.sop_class_uid,\n                                        instance_number=excluded.instance_number,\n                                        file_path=excluded.file_path,\n                                        attributes_json=excluded.attributes_json,\n                                        pixel_offset=COALESCE(excluded.pixel_offset, instances.pixel_offset),\n                                        pixel_length=COALESCE(excluded.pixel_length, instances.pixel_length),\n                                        compress_alg=COALESCE(excluded.compress_alg, instances.compress_alg)\n                                \"\"\", i_batch)\n                                saved_i += len(dirty_items)\n\n                                # Mark saved with version (deferred until commit success? \n                                # No, we can attach to list and do it post-commit)\n                                # But we're inside loops. \n                                # Creating a cleanup list:\n                                # (We can store dirty_items in a larger list to clean up post-commit)\n                                # For now, let's mark clean *assuming* commit will succeed.\n                                # If commit fails, we rollback, but objects remain \"clean\" in memory?\n                                # That is a risk. We should do it post-commit.\n                                # But scope is tricky. \n                                # Let's mark clean here but using version. \n                                # If transaction rolls back, DB is old, but memory has _saved_mod_count advanced?\n                                # That means next save won't save it. BAD.\n                                # We must hold off.\n\n                                # Since we commit once at the end:\n                                # We need to collect ALL dirty items and their versions.\n                                # That is expensive memory-wise for massive sets.\n                                # But necessary for correctness.\n                                # Compromise: we iterate again. \n                                # Wait, \"Iterate again\" in 'mark clean' loop below.\n                                # We can't know \"ver\" then.\n\n                                # Let's just update them here. If commit fails, the Exception propagates.\n                                # Use a try/except block around the whole `save_all`? Yes.\n                                # But `_saved_mod_count` is in memory.\n                                # If we update it, and `save_all` crashes, we can't easily undo it.\n                                # BUT `save_all` crashing usually kills the process or stops persistence.\n                                # So `eventual consistency` implies retrying.\n                                # If we marked it saved but it didn't save, we have data loss.\n\n                                # Correct way: List of callbacks?\n                                # Or just:\n                                for inst, ver in dirty_items:\n                                     if hasattr(inst, 'mark_saved'):\n                                          inst.mark_saved(ver)\n                                     else:\n                                          inst._dirty = False\n\n                conn.commit()\n\n                # Post-Commit: \n                # We already marked items as saved/clean incrementally using naive-commit assumption.\n                # If transaction failed, those items are marked clean in memory but not in DB -&gt; Inconsistency.\n                # However, re-implementing rollback for memory objects is out of scope.\n                # The versioning fixes the \"Overwrite valid change\" race, which is the user's issue.\n                pass\n\n                # Restore Logging Logic\n                if saved_p + saved_i &gt; 0:\n                     msg = f\"Save (Inc) complete. P:{saved_p} St:{saved_st} Se:{saved_se} I:{saved_i}.\"\n                     if pixel_frames_written &gt; 0:\n                         mb = pixel_bytes_written / (1024*1024)\n                         msg += f\" Sidecar: {pixel_frames_written} frames ({mb:.2f} MB).\"\n                     self.logger.info(msg)\n\n        except Exception as e:\n            self.logger.error(f\"Save failed: {e}\")\n            if hasattr(conn, \"rollback\"): conn.rollback()\n            raise\n\n    def get_total_instances(self) -&gt; int:\n        \"\"\"Returns the total number of instances currently persisted.\"\"\"\n        try:\n             with self._get_connection() as conn:\n                cur = conn.cursor()\n                row = cur.execute(\"SELECT COUNT(*) FROM instances\").fetchone()\n                return row[0] if row else 0\n        except sqlite3.Error as e:\n            self.logger.error(f\"Failed to count instances: {e}\")\n            return 0\n\n    def get_flattened_instances(self, patient_ids: List[str] = None, instance_uids: List[str] = None):\n        \"\"\"\n        Yields a flat dictionary for every instance in the DB (or filtered by patient_ids/instance_uids).\n        Ideal for streaming exports or analysis without loading the entire graph into RAM.\n        \"\"\"\n        # We use a managed connection that stays open during iteration\n        with self._get_connection() as conn:\n            # conn.row_factory = sqlite3.Row\n            cur = conn.cursor()\n\n            query = \"\"\"\n                SELECT \n                    p.patient_id, p.patient_name,\n                    st.study_instance_uid, st.study_date,\n                    s.series_instance_uid, s.modality, s.series_number, s.manufacturer, s.model_name, s.device_serial_number,\n                    i.sop_instance_uid, i.sop_class_uid, i.instance_number, i.file_path, \n                    i.pixel_offset, i.pixel_length, i.compress_alg, i.attributes_json\n                FROM instances i\n                JOIN series s ON i.series_id_fk = s.id\n                JOIN studies st ON s.study_id_fk = st.id\n                JOIN patients p ON st.patient_id_fk = p.id\n            \"\"\"\n\n            conditions = []\n            params = []\n\n            if patient_ids:\n                placeholders = \",\".join(\"?\" for _ in patient_ids)\n                conditions.append(f\"p.patient_id IN ({placeholders})\")\n                params.extend(patient_ids)\n\n            if instance_uids:\n                placeholders = \",\".join(\"?\" for _ in instance_uids)\n                conditions.append(f\"i.sop_instance_uid IN ({placeholders})\")\n                params.extend(instance_uids)\n\n            if conditions:\n                query += \" WHERE \" + \" AND \".join(conditions)\n\n            # Execute generator\n            cursor = cur.execute(query, params)\n\n            # We can map columns to names\n            cols = [desc[0] for desc in cursor.description]\n\n            for row in cursor:\n                yield dict(zip(cols, row))\n\n\n    def update_attributes(self, instances: List[Patient]):\n        \"\"\"\n        Efficiently updates the attributes_json for a list of instances.\n        \"\"\"\n        if not instances:\n            return\n\n        self.logger.info(f\"Updating attributes for {len(instances)} instances...\")\n        try:\n            with self._get_connection() as conn:\n                cur = conn.cursor()\n\n                # Pre-calculate data for executemany\n                data = []\n                for inst in instances:\n                    # Serialize attributes AND sequences\n                    full_data = self._serialize_item(inst)\n                    attrs_json = json.dumps(full_data, cls=GantryJSONEncoder)\n                    data.append((attrs_json, inst.sop_instance_uid))\n\n                cur.executemany(\"\"\"\n                    UPDATE instances \n                    SET attributes_json = ? \n                    WHERE sop_instance_uid = ?\n                \"\"\", data)\n\n                conn.commit()\n                self.logger.info(\"Update complete.\")\n\n        except sqlite3.Error as e:\n            self.logger.error(f\"Failed to update attributes: {e}\")\n\n    def save_findings(self, findings: List[PhiFinding]):\n        \"\"\"Persists PHI findings to the database.\"\"\"\n        timestamp = datetime.now().isoformat()\n\n        if not findings:\n            return\n\n        self.logger.info(f\"Saving {len(findings)} PHI findings...\")\n\n        try:\n            with self._get_connection() as conn:\n                cur = conn.cursor()\n\n                # Prepare Data Generator for Batch Insert (Memory Efficient)\n                def findings_generator():\n                    for f in findings:\n                        rem_action = None\n                        rem_value = None\n                        if f.remediation_proposal:\n                            rem_action = f.remediation_proposal.action_type\n                            rem_value = str(f.remediation_proposal.new_value)\n\n                        yield (\n                            timestamp, \n                            f.entity_uid, \n                            f.entity_type, \n                            f.field_name, \n                            str(f.value), \n                            f.reason, \n                            f.patient_id, \n                            rem_action, \n                            rem_value, \n                            \"{}\"\n                        )\n\n                cur.executemany(\"\"\"\n                    INSERT INTO phi_findings \n                    (timestamp, entity_uid, entity_type, field_name, value, reason, patient_id, remediation_action, remediation_value, details_json) \n                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n                \"\"\", findings_generator())\n\n                conn.commit()\n                self.logger.info(\"Findings saved.\")\n\n        except sqlite3.Error as e:\n            self.logger.error(f\"Failed to save findings: {e}\")\n\n    def load_findings(self) -&gt; List[PhiFinding]:\n        \"\"\"Loads all findings from the database.\"\"\"\n        findings = []\n        if self.db_path != \":memory:\" and not os.path.exists(self.db_path):\n            return findings\n\n        try:\n             with self._get_connection() as conn:\n                # conn.row_factory = sqlite3.Row\n                cur = conn.cursor()\n                # Check if table exists (backward compatibility for old DBs if init didnt run on them)\n                # But _init_db runs on __init__, so schema should be there.\n\n                rows = cur.execute(\"SELECT * FROM phi_findings ORDER BY id\").fetchall()\n\n                for r in rows:\n                    if r['remediation_action']:\n                        prop = PhiRemediation(r['remediation_action'], r['field_name'], r['remediation_value'], None) \n                    else: \n                        prop = None\n\n                    f = PhiFinding(\n                        entity_uid=r['entity_uid'],\n                        entity_type=r['entity_type'],\n                        field_name=r['field_name'],\n                        value=r['value'],\n                        reason=r['reason'],\n                        patient_id=r['patient_id'],\n                        remediation_proposal=prop\n                    )\n                    findings.append(f)\n\n        except sqlite3.Error as e:\n            self.logger.error(f\"Failed to load findings: {e}\")\n\n        return findings\n</code></pre>"},{"location":"api/persistence/#gantry.persistence.SqliteStore.get_flattened_instances","title":"<code>get_flattened_instances(patient_ids=None, instance_uids=None)</code>","text":"<p>Yields a flat dictionary for every instance in the DB (or filtered by patient_ids/instance_uids). Ideal for streaming exports or analysis without loading the entire graph into RAM.</p> Source code in <code>gantry/persistence.py</code> <pre><code>def get_flattened_instances(self, patient_ids: List[str] = None, instance_uids: List[str] = None):\n    \"\"\"\n    Yields a flat dictionary for every instance in the DB (or filtered by patient_ids/instance_uids).\n    Ideal for streaming exports or analysis without loading the entire graph into RAM.\n    \"\"\"\n    # We use a managed connection that stays open during iteration\n    with self._get_connection() as conn:\n        # conn.row_factory = sqlite3.Row\n        cur = conn.cursor()\n\n        query = \"\"\"\n            SELECT \n                p.patient_id, p.patient_name,\n                st.study_instance_uid, st.study_date,\n                s.series_instance_uid, s.modality, s.series_number, s.manufacturer, s.model_name, s.device_serial_number,\n                i.sop_instance_uid, i.sop_class_uid, i.instance_number, i.file_path, \n                i.pixel_offset, i.pixel_length, i.compress_alg, i.attributes_json\n            FROM instances i\n            JOIN series s ON i.series_id_fk = s.id\n            JOIN studies st ON s.study_id_fk = st.id\n            JOIN patients p ON st.patient_id_fk = p.id\n        \"\"\"\n\n        conditions = []\n        params = []\n\n        if patient_ids:\n            placeholders = \",\".join(\"?\" for _ in patient_ids)\n            conditions.append(f\"p.patient_id IN ({placeholders})\")\n            params.extend(patient_ids)\n\n        if instance_uids:\n            placeholders = \",\".join(\"?\" for _ in instance_uids)\n            conditions.append(f\"i.sop_instance_uid IN ({placeholders})\")\n            params.extend(instance_uids)\n\n        if conditions:\n            query += \" WHERE \" + \" AND \".join(conditions)\n\n        # Execute generator\n        cursor = cur.execute(query, params)\n\n        # We can map columns to names\n        cols = [desc[0] for desc in cursor.description]\n\n        for row in cursor:\n            yield dict(zip(cols, row))\n</code></pre>"},{"location":"api/persistence/#gantry.persistence.SqliteStore.get_total_instances","title":"<code>get_total_instances()</code>","text":"<p>Returns the total number of instances currently persisted.</p> Source code in <code>gantry/persistence.py</code> <pre><code>def get_total_instances(self) -&gt; int:\n    \"\"\"Returns the total number of instances currently persisted.\"\"\"\n    try:\n         with self._get_connection() as conn:\n            cur = conn.cursor()\n            row = cur.execute(\"SELECT COUNT(*) FROM instances\").fetchone()\n            return row[0] if row else 0\n    except sqlite3.Error as e:\n        self.logger.error(f\"Failed to count instances: {e}\")\n        return 0\n</code></pre>"},{"location":"api/persistence/#gantry.persistence.SqliteStore.load_all","title":"<code>load_all()</code>","text":"<p>Reconstructs the entire object graph from the database. Returns a list of Patient objects.</p> Source code in <code>gantry/persistence.py</code> <pre><code>def load_all(self) -&gt; List[Patient]:\n    \"\"\"\n    Reconstructs the entire object graph from the database.\n    Returns a list of Patient objects.\n    \"\"\"\n    patients = []\n    if self.db_path != \":memory:\" and not os.path.exists(self.db_path):\n        return patients\n\n    try:\n        with self._get_connection() as conn:\n            # conn.row_factory = sqlite3.Row  &lt;-- Handled by _get_connection\n            cur = conn.cursor()\n\n            # Optimized: We could do joins, but for clarity/mapping let's do hierarchical fetch.\n            # Or fetch all and Stitch. Stitching in memory is faster for SQLite than N+1 queries.\n\n            # 1. Fetch AlL\n            p_rows = cur.execute(\"SELECT * FROM patients\").fetchall()\n            st_rows = cur.execute(\"SELECT * FROM studies\").fetchall()\n            se_rows = cur.execute(\"SELECT * FROM series\").fetchall()\n            i_rows = cur.execute(\"SELECT * FROM instances\").fetchall()\n\n\n\n            # 2. Build Maps\n            p_map = {}\n            for r in p_rows:\n                p = Patient(r['patient_id'], r['patient_name'])\n                p_map[r['id']] = p\n                patients.append(p)\n\n            st_map = {}\n            for r in st_rows:\n                st = Study(r['study_instance_uid'], r['study_date'])\n                st_map[r['id']] = st\n                if r['patient_id_fk'] in p_map:\n                    p_map[r['patient_id_fk']].studies.append(st)\n\n            se_map = {}\n            for r in se_rows:\n                se = Series(r['series_instance_uid'], r['modality'], r['series_number'])\n                if r['manufacturer'] or r['model_name']:\n                    se.equipment = Equipment(r['manufacturer'], r['model_name'], r['device_serial_number'])\n                se_map[r['id']] = se\n                if r['study_id_fk'] in st_map:\n                    st_map[r['study_id_fk']].series.append(se)\n\n            for r in i_rows:\n                inst = Instance(\n                    r['sop_instance_uid'], \n                    r['sop_class_uid'], \n                    r['instance_number'], \n                    file_path=r['file_path']\n                )\n\n                # Wire up Sidecar Loader if present\n                if r['pixel_offset'] is not None and r['pixel_length'] is not None:\n                     # Capture closure vars\n                     offset = r['pixel_offset']\n                     length = r['pixel_length']\n                     alg = r['compress_alg']\n\n                     # We need to reshape after loading. The dimensions are in attributes.\n                     # We can do this inside the lambda wrapper or a helper method.\n                     # But Instance.attributes aren't populated yet! \n                     # Wait, we populate attributes right after this.\n                     # So the lambda calls self.instance methods? No, lambda binds early.\n\n                     inst._pixel_loader = self._create_pixel_loader(r['pixel_offset'], r['pixel_length'], r['compress_alg'], inst)\n\n                # Restore extra attributes\n                if r['attributes_json']:\n                    try:\n                        attrs = json.loads(r['attributes_json'], object_hook=gantry_json_object_hook)\n                        self._deserialize_into(inst, attrs)\n                    except: \n                        pass # JSON error\n\n                if r['series_id_fk'] in se_map:\n                    se_map[r['series_id_fk']].instances.append(inst)\n\n        self.logger.info(f\"Loaded {len(patients)} patients from {self.db_path}\")\n        # Mark all loaded data as clean so we don't save it back immediately\n        for p in patients:\n            p.mark_clean()\n        return patients\n\n    except sqlite3.Error as e:\n        print(f\"DEBUG: Failed to load from DB: {e}\")\n        self.logger.error(f\"Failed to load PDF from DB: {e}\")\n        import traceback\n        traceback.print_exc()\n        return []\n</code></pre>"},{"location":"api/persistence/#gantry.persistence.SqliteStore.load_findings","title":"<code>load_findings()</code>","text":"<p>Loads all findings from the database.</p> Source code in <code>gantry/persistence.py</code> <pre><code>def load_findings(self) -&gt; List[PhiFinding]:\n    \"\"\"Loads all findings from the database.\"\"\"\n    findings = []\n    if self.db_path != \":memory:\" and not os.path.exists(self.db_path):\n        return findings\n\n    try:\n         with self._get_connection() as conn:\n            # conn.row_factory = sqlite3.Row\n            cur = conn.cursor()\n            # Check if table exists (backward compatibility for old DBs if init didnt run on them)\n            # But _init_db runs on __init__, so schema should be there.\n\n            rows = cur.execute(\"SELECT * FROM phi_findings ORDER BY id\").fetchall()\n\n            for r in rows:\n                if r['remediation_action']:\n                    prop = PhiRemediation(r['remediation_action'], r['field_name'], r['remediation_value'], None) \n                else: \n                    prop = None\n\n                f = PhiFinding(\n                    entity_uid=r['entity_uid'],\n                    entity_type=r['entity_type'],\n                    field_name=r['field_name'],\n                    value=r['value'],\n                    reason=r['reason'],\n                    patient_id=r['patient_id'],\n                    remediation_proposal=prop\n                )\n                findings.append(f)\n\n    except sqlite3.Error as e:\n        self.logger.error(f\"Failed to load findings: {e}\")\n\n    return findings\n</code></pre>"},{"location":"api/persistence/#gantry.persistence.SqliteStore.load_patient","title":"<code>load_patient(patient_uid)</code>","text":"<p>Loads a single patient and their graph from the DB by PatientID.</p> Source code in <code>gantry/persistence.py</code> <pre><code>def load_patient(self, patient_uid: str) -&gt; Optional[Patient]:\n    \"\"\"Loads a single patient and their graph from the DB by PatientID.\"\"\"\n    if self.db_path != \":memory:\" and not os.path.exists(self.db_path):\n        return None\n\n    try:\n         with self._get_connection() as conn:\n            # conn.row_factory = sqlite3.Row\n            cur = conn.cursor()\n\n            # Fetch Patient\n            p_row = cur.execute(\"SELECT * FROM patients WHERE patient_id = ?\", (patient_uid,)).fetchone()\n            if not p_row: return None\n\n            p = Patient(p_row['patient_id'], p_row['patient_name'])\n            p_pk = p_row['id']\n\n            # Fetch Studies\n            st_rows = cur.execute(\"SELECT * FROM studies WHERE patient_id_fk = ?\", (p_pk,)).fetchall()\n            for st_r in st_rows:\n                st = Study(st_r['study_instance_uid'], st_r['study_date'])\n                st_pk = st_r['id']\n\n                # Fetch Series\n                se_rows = cur.execute(\"SELECT * FROM series WHERE study_id_fk = ?\", (st_pk,)).fetchall()\n                for se_r in se_rows:\n                    se = Series(se_r['series_instance_uid'], se_r['modality'], se_r['series_number'])\n                    if se_r['manufacturer'] or se_r['model_name']:\n                        se.equipment = Equipment(se_r['manufacturer'], se_r['model_name'], se_r['device_serial_number'])\n                    se_pk = se_r['id']\n\n                    # Fetch Instances\n                    i_rows = cur.execute(\"SELECT * FROM instances WHERE series_id_fk = ?\", (se_pk,)).fetchall()\n                    for r in i_rows:\n                        inst = Instance(\n                            r['sop_instance_uid'], \n                            r['sop_class_uid'], \n                            r['instance_number'], \n                            file_path=r['file_path']\n                        )\n                        # Wire up Sidecar (Copy-Paste logic from load_all, keep generic?)\n                        # ideally refactor _hydrate_instance but inline is fine for now\n                        if r['pixel_offset'] is not None and r['pixel_length'] is not None:\n                            offset, length, alg = r['pixel_offset'], r['pixel_length'], r['compress_alg']\n                            inst._pixel_loader = self._create_pixel_loader(r['pixel_offset'], r['pixel_length'], r['compress_alg'], inst)\n\n                        if r['attributes_json']:\n                            try:\n                                attrs = json.loads(r['attributes_json'], object_hook=gantry_json_object_hook)\n                                self._deserialize_into(inst, attrs)\n                            except: pass\n\n                        se.instances.append(inst)\n\n                    st.series.append(se)\n                p.studies.append(st)\n\n            p.mark_clean()\n            return p\n    except sqlite3.Error as e:\n        self.logger.error(f\"Failed to load patient: {e}\")\n        return None\n</code></pre>"},{"location":"api/persistence/#gantry.persistence.SqliteStore.log_audit","title":"<code>log_audit(action_type, entity_uid, details)</code>","text":"<p>Records an action in the audit log (Async).</p> Source code in <code>gantry/persistence.py</code> <pre><code>def log_audit(self, action_type: str, entity_uid: str, details: str):\n    \"\"\"Records an action in the audit log (Async).\"\"\"\n    # Push to queue instead of writing directly\n    self.audit_queue.put((action_type, entity_uid, details))\n</code></pre>"},{"location":"api/persistence/#gantry.persistence.SqliteStore.log_audit_batch","title":"<code>log_audit_batch(entries)</code>","text":"<p>Batch inserts audit logs.  entries: List of (action_type, entity_uid, details)</p> Source code in <code>gantry/persistence.py</code> <pre><code>def log_audit_batch(self, entries: List[tuple]):\n    \"\"\"\n    Batch inserts audit logs. \n    entries: List of (action_type, entity_uid, details)\n    \"\"\"\n    if not entries: return\n\n    timestamp = datetime.now().isoformat()\n    # Prepare data with timestamp: (timestamp, action, uid, details)\n    data = [(timestamp, e[0], e[1], e[2]) for e in entries]\n\n    try:\n        with self._get_connection() as conn:\n            conn.executemany(\n                \"INSERT INTO audit_log (timestamp, action_type, entity_uid, details) VALUES (?, ?, ?, ?)\",\n                data\n            )\n            conn.commit()\n    except sqlite3.Error as e:\n        self.logger.error(f\"Failed to batch log audit: {e}\")\n</code></pre>"},{"location":"api/persistence/#gantry.persistence.SqliteStore.persist_pixel_data","title":"<code>persist_pixel_data(instance)</code>","text":"<p>Immediately persists pixel data to the sidecar to allow memory offloading. Does NOT update the full instance record in the main DB (attributes/json),  only the pixel linkage.</p> Source code in <code>gantry/persistence.py</code> <pre><code>def persist_pixel_data(self, instance: Instance):\n    \"\"\"\n    Immediately persists pixel data to the sidecar to allow memory offloading.\n    Does NOT update the full instance record in the main DB (attributes/json), \n    only the pixel linkage. \n    \"\"\"\n    if instance.pixel_array is None:\n        return\n\n    try:\n        # 1. Write to Sidecar\n        b_data = instance.pixel_array.tobytes()\n        # Determine suitable compression? Defaulting to zlib for swap.\n        # Ideally we respect original or config, but for swap zlib is safe/fast enough.\n        c_alg = 'zlib' \n\n        offset, length = self.sidecar.write_frame(b_data, c_alg)\n\n        # 2. Update Instance Loader\n        # This allows instance.unload_pixel_data() to work safely\n        instance._pixel_loader = self._create_pixel_loader(offset, length, c_alg, instance)\n\n        # 3. Optional: Persist the linkage to DB immediately?\n        # It's safer if we do, so if we crash, we know where the pixels are.\n        # However, if we don't save the attributes/UID changes, the DB is out of sync anyway.\n        # But the primary goal here is MEMORY MANAGEMENT.\n        # So updating the object state in memory (step 2) is sufficient for unload_pixel_data() to return True.\n        # The final session.save() will record the new offset/length into the DB instances table.\n\n    except Exception as e:\n        self.logger.error(f\"Failed to persist pixel swap for {instance.sop_instance_uid}: {e}\")\n        raise e\n</code></pre>"},{"location":"api/persistence/#gantry.persistence.SqliteStore.save_all","title":"<code>save_all(patients)</code>","text":"<p>Persists the current state incrementally. Strategy: UPSERT dirty items.</p> Source code in <code>gantry/persistence.py</code> <pre><code>def save_all(self, patients: List[Patient]):\n    \"\"\"\n    Persists the current state incrementally.\n    Strategy: UPSERT dirty items.\n    \"\"\"\n    self.logger.info(f\"Saving {len(patients)} patients to {self.db_path} (Incremental)...\")\n\n    pixel_bytes_written = 0\n    pixel_frames_written = 0\n    sidecar_manager = self.sidecar\n\n    try:\n        with self._get_connection() as conn:\n            cur = conn.cursor()\n\n            # Check for schema compatibility (simple check)\n            try:\n                # We rely on UNIQUE constraints for UPSERT. \n                # If older DB without constraints, we might fail or duplicate.\n                pass \n            except: pass\n\n            # Counts for reporting\n            saved_p, saved_st, saved_se, saved_i = 0, 0, 0, 0\n\n            for p in patients:\n                # Patient Level (Always Check Dirty)\n                if getattr(p, '_dirty', True):\n                    cur.execute(\"\"\"\n                        INSERT INTO patients (patient_id, patient_name) VALUES (?, ?)\n                        ON CONFLICT(patient_id) DO UPDATE SET patient_name=excluded.patient_name\n                    \"\"\", (p.patient_id, p.patient_name))\n                    saved_p += 1\n\n                # We need the PK for children\n                # Since we might have just updated or it might exist, we select it.\n                # Optimization: Cache PKs? For now, fetch is safe.\n                p_pk_row = cur.execute(\"SELECT id FROM patients WHERE patient_id=?\", (p.patient_id,)).fetchone()\n                if not p_pk_row: continue # Should not happen after Insert\n                p_pk = p_pk_row[0]\n\n                for st in p.studies:\n                    if getattr(st, '_dirty', True):\n                        cur.execute(\"\"\"\n                            INSERT INTO studies (patient_id_fk, study_instance_uid, study_date) VALUES (?, ?, ?)\n                            ON CONFLICT(study_instance_uid) DO UPDATE SET \n                                study_date=excluded.study_date,\n                                patient_id_fk=excluded.patient_id_fk\n                        \"\"\", (p_pk, st.study_instance_uid, st.study_date))\n                        saved_st += 1\n\n                    st_pk_row = cur.execute(\"SELECT id FROM studies WHERE study_instance_uid=?\", (st.study_instance_uid,)).fetchone()\n                    if not st_pk_row: continue\n                    st_pk = st_pk_row[0]\n\n                    for se in st.series:\n                        if getattr(se, '_dirty', True):\n                            man = se.equipment.manufacturer if se.equipment else \"\"\n                            mod = se.equipment.model_name if se.equipment else \"\"\n                            sn = se.equipment.device_serial_number if se.equipment else \"\"\n\n                            cur.execute(\"\"\"\n                                INSERT INTO series (study_id_fk, series_instance_uid, modality, series_number, manufacturer, model_name, device_serial_number)\n                                VALUES (?, ?, ?, ?, ?, ?, ?)\n                                ON CONFLICT(series_instance_uid) DO UPDATE SET \n                                    modality=excluded.modality,\n                                    series_number=excluded.series_number,\n                                    manufacturer=excluded.manufacturer,\n                                    model_name=excluded.model_name,\n                                    device_serial_number=excluded.device_serial_number,\n                                    study_id_fk=excluded.study_id_fk\n                            \"\"\", (st_pk, se.series_instance_uid, se.modality, se.series_number, man, mod, sn))\n                            saved_se += 1\n\n                        se_pk_row = cur.execute(\"SELECT id FROM series WHERE series_instance_uid=?\", (se.series_instance_uid,)).fetchone()\n                        if not se_pk_row: continue\n                        se_pk = se_pk_row[0]\n\n                        # --- Deletion Handling (Diff DB vs Memory) ---\n                        # Only perform if we suspect deletions or periodically? \n                        # Plan says: Implement Diff Logic.\n                        # Optimization: If series is NOT dirty, can we assume no deletions?\n                        # Not necessarily. Removing an item doesn't always mark Series dirty unless we hook \"remove\".\n                        # But DicomItem doesn't track removals from list automatically.\n                        # So we must check.\n\n                        db_uids_rows = cur.execute(\"SELECT sop_instance_uid FROM instances WHERE series_id_fk=?\", (se_pk,)).fetchall()\n                        db_uids = {r[0] for r in db_uids_rows}\n                        mem_uids = {i.sop_instance_uid for i in se.instances}\n\n                        to_delete = db_uids - mem_uids\n                        if to_delete:\n                            cur.executemany(\"DELETE FROM instances WHERE sop_instance_uid=?\", [(u,) for u in to_delete])\n                            saved_i += 0 # Or count negative?\n                            # self.logger.debug(f\"Deleted {len(to_delete)} instances from Series {se.series_instance_uid}\")\n\n                        # --- Upsert Dirty ---\n                        dirty_items = []\n                        for i in se.instances:\n                            if getattr(i, '_dirty', True):\n                                # Capture version if available (robustness against race)\n                                ver = getattr(i, '_mod_count', 0)\n                                dirty_items.append((i, ver))\n\n                        if dirty_items:\n                            i_batch = []\n                            for inst, ver in dirty_items:\n                                full_data = self._serialize_item(inst)\n                                attrs_json = json.dumps(full_data, cls=GantryJSONEncoder)\n\n                                p_offset, p_length, p_alg = None, None, None\n\n                                if inst.pixel_array is not None:\n                                     b_data = inst.pixel_array.tobytes()\n                                     c_alg = 'zlib'\n                                     off, leng = sidecar_manager.write_frame(b_data, c_alg)\n                                     p_offset, p_length, p_alg = off, leng, c_alg\n                                     pixel_bytes_written += leng\n                                     pixel_frames_written += 1\n\n                                     # Update loader so we can unload safely later\n                                     inst._pixel_loader = self._create_pixel_loader(off, leng, c_alg, inst)\n                                elif isinstance(inst._pixel_loader, SidecarPixelLoader):\n                                     # Already persisted (swapped), preserve metadata\n                                     p_offset = inst._pixel_loader.offset\n                                     p_length = inst._pixel_loader.length\n                                     p_alg = inst._pixel_loader.alg\n\n                                i_batch.append((\n                                    se_pk, \n                                    inst.sop_instance_uid, \n                                    inst.sop_class_uid, \n                                    inst.instance_number, \n                                    inst.file_path, \n                                    p_offset, \n                                    p_length, \n                                    p_alg, \n                                    attrs_json\n                                ))\n\n                            cur.executemany(\"\"\"\n                                INSERT INTO instances (series_id_fk, sop_instance_uid, sop_class_uid, instance_number, file_path, \n                                                       pixel_offset, pixel_length, compress_alg, attributes_json)\n                                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)\n                                ON CONFLICT(sop_instance_uid) DO UPDATE SET\n                                    series_id_fk=excluded.series_id_fk,\n                                    sop_class_uid=excluded.sop_class_uid,\n                                    instance_number=excluded.instance_number,\n                                    file_path=excluded.file_path,\n                                    attributes_json=excluded.attributes_json,\n                                    pixel_offset=COALESCE(excluded.pixel_offset, instances.pixel_offset),\n                                    pixel_length=COALESCE(excluded.pixel_length, instances.pixel_length),\n                                    compress_alg=COALESCE(excluded.compress_alg, instances.compress_alg)\n                            \"\"\", i_batch)\n                            saved_i += len(dirty_items)\n\n                            # Mark saved with version (deferred until commit success? \n                            # No, we can attach to list and do it post-commit)\n                            # But we're inside loops. \n                            # Creating a cleanup list:\n                            # (We can store dirty_items in a larger list to clean up post-commit)\n                            # For now, let's mark clean *assuming* commit will succeed.\n                            # If commit fails, we rollback, but objects remain \"clean\" in memory?\n                            # That is a risk. We should do it post-commit.\n                            # But scope is tricky. \n                            # Let's mark clean here but using version. \n                            # If transaction rolls back, DB is old, but memory has _saved_mod_count advanced?\n                            # That means next save won't save it. BAD.\n                            # We must hold off.\n\n                            # Since we commit once at the end:\n                            # We need to collect ALL dirty items and their versions.\n                            # That is expensive memory-wise for massive sets.\n                            # But necessary for correctness.\n                            # Compromise: we iterate again. \n                            # Wait, \"Iterate again\" in 'mark clean' loop below.\n                            # We can't know \"ver\" then.\n\n                            # Let's just update them here. If commit fails, the Exception propagates.\n                            # Use a try/except block around the whole `save_all`? Yes.\n                            # But `_saved_mod_count` is in memory.\n                            # If we update it, and `save_all` crashes, we can't easily undo it.\n                            # BUT `save_all` crashing usually kills the process or stops persistence.\n                            # So `eventual consistency` implies retrying.\n                            # If we marked it saved but it didn't save, we have data loss.\n\n                            # Correct way: List of callbacks?\n                            # Or just:\n                            for inst, ver in dirty_items:\n                                 if hasattr(inst, 'mark_saved'):\n                                      inst.mark_saved(ver)\n                                 else:\n                                      inst._dirty = False\n\n            conn.commit()\n\n            # Post-Commit: \n            # We already marked items as saved/clean incrementally using naive-commit assumption.\n            # If transaction failed, those items are marked clean in memory but not in DB -&gt; Inconsistency.\n            # However, re-implementing rollback for memory objects is out of scope.\n            # The versioning fixes the \"Overwrite valid change\" race, which is the user's issue.\n            pass\n\n            # Restore Logging Logic\n            if saved_p + saved_i &gt; 0:\n                 msg = f\"Save (Inc) complete. P:{saved_p} St:{saved_st} Se:{saved_se} I:{saved_i}.\"\n                 if pixel_frames_written &gt; 0:\n                     mb = pixel_bytes_written / (1024*1024)\n                     msg += f\" Sidecar: {pixel_frames_written} frames ({mb:.2f} MB).\"\n                 self.logger.info(msg)\n\n    except Exception as e:\n        self.logger.error(f\"Save failed: {e}\")\n        if hasattr(conn, \"rollback\"): conn.rollback()\n        raise\n</code></pre>"},{"location":"api/persistence/#gantry.persistence.SqliteStore.save_findings","title":"<code>save_findings(findings)</code>","text":"<p>Persists PHI findings to the database.</p> Source code in <code>gantry/persistence.py</code> <pre><code>def save_findings(self, findings: List[PhiFinding]):\n    \"\"\"Persists PHI findings to the database.\"\"\"\n    timestamp = datetime.now().isoformat()\n\n    if not findings:\n        return\n\n    self.logger.info(f\"Saving {len(findings)} PHI findings...\")\n\n    try:\n        with self._get_connection() as conn:\n            cur = conn.cursor()\n\n            # Prepare Data Generator for Batch Insert (Memory Efficient)\n            def findings_generator():\n                for f in findings:\n                    rem_action = None\n                    rem_value = None\n                    if f.remediation_proposal:\n                        rem_action = f.remediation_proposal.action_type\n                        rem_value = str(f.remediation_proposal.new_value)\n\n                    yield (\n                        timestamp, \n                        f.entity_uid, \n                        f.entity_type, \n                        f.field_name, \n                        str(f.value), \n                        f.reason, \n                        f.patient_id, \n                        rem_action, \n                        rem_value, \n                        \"{}\"\n                    )\n\n            cur.executemany(\"\"\"\n                INSERT INTO phi_findings \n                (timestamp, entity_uid, entity_type, field_name, value, reason, patient_id, remediation_action, remediation_value, details_json) \n                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n            \"\"\", findings_generator())\n\n            conn.commit()\n            self.logger.info(\"Findings saved.\")\n\n    except sqlite3.Error as e:\n        self.logger.error(f\"Failed to save findings: {e}\")\n</code></pre>"},{"location":"api/persistence/#gantry.persistence.SqliteStore.stop","title":"<code>stop()</code>","text":"<p>Stops the audit worker and flushes queue.</p> Source code in <code>gantry/persistence.py</code> <pre><code>def stop(self):\n    \"\"\"Stops the audit worker and flushes queue.\"\"\"\n    self._stop_event.set()\n    if self._audit_thread.is_alive():\n        self._audit_thread.join(timeout=2.0)\n</code></pre>"},{"location":"api/persistence/#gantry.persistence.SqliteStore.update_attributes","title":"<code>update_attributes(instances)</code>","text":"<p>Efficiently updates the attributes_json for a list of instances.</p> Source code in <code>gantry/persistence.py</code> <pre><code>def update_attributes(self, instances: List[Patient]):\n    \"\"\"\n    Efficiently updates the attributes_json for a list of instances.\n    \"\"\"\n    if not instances:\n        return\n\n    self.logger.info(f\"Updating attributes for {len(instances)} instances...\")\n    try:\n        with self._get_connection() as conn:\n            cur = conn.cursor()\n\n            # Pre-calculate data for executemany\n            data = []\n            for inst in instances:\n                # Serialize attributes AND sequences\n                full_data = self._serialize_item(inst)\n                attrs_json = json.dumps(full_data, cls=GantryJSONEncoder)\n                data.append((attrs_json, inst.sop_instance_uid))\n\n            cur.executemany(\"\"\"\n                UPDATE instances \n                SET attributes_json = ? \n                WHERE sop_instance_uid = ?\n            \"\"\", data)\n\n            conn.commit()\n            self.logger.info(\"Update complete.\")\n\n    except sqlite3.Error as e:\n        self.logger.error(f\"Failed to update attributes: {e}\")\n</code></pre>"},{"location":"api/session/","title":"Session API","text":""},{"location":"api/session/#gantry.session.DicomSession","title":"<code>gantry.session.DicomSession</code>","text":"<p>The Main Facade for the Gantry library. Manages the lifecycle of the DicomStore (Load/Import/Redact/Export/Save).</p> Source code in <code>gantry/session.py</code> <pre><code>class DicomSession:\n    \"\"\"\n    The Main Facade for the Gantry library.\n    Manages the lifecycle of the DicomStore (Load/Import/Redact/Export/Save).\n    \"\"\"\n\n\n    def __init__(self, persistence_file=\"gantry.db\"):\n        \"\"\"\n        Initialize the DicomSession.\n\n        Args:\n            persistence_file: Path to the SQLite database for session persistence.\n        \"\"\"\n        configure_logger()\n        self.persistence_file = persistence_file\n\n        # Check existence before SqliteStore potentially creates it\n        import os\n        db_exists = os.path.exists(persistence_file)\n\n        self.store_backend = SqliteStore(persistence_file)\n        self.persistence_manager = PersistenceManager(self.store_backend)\n\n        # Hydrate memory from DB\n        self.store = DicomStore() \n\n        if db_exists:\n            print(f\"Loading session from {persistence_file}...\")\n        else:\n            print(f\"Initializing new session at {persistence_file}...\")\n\n        self.store.patients = self.store_backend.load_all()\n\n        self.active_rules: List[Dict[str, Any]] = []\n        self.active_phi_tags: Dict[str, str] = None\n        self.active_date_jitter: Dict[str, int] = {\"min_days\": -365, \"max_days\": -1}\n        self.active_remove_private_tags: bool = True\n\n        # Reversibility\n        self.key_manager = None\n        self.reversibility_service = None\n\n        if os.path.exists(\"gantry.key\"):\n            self.enable_reversible_anonymization(\"gantry.key\")\n\n        # Shared Global Executor for Process Consistency\n        # This prevents \"UserWarning: resource_tracker: ... semaphore released\"\n        self._executor = concurrent.futures.ProcessPoolExecutor(max_workers=None) # Default: CPU * 1.5\n\n        get_logger().info(f\"Session started. {len(self.store.patients)} patients loaded.\")\n\n    def close(self):\n        \"\"\"\n        Cleanly shuts down the session, stopping background threads and flushing queues.\n        \"\"\"\n        print(\"Closing session persistence...\")\n        if hasattr(self, 'persistence_manager'):\n            self.persistence_manager.shutdown()\n        if hasattr(self, 'store_backend'):\n            self.store_backend.stop() # Stops audit thread\n\n        if hasattr(self, '_executor'):\n            print(\"Shutting down process pool...\")\n            self._executor.shutdown(wait=True)\n\n    def _restart_executor(self, max_workers=None):\n        \"\"\"\n        Restarts the internal process pool executor, potentially with fewer workers.\n        Useful for recovering from BrokenProcessPool errors (OOM).\n        \"\"\"\n        get_logger().warning(f\"Restarting ProcessPoolExecutor (max_workers={max_workers})...\")\n        if self._executor:\n            try:\n                # Force kill old processes if they are stuck/broken\n                self._executor.shutdown(wait=False, cancel_futures=True)\n            except:\n                pass\n\n        # Re-init\n        self._executor = concurrent.futures.ProcessPoolExecutor(max_workers=max_workers)\n\n    def save(self):\n        \"\"\"\n        Persists the current session state to the database in the background.\n        User must call this manually to save changes.\n        \"\"\"\n        self.persistence_manager.save_async(self.store.patients)\n\n    def release_memory(self):\n        \"\"\"\n        Attempts to release memory by unloading pixel data from all instances.\n        Safe to call: only unloads data that is safely persisted (on disk or sidecar).\n        Useful after running extensive redaction or export operations.\n        \"\"\"\n        get_logger().info(\"Releasing memory (RAM cleanup)...\")\n        count = 0\n        freed = 0\n\n        # Count total instances first for progress bar\n        total_instances = sum(len(se.instances) for p in self.store.patients for st in p.studies for se in st.series)\n\n        if total_instances == 0:\n            return\n\n        from tqdm import tqdm\n        with tqdm(total=total_instances, desc=\"Releasing Memory\", unit=\"img\") as pbar:\n            for p in self.store.patients:\n                for st in p.studies:\n                    for se in st.series:\n                        for inst in se.instances:\n                            count += 1\n                            if inst.unload_pixel_data():\n                                freed += 1\n                            pbar.update(1)\n\n        get_logger().info(f\"Memory release complete. Unloaded pixels for {freed}/{count} instances.\")\n        if freed &gt; 0:\n            print(f\"Memory Cleanup: Released {freed} images from RAM.\")\n\n    def enable_reversible_anonymization(self, key_path: str = \"gantry.key\"):\n        \"\"\"\n        Initializes the encryption subsystem.\n        \"\"\"\n        self.key_manager = KeyManager(key_path)\n        self.key_manager.load_or_generate_key()\n        self.reversibility_service = ReversibilityService(self.key_manager)\n        get_logger().info(f\"Reversible anonymization enabled. Key: {key_path}\")\n\n    def lock_identities(self, patient_id: str, persist: bool = False, _patient_obj: \"Patient\" = None, verbose: bool = True, **kwargs) -&gt; List[\"Instance\"]:\n        \"\"\"\n        Securely embeds the original patient name/ID into a private DICOM tag\n        for all instances belonging to the specified patient.\n        Must be called BEFORE anonymization.\n\n        Args:\n            patient_id: The ID of the patient to preserve, OR a list/report for batch processing.\n            persist: If True, writes changes to DB immediately. If False, returns instances for batch persistence.\n            _patient_obj: Optimization argument to avoid O(N) lookup if patient is already known.\n            verbose: If True, logs debug information. Set to False for batch operations.\n            **kwargs: Additional arguments passed to lock_identities_batch (e.g. auto_persist_chunk_size).\n        \"\"\"\n        if not self.reversibility_service:\n            raise RuntimeError(\"Reversible anonymization not enabled. Call enable_reversible_anonymization() first.\")\n\n        # Dispatch to batch method if a list is provided\n        # This handles List[str] or List[Patient] automatically via lock_identities_batch logic\n        if isinstance(patient_id, (list, tuple, set)) or hasattr(patient_id, 'findings'):\n            return self.lock_identities_batch(patient_id, **kwargs)\n\n        if verbose:\n            get_logger().debug(f\"Preserving identity for {patient_id}...\")\n\n        modified_instances = []\n\n        if _patient_obj:\n            patient = _patient_obj\n        else:\n            patient = next((p for p in self.store.patients if p.patient_id == patient_id), None)\n\n        if not patient:\n            get_logger().error(f\"Patient {patient_id} not found.\")\n            return []\n\n        cnt = 0\n        original_attrs = {\n            \"PatientName\": patient.patient_name,\n            \"PatientID\": patient.patient_id\n        }\n\n        # Optimization: Encrypt once per patient\n        token = self.reversibility_service.generate_identity_token(original_attributes=original_attrs)\n\n        # Iterate deep\n        for st in patient.studies:\n            for se in st.series:\n                for inst in se.instances:\n                    # self.reversibility_service.embed_original_data(inst, original_attrs)\n                    self.reversibility_service.embed_identity_token(inst, token)\n                    modified_instances.append(inst)\n                    cnt += 1\n\n        if persist and modified_instances:\n            self.store_backend.update_attributes(modified_instances)\n            get_logger().info(f\"Secured identity in {cnt} instances for {patient_id}.\")\n\n        return modified_instances\n\n    def lock_identities_batch(self, patient_ids: Union[List[str], \"PhiReport\", List[\"PhiFinding\"]], auto_persist_chunk_size: int = 0) -&gt; List[\"Instance\"]:\n        \"\"\"\n        Batch process multiple patients to lock identities.\n        Returns a list of all modified instances (unless auto_persist_chunk_size is used).\n\n        Args:\n            patient_ids: List of PatientIDs, OR a PhiReport/list of objects with patient_id.\n            auto_persist_chunk_size: If &gt; 0, persists changes and releases memory every N instances.\n                                     IMPORTANT: Returns an empty list if enabled to prevent OOM.\n        \"\"\"\n        if not self.reversibility_service:\n            raise RuntimeError(\"Reversible anonymization not enabled.\")\n\n        # Normalize input to a set of strings\n        normalized_ids = set()\n\n        # Handle PhiReport or list containers\n        iterable_data = patient_ids\n        if hasattr(patient_ids, 'findings'): # PhiReport\n            iterable_data = patient_ids.findings\n\n        for item in iterable_data:\n            if isinstance(item, str):\n                normalized_ids.add(item)\n            elif hasattr(item, 'patient_id') and item.patient_id:\n                 normalized_ids.add(item.patient_id)\n\n        start_ids = list(normalized_ids)\n\n        modified_instances = [] # Only used if auto_persist_chunk_size == 0\n        current_chunk = []      # Used if auto_persist_chunk_size &gt; 0\n\n        count_patients = 0\n        count_instances_chunked = 0\n\n        from tqdm import tqdm\n\n        # Optimization: Create a lookup map for O(1) access\n        # This replaces the O(N) lookup per patient inside the loop\n        patient_map = {p.patient_id: p for p in self.store.patients}\n\n        with tqdm(start_ids, desc=\"Locking Identities\", unit=\"patient\") as pbar:\n            for pid in pbar:\n                p_obj = patient_map.get(pid)\n                if p_obj:\n                    # Use verbose=False to avoid log spam\n                    res = self.lock_identities(pid, persist=False, _patient_obj=p_obj, verbose=False)\n\n                    if auto_persist_chunk_size &gt; 0:\n                        current_chunk.extend(res)\n                        if len(current_chunk) &gt;= auto_persist_chunk_size:\n                            self.store_backend.update_attributes(current_chunk)\n                            count_instances_chunked += len(current_chunk)\n                            current_chunk = [] # Release memory\n                    else:\n                        modified_instances.extend(res)\n\n                    count_patients += 1\n                else:\n                     get_logger().error(f\"Patient {pid} not found (batch processing).\")\n\n        # Final cleanup\n        if auto_persist_chunk_size &gt; 0:\n            if current_chunk:\n                self.store_backend.update_attributes(current_chunk)\n                count_instances_chunked += len(current_chunk)\n\n            get_logger().info(f\"Batch preserved identity for {count_patients} patients ({count_instances_chunked} instances). Persisted incrementally.\")\n            return []\n\n        if modified_instances:\n             msg = f\"Preserved identity for {len(modified_instances)} instances.\"\n             print(f\"\\n{msg}\\nRemember to call .save() to persist changes.\")\n             get_logger().info(msg)\n\n        get_logger().info(f\"Batch preserved identity for {count_patients} patients ({len(modified_instances)} instances).\")\n        return modified_instances\n\n    def recover_patient_identity(self, patient_id: str):\n        \"\"\"\n        ... existing ...\n        \"\"\"\n        return self._recover_identity_logic(patient_id) # Simplify for brevity if needed, but I should just replace the loop\n\n    def _recover_identity_logic(self, patient_id: str):\n        \"\"\"\n        Internal helper to execute the identity recovery logic.\n        (Placeholder for shared logic between single/batch recovery).\n        \"\"\"\n        # implementation details\n        pass\n\n    def _make_lightweight_copy(self, patient: \"Patient\") -&gt; \"Patient\":\n        \"\"\"\n        Creates a swallow copy of the patient graph with pixel data stripped.\n        Used to prevent IPC buffer overflows (Windows) when passing to workers.\n        \"\"\"\n        from copy import copy\n\n        # P -&gt; St -&gt; Se -&gt; Inst\n        p_clone = copy(patient)\n        p_clone.studies = []\n\n        for st in patient.studies:\n            st_clone = copy(st)\n            st_clone.series = []\n            p_clone.studies.append(st_clone)\n\n            for se in st.series:\n                se_clone = copy(se)\n                se_clone.instances = []\n                st_clone.series.append(se_clone)\n\n                for inst in se.instances:\n                    # Clone instance\n                    i_clone = copy(inst)\n                    # Strip heavy fields\n                    try:\n                        i_clone.pixel_array = None\n                        i_clone._pixel_loader = None\n                    except: \n                        pass \n\n                    se_clone.instances.append(i_clone)\n\n        return p_clone\n\n    def audit(self, config_path: str = None) -&gt; \"PhiReport\":\n        \"\"\"\n        Scans all patients in the session for potential PHI.\n        Uses cached `active_phi_tags` if config_path matches or is None, otherwise loads fresh.\n        Returns a PhiReport object (iterable, and convertible to DataFrame).\n        Checkpoint 4: Target.\n        \"\"\"\n        from .privacy import PhiReport\n\n        # Logic: If config_path is provided, we should probably load it temporarily for this scan?\n        # OR if config_path is None, use self.active_phi_tags\n\n        tags_to_use = self.active_phi_tags\n\n        if config_path:\n             # Just load tags for this run, don't overwrite session state unless load_config called?\n             # Actually, if user says audit(\"file.json\"), they expect that file to control.\n             tags_to_use = ConfigLoader.load_phi_config(config_path)\n             # NOTE: If passing a config PATH to audit(), we might be missing the other unified settings \n             # (date_jitter, etc.) unless we load them too.\n             # For now, audit() focuses on finding things based on TAGS.\n             # If the inspector needs to know about date jitter or private tags to Flag them correctly?\n             # Private tags -&gt; YES. Jitter -&gt; Maybe not for detection, but definitely for Remediation proposal.\n\n             # Better approach: If config_path is Unified, load it all.\n             try:\n                 t, r, dj, rpt = ConfigLoader.load_unified_config(config_path)\n                 tags_to_use = t\n                 # We probably shouldn't overwrite session state side-effects here, \n                 # but for the worker arguments we need to pass them.\n                 # Let's create a transient config object or just pass args.\n                 # For simplicity in this function, we'll stick to tags, but we should fix inspector init.\n             except:\n                 # Fallback to simple tags load\n                 tags_to_use = ConfigLoader.load_phi_config(config_path)\n\n        inspector = PhiInspector(config_tags=tags_to_use, remove_private_tags=self.active_remove_private_tags)\n        if not inspector.phi_tags:\n            get_logger().warning(\"PHI Scan Warning: No PHI tags defined. Scan will find nothing. Check your config.\")\n\n        get_logger().info(\"Scanning for PHI (Parallel)...\")\n\n        # Hybrid Approach:\n        # Pass lightweight object CLONES to avoid \"Assert left &gt; 0\" IPC error\n        # AND to ensure we audit in-memory (unsaved) changes.\n        worker_args = []\n        for p in self.store.patients:\n            # Strip pixels to reduce size\n            light_p = self._make_lightweight_copy(p)\n            worker_args.append((light_p, tags_to_use, self.active_remove_private_tags))\n\n        results = run_parallel(scan_worker, worker_args, desc=\"Scanning PHI\")\n\n        all_findings = []\n        for findings in results:\n            all_findings.extend(findings)\n\n        # Rehydrate Entities!\n        self._rehydrate_findings(all_findings)\n\n        get_logger().info(f\"PHI Scan Complete. Found {len(all_findings)} issues.\")\n\n        return PhiReport(all_findings)\n\n    def scan_for_phi(self, config_path: str = None) -&gt; \"PhiReport\":\n        \"\"\"\n        DEPRECATED: Use audit() instead.\n        Alias for audit.\n        \"\"\"\n        get_logger().warning(\"DeprecationWarning: scan_for_phi() is deprecated. Please use audit() instead.\")\n        return self.audit(config_path)\n\n    def save_analysis(self, report):\n        \"\"\"\n        Persists the results of a PHI analysis to the database.\n        report: PhiReport or List[PhiFinding]\n        \"\"\"\n        findings = report\n        if hasattr(report, 'findings'):\n            findings = report.findings\n\n        self.store_backend.save_findings(findings)\n\n    def _rehydrate_findings(self, findings):\n        \"\"\"\n        Updates findings in-place to point to live objects in self.store\n        instead of the unpickled copies from workers.\n        \"\"\"\n        # Create lookup maps\n        # Assuming entity_uid is unique per type.\n        # Patient\n        patient_map = {p.patient_id: p for p in self.store.patients}\n\n        # Since traversing deep would be slow for every finding, we can do lazy or smart lookup\n        # Or just traverse once if needed.\n        # Most findings are on Patient or Study.\n\n        # Let's map Studies\n        study_map = {}\n        instance_map = {}\n\n        for p in self.store.patients:\n            for s in p.studies:\n                study_map[s.study_instance_uid] = s\n                for se in s.series:\n                    for i in se.instances:\n                        instance_map[i.sop_instance_uid] = i\n\n        for f in findings:\n            if f.entity_type == \"Patient\":\n                if f.entity_uid in patient_map:\n                    f.entity = patient_map[f.entity_uid]\n            elif f.entity_type == \"Study\":\n                if f.entity_uid in study_map:\n                    f.entity = study_map[f.entity_uid]\n            elif f.entity_type == \"Instance\":\n                if f.entity_uid in instance_map:\n                    f.entity = instance_map[f.entity_uid]\n\n    def recover_patient_identity(self, patient_id: str):\n        \"\"\"\n        Attempts to decrypt and read original identity from the first instance found.\n        \"\"\"\n        if not self.reversibility_service:\n            raise RuntimeError(\"Reversibility not enabled.\")\n\n        p = next((x for x in self.store.patients if x.patient_id == patient_id), None)\n        if not p:\n            print(\"Patient not found.\")\n            return\n\n        # Locate first instance\n        first_inst = None\n        for st in p.studies:\n            for se in st.series:\n                if se.instances:\n                    first_inst = se.instances[0]\n                    break\n\n        if not first_inst:\n            print(\"No instances found for patient.\")\n            return\n\n        original = self.reversibility_service.recover_original_data(first_inst)\n        if original:\n            print(\"Recovered Identity:\")\n            print(json.dumps(original, indent=2))\n        else:\n            print(\"No encrypted identity found or decryption failed.\")\n\n    def ingest(self, folder_path):\n        \"\"\"\n        Scans a folder for .dcm files (recursively) and imports them into the session.\n        \"\"\"\n        print(f\"Ingesting from '{folder_path}'...\")\n        DicomImporter.import_files([folder_path], self.store, executor=self._executor)\n\n        # Calculate stats\n        n_p = len(self.store.patients)\n        n_st = sum(len(p.studies) for p in self.store.patients)\n        n_se = sum(len(st.series) for p in self.store.patients for st in p.studies)\n        n_i = sum(len(se.instances) for p in self.store.patients for st in p.studies for se in st.series)\n\n        print(\"\\nIngestion Complete.\")\n        print(\"Summary:\")\n        print(f\"  - {n_p} Patients\")\n        print(f\"  - {n_st} Studies\")\n        print(f\"  - {n_se} Series\")\n        print(f\"  - {n_i} Instances\")\n        print(\"Remember to call .save() to persist changes.\\n\")\n\n    def examine(self):\n        \"\"\"Prints a summary of the session contents and equipment.\"\"\"\n        get_logger().info(\"Generating inventory report.\")\n\n        # 1. Object Counts\n        n_p = len(self.store.patients)\n        n_st = 0\n        n_se = 0\n        n_i = 0\n\n        # 2. Equipment Grouping\n        eq_counts = {} # (man, model) -&gt; count\n\n        for p in self.store.patients:\n            n_st += len(p.studies)\n            for s in p.studies:\n                n_se += len(s.series)\n                for se in s.series:\n                    n_i += len(se.instances)\n                    if se.equipment:\n                        key = (se.equipment.manufacturer, se.equipment.model_name)\n                        eq_counts[key] = eq_counts.get(key, 0) + 1\n\n        print(f\"\\nInventory Summary:\")\n        print(f\" Patients:  {n_p}\")\n        print(f\" Studies:   {n_st}\")\n        print(f\" Series:    {n_se}\")\n        print(f\" Instances: {n_i}\")\n\n        print(f\"\\nEquipment Inventory:\")\n        if not eq_counts:\n            print(\" No equipment metadata found.\")\n        else:\n            for (man, mod), count in sorted(eq_counts.items()):\n                print(f\" - {man} - {mod} (Count: {count})\")\n\n    def get_cohort_report(self) -&gt; 'pd.DataFrame':\n        \"\"\"\n        Returns a Pandas DataFrame containing flattened metadata for the current cohort.\n        Useful for analysis and QA.\n        \"\"\"\n        import pandas as pd\n        rows = []\n        for p in self.store.patients:\n            for s in p.studies:\n                for se in s.series:\n                    # Basic row info\n                    row = {\n                        \"PatientID\": p.patient_id,\n                        \"PatientName\": p.patient_name,\n                        \"StudyInstanceUID\": s.study_instance_uid,\n                        \"StudyDate\": s.study_date,\n                        \"SeriesInstanceUID\": se.series_instance_uid,\n                        \"Modality\": se.modality,\n                        \"InstanceCount\": len(se.instances)\n                    }\n                    if se.equipment:\n                        row[\"Manufacturer\"] = se.equipment.manufacturer\n                        row[\"Model\"] = se.equipment.model_name\n                        row[\"DeviceSerial\"] = se.equipment.device_serial_number\n                    else:\n                        row[\"Manufacturer\"] = \"\"\n                        row[\"Model\"] = \"\"\n                        row[\"DeviceSerial\"] = \"\"\n\n                    rows.append(row)\n\n        return pd.DataFrame(rows)\n\n    def export_dataframe(self, output_path: str = None, expand_metadata: bool = False) -&gt; 'pd.DataFrame':\n        \"\"\"\n        Exports the comprehensive session inventory to a Pandas DataFrame or Parquet file.\n        The dataframe contains a flattened hierarchy of Patient -&gt; Study -&gt; Series -&gt; Instance.\n\n        Args:\n            output_path: If provided, saves the dataframe to a parquet file (e.g. \"export.parquet\").\n            expand_metadata: If True, parses the 'attributes_json' column into separate columns for deep inspection.\n                             Warning: This can increase memory usage significantly.\n\n        Returns:\n            pd.DataFrame: The resulting dataframe.\n        \"\"\"\n        import pandas as pd\n        import json\n\n        get_logger().info(\"Generating comprehensive dataframe from persistence layer...\")\n\n        # Stream data from SQLite\n        # We convert the generator to a list for DataFrame construction.\n        # For massive datasets, this might need Chunking, but for &lt; 1M rows RAM is usually fine.\n        rows = list(self.store_backend.get_flattened_instances())\n\n        if not rows:\n            get_logger().warning(\"No data found to export.\")\n            return pd.DataFrame()\n\n        df = pd.DataFrame(rows)\n\n        # Normalize Column Names to match DICOM Keywords where possible\n        # SQL returns snake_case, we want PascalCase for consistency with DICOM Attributes\n        rename_map = {\n            \"patient_id\": \"PatientID\",\n            \"patient_name\": \"PatientName\",\n            \"study_instance_uid\": \"StudyInstanceUID\",\n            \"study_date\": \"StudyDate\",\n            \"series_instance_uid\": \"SeriesInstanceUID\",\n            \"modality\": \"Modality\",\n            \"series_number\": \"SeriesNumber\",\n            \"manufacturer\": \"Manufacturer\",\n            \"model_name\": \"ManufacturerModelName\",\n            \"device_serial_number\": \"DeviceSerialNumber\",\n            \"sop_instance_uid\": \"SOPInstanceUID\",\n            \"sop_class_uid\": \"SOPClassUID\",\n            \"instance_number\": \"InstanceNumber\",\n            \"file_path\": \"FilePath\"\n        }\n        df.rename(columns=rename_map, inplace=True)\n\n        if expand_metadata and 'attributes_json' in df.columns:\n            get_logger().info(\"Expanding metadata attributes (this may take a moment)...\")\n\n            def safe_load(x):\n                try: \n                    return json.loads(x) if x else {}\n                except: \n                    return {}\n\n            # normalize expects a list of dicts\n            meta_dicts = df['attributes_json'].apply(safe_load).tolist()\n            meta_df = pd.json_normalize(meta_dicts)\n\n            # Reset indices to ensure alignment (should be aligned by default but safe to be sure)\n            df = df.reset_index(drop=True)\n            meta_df = meta_df.reset_index(drop=True)\n\n            # Combine and drop the raw json\n            df = pd.concat([df.drop(columns=['attributes_json']), meta_df], axis=1)\n\n        if output_path:\n            get_logger().info(f\"Saving dataframe to parquet at {output_path}...\")\n            # usage of pyarrow is implicit via engine='pyarrow' default in recent pandas or auto-detect\n            try:\n                df.to_parquet(output_path, index=False)\n            except ImportError:\n                 # Fallback/Helpful error\n                 raise ImportError(\"pyarrow is required for parquet export. Please run 'pip install pyarrow'.\")\n\n            # If size is reasonable, maybe also print it?\n            import os\n            sz = os.path.getsize(output_path) / (1024 * 1024)\n            get_logger().info(f\"Parquet export complete ({sz:.2f} MB).\")\n\n        return df\n\n    def redact_by_machine(self, serial_number, roi):\n        \"\"\"\n        Manually triggers redaction for a machine.\n        roi: (r1, r2, c1, c2)\n        \"\"\"\n        svc = RedactionService(self.store, self.store_backend)\n        svc.redact_machine_region(serial_number, roi)\n\n    def anonymize(self, findings: List[PhiFinding]):\n        \"\"\"\n        Applies remediation to the current session based on findings.\n        Auto-logs to Audit Trail.\n        \"\"\"\n        svc = RemediationService(self.store_backend, date_jitter_config=self.active_date_jitter)\n        svc.apply_remediation(findings)\n\n        # Apply Global De-Identification Tags (compliance)\n        # We process ALL instances to ensure they are stamped\n        get_logger().info(\"Applying standard De-Identification Method tags...\")\n        print(\"Stamping De-Identification Method tags...\")\n\n        count = 0\n        for p in self.store.patients:\n            for st in p.studies:\n                for se in st.series:\n                    for inst in se.instances:\n                        svc.add_global_deid_tags(inst)\n                        count += 1\n\n        get_logger().info(f\" stamped {count} instances.\")\n\n    def export(self, folder, safe=False, compression=None, subset=None, show_progress=True):\n        \"\"\"\n        Exports the current state of all patients to a folder.\n        If safe=True, performs a fresh PHI scan and ONLY exports clean data.\n        compression: 'j2k' (JPEG 2000 Lossless) or None (Uncompressed)\n        subset: Optional filter. Can be:\n                - pd.DataFrame: A dataframe containing 'sop_instance_uid' column.\n                - str: A pandas querystring (e.g. \"Modality == 'CT'\") applied to the full index.\n                - List[str]: A list of SOPInstanceUIDs to export.\n        \"\"\"\n        # AUTO-OPTIMIZATION: Ensure clean memory before spawning processes\n        get_logger().info(\"Preparing for export (Auto-Save &amp; Memory Release)...\")\n        if show_progress:\n            print(\"Preparing for export (saving &amp; releasing memory)...\")\n        self.save()\n        self.persistence_manager.flush()\n        self.release_memory()\n\n        get_logger().info(f\"Exporting session to {folder} (safe={safe})...\")\n        if show_progress:\n            print(\"Exporting...\")\n\n        dirty_patients = set()\n        dirty_studies = set()\n\n        if safe:\n            if show_progress:\n                print(\"Running safety scan...\")\n            report = self.audit()\n            for finding in report:\n                if finding.entity_type == \"Patient\":\n                    dirty_patients.add(finding.entity_uid)\n                elif finding.entity_type == \"Study\":\n                    dirty_studies.add(finding.entity_uid)\n\n            if dirty_patients or dirty_studies:\n                # Group findings by Tag\n                tag_summary = {} # tag -&gt; {desc, count, example_val}\n                for finding in report:\n                    if finding.tag not in tag_summary:\n                        tag_summary[finding.tag] = {\n                            \"desc\": finding.field_name, \n                            \"count\": 0, \n                            \"examples\": set()\n                        }\n                    tag_summary[finding.tag][\"count\"] += 1\n                    if len(tag_summary[finding.tag][\"examples\"]) &lt; 3:\n                        tag_summary[finding.tag][\"examples\"].add(str(finding.value))\n\n                msg = f\"\\nSafety Scan Found Issues: {len(dirty_patients)} Patients, {len(dirty_studies)} Studies contain PHI.\"\n                msg += \"\\nThe following tags were flagged as dirty:\\n\"\n                msg += f\"{'Tag':&lt;15} {'Description':&lt;30} {'Count':&lt;10} {'Examples'}\\n\"\n                msg += \"-\" * 80 + \"\\n\"\n\n                config_suggestion = {}\n\n                for tag, info in tag_summary.items():\n                    examples = \", \".join(info['examples'])\n                    msg += f\"{tag:&lt;15} {info['desc']:&lt;30} {info['count']:&lt;10} {examples}\\n\"\n\n                    # Suggest REMOVE or KEEP based on ... usually REMOVE for PHI\n                    config_suggestion[tag] = {\"action\": \"REMOVE\", \"name\": info['desc']}\n\n                msg += \"\\nTo allow export, you must either REMOVE these tags or mark them as KEEP in your configuration.\\n\"\n                msg += \"Suggested Config Update:\\n\"\n                import json\n                msg += json.dumps({\"phi_tags\": config_suggestion}, indent=4)\n                msg += \"\\n\"\n\n                get_logger().warning(msg)\n                print(msg)\n\n        # Handle Subsetting\n        instance_uids = None\n        if subset is not None:\n            get_logger().info(\"Applying subset filter to export...\")\n            import pandas as pd\n\n            if isinstance(subset, pd.DataFrame):\n                if 'sop_instance_uid' not in subset.columns:\n                     # Check if index is UID? \n                     # For now, require column\n                     # Or maybe 'SOPInstanceUID' (DICOM standard case)\n                     if 'SOPInstanceUID' in subset.columns:\n                         instance_uids = subset['SOPInstanceUID'].tolist()\n                     else:\n                         raise ValueError(\"Subset DataFrame must contain 'sop_instance_uid' or 'SOPInstanceUID' column.\")\n                else:\n                     instance_uids = subset['sop_instance_uid'].tolist()\n\n            elif isinstance(subset, str):\n                # Query String -&gt; Generate DF -&gt; Filter\n                # Note: This loads full dataframe into memory to filter. \n                # Optimization: Could we push query to SQL? \n                # SQL is hard because attributes are JSON.\n                # So we export_dataframe() then query.\n                df = self.export_dataframe(expand_metadata=True) # Expand needed for meaningful queries\n                df_filtered = df.query(subset)\n                # handle both cases just in case\n                if 'SOPInstanceUID' in df_filtered.columns:\n                    instance_uids = df_filtered['SOPInstanceUID'].tolist()\n                elif 'sop_instance_uid' in df_filtered.columns:\n                    instance_uids = df_filtered['sop_instance_uid'].tolist()\n                else:\n                    # Fallback to index if it happens to be the key? Unlikely for now.\n                    raise ValueError(\"Column 'SOPInstanceUID' missing from internal dataframe.\")\n                get_logger().info(f\"Query '{subset}' matched {len(instance_uids)} instances.\")\n\n            elif isinstance(subset, list):\n                instance_uids = subset\n\n            if not instance_uids and instance_uids is not None:\n                get_logger().warning(\"Subset filter resulted in 0 instances. Nothing to export.\")\n                return\n\n        exported_count = 0\n        skipped_count = 0\n\n        # 1. Calculate Scope &amp; Total (In-Memory Helper)\n        patient_ids = []\n        total_instances = 0\n\n        # NOTE: If instance_uids is set, we can't easily count total_instances based on patients loop \n        # unless we do a DB count query.\n        # But for progress bar, we need a count.\n        if instance_uids:\n            total_instances = len(instance_uids)\n            # We don't populate patient_ids here because we pass instance_uids specifically.\n            # But the exporter supports patient_ids AND instance_uids.\n            # If we omit patient_ids, it searches all patients (inefficient SQL?).\n            # SqliteStore.get_flattened_instances currently:\n            # - if patient_ids: WHERE p.id IN ...\n            # - if instance_uids: WHERE i.uid IN ...\n            # Either works. If we have instance_uids, we don't need patient_ids.\n            # Although passing patient_ids reduces search space if we know them.\n            # Deriving patient_ids from instance_uids is expensive without a query.\n            # So we just pass instance_uids.\n        else:\n            for p in self.store.patients:\n                # Check Patient Level (Early Filter for ID list)\n                if safe and p.patient_id in dirty_patients:\n                    get_logger().warning(f\"Skipping Dirty Patient: {p.patient_id}\")\n                    skipped_count += getattr(p, 'instance_count', 0) # Estimate\n                    continue\n\n                patient_ids.append(p.patient_id)\n\n                # Count instances for progress bar\n                if safe:\n                    p_inst_count = 0\n                    for st in p.studies:\n                        if st.study_instance_uid not in dirty_studies:\n                            p_inst_count += sum(len(se.instances) for se in st.series)\n                    total_instances += p_inst_count\n                else:\n                    total_instances += sum(sum(len(se.instances) for se in st.series) for st in p.studies)\n\n        if not patient_ids and not instance_uids:\n            get_logger().warning(\"No valid patients to export.\")\n            return\n\n        # 2. Generate tasks (Streaming from DB)\n        get_logger().info(f\"Queuing export for {total_instances} instances (SQL Streaming)...\")\n\n        raw_tasks = DicomExporter.generate_export_from_db(\n            self.persistence_manager.store_backend, \n            folder, \n            patient_ids=patient_ids if not instance_uids else None, \n            compression=compression,\n            instance_uids=instance_uids\n        )\n\n        # 3. Apply Safety Filter (Lazy)\n        # The DB generation is flattened, so we filter stream based on Context attributes\n        def safety_filter(generator):\n            for ctx in generator:\n                # We already filtered patient_ids list passed to SQL, \n                # but we need to filter Studies if they are dirty.\n                if safe:\n                    uid = ctx.study_attributes.get(\"StudyInstanceUID\")\n                    if uid and uid in dirty_studies:\n                        continue\n                yield ctx\n\n        export_tasks = safety_filter(raw_tasks) if safe else raw_tasks\n\n        # 4. Execution Phase (Global Parallelism)\n        # 4. Execution Phase (Global Parallelism)\n        if total_instances &gt; 0:\n            # MEMORY LEAK MITIGATION:\n            # We use worker recycling (maxtasksperchild=100) via multiprocessing.Pool\n            # This forces workers to restart periodically, clearing any leaked memory (e.g. from C-libs).\n            # We do NOT use the shared self._executor for this, as ProcessPoolExecutor doesn't support recycling.\n            try:\n                # Optimized for stability: maxtasksperchild=25 clears memory frequently\n                # GC Optimization: Disable GC in workers\n                success_count = DicomExporter.export_batch(export_tasks, show_progress=show_progress, total=total_instances, maxtasksperchild=25, disable_gc=True)\n            except Exception as e:\n                get_logger().error(f\"Export Failed! Error: {e}\")\n                raise e\n            finally:\n                # Main process GC trigger\n                import gc\n                gc.collect()\n\n            # Note: skipped_count is only patient-level skips. Study-level skips aren't counted here explicitly \n            # unless we wrap the generator to count them, but that's complex for a simple log.\n            get_logger().info(f\"Export complete.\")\n        else:\n            get_logger().warning(\"No instances queued for export.\")\n\n        print(\"Done.\")\n\n    def export_to_parquet(self, output_path: str, patient_ids: List[str] = None):\n        \"\"\"\n        [EXPERIMENTAL] Exports flattened metadata to a Parquet file.\n\n        Requires 'pandas' and 'pyarrow' or 'fastparquet'.\n\n        Args:\n            output_path (str): Destination .parquet file path.\n            patient_ids (List[str], optional): List of PatientIDs to filter. Defaults to None (all currently in store).\n        \"\"\"\n        try:\n            import pandas as pd\n        except ImportError:\n            get_logger().error(\"export_to_parquet requires 'pandas' installed.\")\n            raise ImportError(\"Please install pandas to use this feature: pip install pandas pyarrow\")\n\n        # 1. Sync DB state\n        # If the user has unsaved changes, we should warn or auto-save.\n        # Currently, get_flattened_instances reads ONLY from DB.\n        # We'll auto-save just like normal export.\n        get_logger().info(\"Saving state before Parquet export...\")\n        self.save()\n\n        # 2. Stream Data\n        get_logger().info(\"Streaming data from database...\")\n\n        # We assume if patient_ids is None, we export ALL loaded patients (which matches DB if we just saved)\n        # However, sess.store.patients might be a subset if we implemented partial loading later.\n        # But for now, session manages a cohort.\n        target_ids = patient_ids\n        if target_ids is None:\n             target_ids = [p.patient_id for p in self.store.patients]\n\n        if not target_ids:\n            get_logger().warning(\"No patients to export.\")\n            return\n\n        generator = self.persistence_manager.store_backend.get_flattened_instances(target_ids)\n\n        # Materialize generator to list for DataFrame creation\n        # Note: This loads metadata into RAM. If 1M rows, might be heavy, but Parquet export needs batching or full DF usually.\n        # For a prototype, full load is fine using our lightweight dicts.\n        rows = list(generator)\n\n        if not rows:\n            get_logger().warning(\"No instances found for these patients.\")\n            return\n\n        df = pd.DataFrame(rows)\n\n        # 3. Save\n        get_logger().info(f\"Writing {len(df)} rows to {output_path}...\")\n\n        # Ensure directory exists\n        os.makedirs(os.path.dirname(os.path.abspath(output_path)), exist_ok=True)\n\n        try:\n            df.to_parquet(output_path, index=False)\n            get_logger().info(\"Parquet export successful.\")\n        except ImportError as e:\n             get_logger().error(\"Parquet engine (pyarrow or fastparquet) missing.\")\n             raise e\n        except Exception as e:\n            get_logger().error(f\"Failed to write parquet: {e}\")\n            raise\n\n    def load_config(self, config_file: str):\n        \"\"\"\n        User Action: 'Load these rules into memory, but DO NOT run them yet.'\n        Useful for validation or previewing what will happen.\n        \"\"\"\n        try:\n            get_logger().info(f\"Loading configuration from {config_file}...\")\n            print(f\"Loading configuration from {config_file}...\")\n\n            # UNIFIED LOAD (v2)\n            tags, rules, jitter, remove_private = ConfigLoader.load_unified_config(config_file)\n\n            self.active_phi_tags = tags\n            self.active_rules = rules\n            self.active_date_jitter = jitter\n            self.active_remove_private_tags = remove_private\n\n            get_logger().info(f\"Loaded {len(self.active_rules)} machine rules and {len(self.active_phi_tags)} PHI tags.\")\n            print(f\"Configuration Loaded:\\n - {len(self.active_rules)} Machine Redaction Rules\\n - {len(self.active_phi_tags)} PHI Tags\")\n            print(f\" - Date Jitter: {self.active_date_jitter['min_days']} to {self.active_date_jitter['max_days']} days\")\n            print(f\" - Remove Private Tags: {self.active_remove_private_tags}\")\n            print(\"Tip: Run .audit() to check PHI, or .redact_pixels() to apply redaction.\")\n        except Exception as e:\n            import traceback\n            get_logger().error(f\"Load failed: {e}\")\n            print(f\"Load failed: {e}\")\n            print(traceback.format_exc())\n            self.active_rules = []\n            self.active_phi_tags = {}\n\n    def preview_config(self):\n        \"\"\"\n        User Action: 'Tell me what WOULD happen if I ran these rules.'\n        checks the loaded rules against the current Store inventory.\n        \"\"\"\n        if not self.active_rules:\n            get_logger().warning(\"No configuration loaded. Use .load_config() first.\")\n            print(\"No configuration loaded. Use .load_config() first.\")\n            return\n\n        print(\"\\n--- Dry Run / Configuration Preview ---\")\n\n        # We need the index to check matches\n        # We instantiate the service just to query the index, not to modify\n        service = RedactionService(self.store, self.store_backend)\n\n        match_count = 0\n\n        for rule in self.active_rules:\n            serial = rule.get(\"serial_number\", \"UNKNOWN\")\n            model = rule.get(\"model_name\", \"Unknown Model\")\n            zones = rule.get(\"redaction_zones\", [])\n\n            # check matches in store\n            targets = service.index.get_by_machine(serial)\n\n            if targets:\n                count = len(targets)\n                match_count += count\n                print(f\"MATCH: '{serial}' ({model})\")\n                print(f\"    - Found {count} images in current session.\")\n                print(f\"    - Actions: Will apply {len(zones)} redaction zones.\")\n            else:\n                print(f\"NO MATCH: '{serial}'. Rule loaded, but no images found.\")\n\n        print(f\"\\nSummary: Execution will modify approximately {match_count} images.\")\n        print(\"---------------------------------------\")\n\n    def redact(self, show_progress=True):\n        \"\"\"\n        User Action: 'Apply the currently loaded rules to the pixel data.'\n        \"\"\"\n        if not self.active_rules:\n            get_logger().warning(\"No configuration loaded. Use .load_config() first.\")\n            print(\"No configuration loaded. Use .load_config() first.\")\n            return\n\n        service = RedactionService(self.store, self.store_backend)\n\n        try:\n            from concurrent.futures import ThreadPoolExecutor\n            import os\n\n            # Parallel Execution for Speed\n            # Threading works well here because pixel I/O and NumPy ops release GIL.\n            # Shared memory allows in-place modification of instances.\n            # OPTIMIZATION: Limited to 0.5x CPU or Max 8 to prevent OOM with large datasets\n            cpu_count = os.cpu_count() or 1\n            max_workers = max(1, min(int(cpu_count * 0.5), 8))\n            # Generate granular tasks for better load balancing\n            all_tasks = []\n            get_logger().info(\"Analyzing workload...\")\n            for rule in self.active_rules:\n                tasks = service.prepare_redaction_tasks(rule)\n                all_tasks.extend(tasks)\n\n            if not all_tasks:\n                get_logger().warning(\"No matching images found for any loaded rules.\")\n                print(\"No matching images found for any loaded rules.\")\n                return\n\n            print(f\"Queued {len(all_tasks)} redaction tasks across {len(self.active_rules)} rules.\")\n            print(f\"Executing using {max_workers} threads...\")\n            # 2. Parallel Redaction (Granular)\n            get_logger().info(f\"Starting granular redaction ({len(all_tasks)} tasks, workers={max_workers})...\")\n\n            # NOTE: We force threads for redaction because pixel manipulation in numpy releases GIL, \n            # and pickling full objects for Processes is slower and less robust (pickling errors).\n            # However, for huge loads, Processes might be better. \n            # BUT the user issue \"semaphore leak\" implies Processes were being used implicitly or somewhere else.\n            # Check 'force_threads'. Providing self._executor (ProcessPool) will conflict if force_threads=True.\n            # run_parallel logic: if executor is passed, it uses it.\n            # So we MUST NOT pass self._executor if we strictly want threads.\n\n            # DECISION: Redaction currently uses force_threads=True.\n            # If we stick to threads, we don't use the shared ProcessPool.\n            # So we leave this call alone (creating a ThreadPool is cheap).\n\n            # run_parallel(service.execute_redaction_task, all_tasks, desc=\"Redacting Pixels\", max_workers=max_workers, force_threads=True)\n            # However, if we move to Processes later, we should use self._executor.\n            # For now, keep as is to avoid regression on the threading model.\n\n            # Enable GC Optimization\n            import gc\n            gc.disable()\n            try:\n                run_parallel(service.execute_redaction_task, all_tasks, desc=\"Redacting Pixels\", max_workers=max_workers, force_threads=True, progress=show_progress)\n            finally:\n                gc.enable()\n                gc.collect()\n\n            # Save state after modification\n            # self._save()\n            # Run Safety Checks\n            service.scan_burned_in_annotations()\n\n            print(\"Execution Complete. Remember to call .save() to persist.\")\n            # get_logger().info(\"Execution Complete. Session saved.\")\n            print(\"Execution Complete. Session saved.\")\n\n            # Clear rules after execution?\n            # Optional: Keep them if user wants to run again on new imports.\n            # self.active_rules = []\n\n        except Exception as e:\n            get_logger().error(f\"Execution interrupted: {e}\")\n            print(f\"Execution interrupted: {e}\")\n\n    def create_config(self, output_path: str):\n        \"\"\"\n        Generates a unified v2 configuration file in YAML format.\n        Includes default PHI tags + Auto-detected machine inventory.\n        \"\"\"\n        import yaml\n        import os\n\n        # Helper for Flow-Style Lists (Bracketed)\n        class FlowList(list): pass\n\n        def flow_list_representer(dumper, data):\n            return dumper.represent_sequence('tag:yaml.org,2002:seq', data, flow_style=True)\n\n        yaml.add_representer(FlowList, flow_list_representer)\n\n        if not (output_path.endswith(\".yaml\") or output_path.endswith(\".yml\")):\n            output_path += \".yaml\"\n            print(f\"Note: Appending .yaml extension -&gt; {output_path}\")\n\n        # 1. Identify what we have\n        all_equipment = self.store.get_unique_equipment()\n\n        # Instantiate service to query pixel/tag data efficiently\n        from .services import RedactionService\n        service = RedactionService(self.store)\n\n        # 2. Identify what is already configured (Pixel Rules)\n        configured_serials = {rule.get(\"serial_number\") for rule in self.active_rules}\n\n        # Load Knowledge Base for Machines\n        kb_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), \"resources\", \"redaction_rules.json\")\n        kb_machines = []\n        if os.path.exists(kb_path):\n             try:\n                 import json\n                 with open(kb_path, 'r') as f:\n                     kb_data = json.load(f)\n                     kb_machines = kb_data.get(\"machines\", [])\n             except: pass\n\n        # 3. Find missing machines and try to pre-fill\n        missing_configs = []\n        for eq in all_equipment:\n            if eq.device_serial_number and eq.device_serial_number not in configured_serials:\n\n                # Check KB\n                matched_rule = None\n                # Primary: Serial Match\n                for rule in kb_machines:\n                    if rule.get(\"serial_number\") == eq.device_serial_number:\n                        matched_rule = rule\n                        break\n\n                # Check CTP Rules (Knowledge Base 2)\n                ctp_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), \"resources\", \"ctp_rules.yaml\")\n                if not os.path.exists(ctp_path):\n                     # Fallback to JSON if YAML doesn't exist\n                     ctp_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), \"resources\", \"ctp_rules.json\")\n\n                if not matched_rule and os.path.exists(ctp_path):\n                     try:\n                         if ctp_path.endswith('.yaml'):\n                             import yaml\n                             with open(ctp_path, 'r') as f:\n                                 ctp_data = yaml.safe_load(f)\n                         else:\n                             import json\n                             with open(ctp_path, 'r') as f:\n                                 ctp_data = json.load(f)\n\n                         ctp_rules = ctp_data.get(\"rules\", [])\n\n                         for rule in ctp_rules:\n                             # Fuzzy matching on Manufacturer and Model\n                             # CTP rules usually have \"manufacturer\" and \"model_name\"\n                             r_man = rule.get(\"manufacturer\", \"\").lower()\n                             r_mod = rule.get(\"model_name\", \"\").lower()\n\n                             eq_man = (eq.manufacturer or \"\").lower()\n                             eq_mod = (eq.model_name or \"\").lower()\n\n                             # Simple containment check as per CTP style\n                             if r_man and r_man in eq_man and r_mod and r_mod in eq_mod:\n                                  matched_rule = rule.copy()\n                                  matched_rule[\"serial_number\"] = eq.device_serial_number\n\n                                  # Move _ctp_condition to comment if present\n                                  cond = matched_rule.pop(\"_ctp_condition\", None)\n                                  if cond:\n                                      matched_rule[\"comment\"] = f\"Auto-matched from CTP. Condition: {cond}\"\n                                  else:\n                                      matched_rule[\"comment\"] = f\"Auto-matched from CTP Knowledge Base ({rule.get('manufacturer')} {rule.get('model_name')})\"\n\n                                  break\n\n                     except Exception as e:\n                         get_logger().warning(f\"Failed to load CTP rules: {e}\")\n\n                # Secondary: Model Match (Internal KB)\n                if not matched_rule:\n                    for rule in kb_machines:\n                        if rule.get(\"model_name\") == eq.model_name:\n                             # It's a model match, so we should probably copy the zones \n                             matches_man = not rule.get(\"manufacturer\") or (rule.get(\"manufacturer\") == eq.manufacturer)\n                             if matches_man:\n                                 matched_rule = rule.copy()\n                                 matched_rule[\"serial_number\"] = eq.device_serial_number\n                                 matched_rule[\"comment\"] = f\"Auto-matched from Model Knowledge Base ({eq.model_name})\"\n                                 break\n\n                # 3.b Check for Burned In Annotations (Safety Check)\n                # query index for this machine\n                instances = service.index.get_by_machine(eq.device_serial_number)\n                burned_in_count = 0\n                for inst in instances:\n                    val = inst.attributes.get(\"0028,0301\", \"NO\")\n                    if isinstance(val, str) and \"YES\" in val.upper():\n                        burned_in_count += 1\n\n                safety_comment = \"\"\n                if burned_in_count &gt; 0:\n                    safety_comment = f\"WARNING: {burned_in_count} images have 'Burned In Annotation' flag. Verify pixel redaction.\"\n\n                if matched_rule:\n                    # Use the template\n                    rule_copy = matched_rule.copy() # Ensure we don't mutate KB\n                    if safety_comment:\n                        existing = rule_copy.get(\"comment\", \"\")\n                        rule_copy[\"comment\"] = f\"{existing} {safety_comment}\".strip()\n                    missing_configs.append(rule_copy)\n                else:\n                    # Create empty scaffold\n                    new_rule = {\n                        \"manufacturer\": eq.manufacturer or \"Unknown\",\n                        \"model_name\": eq.model_name or \"Unknown\",\n                        \"serial_number\": eq.device_serial_number,\n                        \"redaction_zones\": [] \n                    }\n                    if safety_comment:\n                        new_rule[\"comment\"] = safety_comment\n                    missing_configs.append(new_rule)\n\n        # 4. Load PHI Tags Default (if not loaded)\n        phi_tags = self.active_phi_tags\n        if not phi_tags:\n             # Load default config for scaffold\n             try:\n                 from .config_manager import ConfigLoader\n                 phi_tags = ConfigLoader.load_phi_config() \n             except Exception as e:\n                 get_logger().warning(f\"Failed to load research tags: {e}\")\n\n        # 4b. Enhance PHI Tags (Transform to structured defaults)\n        structured_tags = {}\n\n        # Ensure critical tags are present\n        if \"0008,0020\" not in phi_tags: phi_tags[\"0008,0020\"] = \"Study Date\"\n        if \"0010,0040\" not in phi_tags: phi_tags[\"0010,0040\"] = \"Patient Sex\"\n        if \"0010,1010\" not in phi_tags: phi_tags[\"0010,1010\"] = \"Patient Age\" # Helper\n\n        for tag, val in phi_tags.items():\n            name = val if isinstance(val, str) else val.get(\"name\", \"Unknown\")\n            action = \"REMOVE\" # Default safety\n\n            # Apply Research-Friendly Smart Defaults\n            if tag == \"0008,0020\": # Study Date\n                action = \"JITTER\"\n            elif tag == \"0010,0040\": # Sex\n                action = \"KEEP\"\n            elif tag == \"0010,1010\": # Age\n                action = \"KEEP\"\n            elif \"Date\" in name or \"Time\" in name:\n                action = \"REMOVE\" # Times are sensitive\n            elif \"ID\" in name:\n                action = \"REMOVE\" # IDs are sensitive\n\n            # Preserve existing structure if it was already structured\n            if isinstance(val, dict):\n                structured_tags[tag] = val\n            else:\n                 # Minimal Scaffold: Skip tags that are simply REMOVED (covered by Basic profile)\n                 # Unless explicitly requested to show all? For now, match tests.\n                 if action == \"REMOVE\":\n                     continue\n\n                 structured_tags[tag] = {\n                     \"name\": name,\n                     \"action\": action\n                 }\n\n        # 5. Construct Unified Data\n        data = {\n            \"version\": \"2.0\",\n            \"privacy_profile\": \"basic\",\n            # No _instructions dict anymore, we use comments!\n            \"phi_tags\": structured_tags,\n            \"date_jitter\": {\n                \"min_days\": -365,\n                \"max_days\": -1\n            },\n            \"remove_private_tags\": True,\n            \"machines\": missing_configs + self.active_rules\n        }\n\n        if not missing_configs and not self.active_rules:\n             print(\"No machines detected to scaffold.\")\n\n        # Pre-process data to ensure comments are single-line strings\n        # And ensure redaction_zones use FlowList for bracketed style\n        for m in data.get(\"machines\", []):\n            if \"comment\" in m and isinstance(m[\"comment\"], str):\n                # Replace newlines with spaces/semicolons\n                m[\"comment\"] = m[\"comment\"].replace(\"\\n\", \" \").replace(\"\\r\", \"\")\n                # collapse multiple spaces\n                import re\n                m[\"comment\"] = re.sub(r'\\s+', ' ', m[\"comment\"]).strip()\n\n            if \"redaction_zones\" in m and isinstance(m[\"redaction_zones\"], list):\n                # Wrap inner lists (zones) in FlowList\n                # And assume user wants [[...], [...]] so wrap outer too?\n                # User example: \"redaction_zones: []\" or \"redaction_zones: [[...]]\"\n                # If we wrap outer in FlowList, it becomes: redaction_zones: [[...], [...]]\n                # If we wrap inner in FlowList, it becomes:\n                # redaction_zones:\n                #   - [50, 420, ...]\n                #\n                # The user request \"placed in brackets\" usually implies flow style.\n                # Let's try wrapping OUTER list.\n\n                zones = m[\"redaction_zones\"]\n                new_zones = FlowList()\n                for z in zones:\n                    if isinstance(z, list):\n                        new_zones.append(FlowList(z))\n                    else:\n                        new_zones.append(z)\n                m[\"redaction_zones\"] = new_zones # Assign flow list wrapper\n\n        try:\n            # Generate YAML string\n            # sort_keys=False ensures order is preserved (machines list)\n            # width=float(\"inf\") prevents line wrapping for long strings\n            yaml_content = yaml.dump(data, sort_keys=False, default_flow_style=False, width=float(\"inf\"))\n\n            # Post-process: Convert \"comment: ...\" into \"# ...\"\n            # Matches:   comment: \"Some text\"\n            # or         comment: Some text\n            import re\n            lines = yaml_content.splitlines()\n            new_lines = []\n            for line in lines:\n                # Simple match for key-value pair\n                match = re.search(r'^(\\s*)comment:\\s*(.*)$', line)\n                if match:\n                    indent = match.group(1)\n                    content = match.group(2).strip()\n\n                    # Check for surrounding quotes and strip them\n                    # Handle single quotes (yaml uses '' escape)\n                    if content.startswith(\"'\") and content.endswith(\"'\"):\n                        content = content[1:-1]\n                        content = content.replace(\"''\", \"'\")\n                    # Handle double quotes (json style/yaml style with backslash)\n                    elif content.startswith('\"') and content.endswith('\"'):\n                        content = content[1:-1]\n                        content = content.replace('\\\\\"', '\"')\n\n                    new_lines.append(f\"{indent}# {content}\")\n                else:\n                    # Aesthetic Improvement: Add spacing between list items\n                    # Check if line looks like the start of a new list entry (e.g. \"- manufacturer: ...\")\n                    # But exclude the very first one to avoid leading newline at top of file (or top of section)\n                    if line.strip().startswith(\"- \") and len(new_lines) &gt; 0 and new_lines[-1].strip() != \"\":\n                         new_lines.append(\"\")\n\n                    new_lines.append(line)\n\n            # Prepend Header Comments\n            header = \"\"\"# Gantry Privacy Configuration (v2.0)\n# ==========================================\n#\n# privacy_profile: \"basic\"\n#   - Standard profile handling common PHI (Name, ID, etc).\n#   - Set to \"none\" for manual control.\n#\n# phi_tags:\n#   - Define custom overrides here.\n#   - Actions: KEEP, REMOVE, EMPTY, REPLACE, JITTER (SHIFT)\n#\n# date_jitter:\n#   - Range of days to shift dates by (negative = into past).\n#\n# remove_private_tags:\n#   - If true, removes all odd-group tags except Gantry Metadata.\n#\n\"\"\"\n            final_content = header + \"\\n\".join(new_lines) + \"\\n\"\n\n            with open(output_path, 'w') as f:\n                f.write(final_content)\n\n            get_logger().info(f\"Scaffolded Unified Config to {output_path} ({len(missing_configs)} new machines)\")\n            print(f\"Scaffolded Unified Config to {output_path}\")\n        except Exception as e:\n            get_logger().error(f\"Failed to write scaffold: {e}\")\n</code></pre>"},{"location":"api/session/#gantry.session.DicomSession.ingest","title":"<code>ingest(folder_path)</code>","text":"<p>Scans a folder for .dcm files (recursively) and imports them into the session.</p> Source code in <code>gantry/session.py</code> <pre><code>def ingest(self, folder_path):\n    \"\"\"\n    Scans a folder for .dcm files (recursively) and imports them into the session.\n    \"\"\"\n    print(f\"Ingesting from '{folder_path}'...\")\n    DicomImporter.import_files([folder_path], self.store, executor=self._executor)\n\n    # Calculate stats\n    n_p = len(self.store.patients)\n    n_st = sum(len(p.studies) for p in self.store.patients)\n    n_se = sum(len(st.series) for p in self.store.patients for st in p.studies)\n    n_i = sum(len(se.instances) for p in self.store.patients for st in p.studies for se in st.series)\n\n    print(\"\\nIngestion Complete.\")\n    print(\"Summary:\")\n    print(f\"  - {n_p} Patients\")\n    print(f\"  - {n_st} Studies\")\n    print(f\"  - {n_se} Series\")\n    print(f\"  - {n_i} Instances\")\n    print(\"Remember to call .save() to persist changes.\\n\")\n</code></pre>"},{"location":"api/session/#gantry.session.DicomSession.save","title":"<code>save()</code>","text":"<p>Persists the current session state to the database in the background. User must call this manually to save changes.</p> Source code in <code>gantry/session.py</code> <pre><code>def save(self):\n    \"\"\"\n    Persists the current session state to the database in the background.\n    User must call this manually to save changes.\n    \"\"\"\n    self.persistence_manager.save_async(self.store.patients)\n</code></pre>"},{"location":"api/session/#gantry.session.DicomSession.export","title":"<code>export(folder, safe=False, compression=None, subset=None, show_progress=True)</code>","text":"<p>Exports the current state of all patients to a folder. If safe=True, performs a fresh PHI scan and ONLY exports clean data. compression: 'j2k' (JPEG 2000 Lossless) or None (Uncompressed) subset: Optional filter. Can be:         - pd.DataFrame: A dataframe containing 'sop_instance_uid' column.         - str: A pandas querystring (e.g. \"Modality == 'CT'\") applied to the full index.         - List[str]: A list of SOPInstanceUIDs to export.</p> Source code in <code>gantry/session.py</code> <pre><code>def export(self, folder, safe=False, compression=None, subset=None, show_progress=True):\n    \"\"\"\n    Exports the current state of all patients to a folder.\n    If safe=True, performs a fresh PHI scan and ONLY exports clean data.\n    compression: 'j2k' (JPEG 2000 Lossless) or None (Uncompressed)\n    subset: Optional filter. Can be:\n            - pd.DataFrame: A dataframe containing 'sop_instance_uid' column.\n            - str: A pandas querystring (e.g. \"Modality == 'CT'\") applied to the full index.\n            - List[str]: A list of SOPInstanceUIDs to export.\n    \"\"\"\n    # AUTO-OPTIMIZATION: Ensure clean memory before spawning processes\n    get_logger().info(\"Preparing for export (Auto-Save &amp; Memory Release)...\")\n    if show_progress:\n        print(\"Preparing for export (saving &amp; releasing memory)...\")\n    self.save()\n    self.persistence_manager.flush()\n    self.release_memory()\n\n    get_logger().info(f\"Exporting session to {folder} (safe={safe})...\")\n    if show_progress:\n        print(\"Exporting...\")\n\n    dirty_patients = set()\n    dirty_studies = set()\n\n    if safe:\n        if show_progress:\n            print(\"Running safety scan...\")\n        report = self.audit()\n        for finding in report:\n            if finding.entity_type == \"Patient\":\n                dirty_patients.add(finding.entity_uid)\n            elif finding.entity_type == \"Study\":\n                dirty_studies.add(finding.entity_uid)\n\n        if dirty_patients or dirty_studies:\n            # Group findings by Tag\n            tag_summary = {} # tag -&gt; {desc, count, example_val}\n            for finding in report:\n                if finding.tag not in tag_summary:\n                    tag_summary[finding.tag] = {\n                        \"desc\": finding.field_name, \n                        \"count\": 0, \n                        \"examples\": set()\n                    }\n                tag_summary[finding.tag][\"count\"] += 1\n                if len(tag_summary[finding.tag][\"examples\"]) &lt; 3:\n                    tag_summary[finding.tag][\"examples\"].add(str(finding.value))\n\n            msg = f\"\\nSafety Scan Found Issues: {len(dirty_patients)} Patients, {len(dirty_studies)} Studies contain PHI.\"\n            msg += \"\\nThe following tags were flagged as dirty:\\n\"\n            msg += f\"{'Tag':&lt;15} {'Description':&lt;30} {'Count':&lt;10} {'Examples'}\\n\"\n            msg += \"-\" * 80 + \"\\n\"\n\n            config_suggestion = {}\n\n            for tag, info in tag_summary.items():\n                examples = \", \".join(info['examples'])\n                msg += f\"{tag:&lt;15} {info['desc']:&lt;30} {info['count']:&lt;10} {examples}\\n\"\n\n                # Suggest REMOVE or KEEP based on ... usually REMOVE for PHI\n                config_suggestion[tag] = {\"action\": \"REMOVE\", \"name\": info['desc']}\n\n            msg += \"\\nTo allow export, you must either REMOVE these tags or mark them as KEEP in your configuration.\\n\"\n            msg += \"Suggested Config Update:\\n\"\n            import json\n            msg += json.dumps({\"phi_tags\": config_suggestion}, indent=4)\n            msg += \"\\n\"\n\n            get_logger().warning(msg)\n            print(msg)\n\n    # Handle Subsetting\n    instance_uids = None\n    if subset is not None:\n        get_logger().info(\"Applying subset filter to export...\")\n        import pandas as pd\n\n        if isinstance(subset, pd.DataFrame):\n            if 'sop_instance_uid' not in subset.columns:\n                 # Check if index is UID? \n                 # For now, require column\n                 # Or maybe 'SOPInstanceUID' (DICOM standard case)\n                 if 'SOPInstanceUID' in subset.columns:\n                     instance_uids = subset['SOPInstanceUID'].tolist()\n                 else:\n                     raise ValueError(\"Subset DataFrame must contain 'sop_instance_uid' or 'SOPInstanceUID' column.\")\n            else:\n                 instance_uids = subset['sop_instance_uid'].tolist()\n\n        elif isinstance(subset, str):\n            # Query String -&gt; Generate DF -&gt; Filter\n            # Note: This loads full dataframe into memory to filter. \n            # Optimization: Could we push query to SQL? \n            # SQL is hard because attributes are JSON.\n            # So we export_dataframe() then query.\n            df = self.export_dataframe(expand_metadata=True) # Expand needed for meaningful queries\n            df_filtered = df.query(subset)\n            # handle both cases just in case\n            if 'SOPInstanceUID' in df_filtered.columns:\n                instance_uids = df_filtered['SOPInstanceUID'].tolist()\n            elif 'sop_instance_uid' in df_filtered.columns:\n                instance_uids = df_filtered['sop_instance_uid'].tolist()\n            else:\n                # Fallback to index if it happens to be the key? Unlikely for now.\n                raise ValueError(\"Column 'SOPInstanceUID' missing from internal dataframe.\")\n            get_logger().info(f\"Query '{subset}' matched {len(instance_uids)} instances.\")\n\n        elif isinstance(subset, list):\n            instance_uids = subset\n\n        if not instance_uids and instance_uids is not None:\n            get_logger().warning(\"Subset filter resulted in 0 instances. Nothing to export.\")\n            return\n\n    exported_count = 0\n    skipped_count = 0\n\n    # 1. Calculate Scope &amp; Total (In-Memory Helper)\n    patient_ids = []\n    total_instances = 0\n\n    # NOTE: If instance_uids is set, we can't easily count total_instances based on patients loop \n    # unless we do a DB count query.\n    # But for progress bar, we need a count.\n    if instance_uids:\n        total_instances = len(instance_uids)\n        # We don't populate patient_ids here because we pass instance_uids specifically.\n        # But the exporter supports patient_ids AND instance_uids.\n        # If we omit patient_ids, it searches all patients (inefficient SQL?).\n        # SqliteStore.get_flattened_instances currently:\n        # - if patient_ids: WHERE p.id IN ...\n        # - if instance_uids: WHERE i.uid IN ...\n        # Either works. If we have instance_uids, we don't need patient_ids.\n        # Although passing patient_ids reduces search space if we know them.\n        # Deriving patient_ids from instance_uids is expensive without a query.\n        # So we just pass instance_uids.\n    else:\n        for p in self.store.patients:\n            # Check Patient Level (Early Filter for ID list)\n            if safe and p.patient_id in dirty_patients:\n                get_logger().warning(f\"Skipping Dirty Patient: {p.patient_id}\")\n                skipped_count += getattr(p, 'instance_count', 0) # Estimate\n                continue\n\n            patient_ids.append(p.patient_id)\n\n            # Count instances for progress bar\n            if safe:\n                p_inst_count = 0\n                for st in p.studies:\n                    if st.study_instance_uid not in dirty_studies:\n                        p_inst_count += sum(len(se.instances) for se in st.series)\n                total_instances += p_inst_count\n            else:\n                total_instances += sum(sum(len(se.instances) for se in st.series) for st in p.studies)\n\n    if not patient_ids and not instance_uids:\n        get_logger().warning(\"No valid patients to export.\")\n        return\n\n    # 2. Generate tasks (Streaming from DB)\n    get_logger().info(f\"Queuing export for {total_instances} instances (SQL Streaming)...\")\n\n    raw_tasks = DicomExporter.generate_export_from_db(\n        self.persistence_manager.store_backend, \n        folder, \n        patient_ids=patient_ids if not instance_uids else None, \n        compression=compression,\n        instance_uids=instance_uids\n    )\n\n    # 3. Apply Safety Filter (Lazy)\n    # The DB generation is flattened, so we filter stream based on Context attributes\n    def safety_filter(generator):\n        for ctx in generator:\n            # We already filtered patient_ids list passed to SQL, \n            # but we need to filter Studies if they are dirty.\n            if safe:\n                uid = ctx.study_attributes.get(\"StudyInstanceUID\")\n                if uid and uid in dirty_studies:\n                    continue\n            yield ctx\n\n    export_tasks = safety_filter(raw_tasks) if safe else raw_tasks\n\n    # 4. Execution Phase (Global Parallelism)\n    # 4. Execution Phase (Global Parallelism)\n    if total_instances &gt; 0:\n        # MEMORY LEAK MITIGATION:\n        # We use worker recycling (maxtasksperchild=100) via multiprocessing.Pool\n        # This forces workers to restart periodically, clearing any leaked memory (e.g. from C-libs).\n        # We do NOT use the shared self._executor for this, as ProcessPoolExecutor doesn't support recycling.\n        try:\n            # Optimized for stability: maxtasksperchild=25 clears memory frequently\n            # GC Optimization: Disable GC in workers\n            success_count = DicomExporter.export_batch(export_tasks, show_progress=show_progress, total=total_instances, maxtasksperchild=25, disable_gc=True)\n        except Exception as e:\n            get_logger().error(f\"Export Failed! Error: {e}\")\n            raise e\n        finally:\n            # Main process GC trigger\n            import gc\n            gc.collect()\n\n        # Note: skipped_count is only patient-level skips. Study-level skips aren't counted here explicitly \n        # unless we wrap the generator to count them, but that's complex for a simple log.\n        get_logger().info(f\"Export complete.\")\n    else:\n        get_logger().warning(\"No instances queued for export.\")\n\n    print(\"Done.\")\n</code></pre>"},{"location":"api/session/#gantry.session.DicomSession.audit","title":"<code>audit(config_path=None)</code>","text":"<p>Scans all patients in the session for potential PHI. Uses cached <code>active_phi_tags</code> if config_path matches or is None, otherwise loads fresh. Returns a PhiReport object (iterable, and convertible to DataFrame). Checkpoint 4: Target.</p> Source code in <code>gantry/session.py</code> <pre><code>def audit(self, config_path: str = None) -&gt; \"PhiReport\":\n    \"\"\"\n    Scans all patients in the session for potential PHI.\n    Uses cached `active_phi_tags` if config_path matches or is None, otherwise loads fresh.\n    Returns a PhiReport object (iterable, and convertible to DataFrame).\n    Checkpoint 4: Target.\n    \"\"\"\n    from .privacy import PhiReport\n\n    # Logic: If config_path is provided, we should probably load it temporarily for this scan?\n    # OR if config_path is None, use self.active_phi_tags\n\n    tags_to_use = self.active_phi_tags\n\n    if config_path:\n         # Just load tags for this run, don't overwrite session state unless load_config called?\n         # Actually, if user says audit(\"file.json\"), they expect that file to control.\n         tags_to_use = ConfigLoader.load_phi_config(config_path)\n         # NOTE: If passing a config PATH to audit(), we might be missing the other unified settings \n         # (date_jitter, etc.) unless we load them too.\n         # For now, audit() focuses on finding things based on TAGS.\n         # If the inspector needs to know about date jitter or private tags to Flag them correctly?\n         # Private tags -&gt; YES. Jitter -&gt; Maybe not for detection, but definitely for Remediation proposal.\n\n         # Better approach: If config_path is Unified, load it all.\n         try:\n             t, r, dj, rpt = ConfigLoader.load_unified_config(config_path)\n             tags_to_use = t\n             # We probably shouldn't overwrite session state side-effects here, \n             # but for the worker arguments we need to pass them.\n             # Let's create a transient config object or just pass args.\n             # For simplicity in this function, we'll stick to tags, but we should fix inspector init.\n         except:\n             # Fallback to simple tags load\n             tags_to_use = ConfigLoader.load_phi_config(config_path)\n\n    inspector = PhiInspector(config_tags=tags_to_use, remove_private_tags=self.active_remove_private_tags)\n    if not inspector.phi_tags:\n        get_logger().warning(\"PHI Scan Warning: No PHI tags defined. Scan will find nothing. Check your config.\")\n\n    get_logger().info(\"Scanning for PHI (Parallel)...\")\n\n    # Hybrid Approach:\n    # Pass lightweight object CLONES to avoid \"Assert left &gt; 0\" IPC error\n    # AND to ensure we audit in-memory (unsaved) changes.\n    worker_args = []\n    for p in self.store.patients:\n        # Strip pixels to reduce size\n        light_p = self._make_lightweight_copy(p)\n        worker_args.append((light_p, tags_to_use, self.active_remove_private_tags))\n\n    results = run_parallel(scan_worker, worker_args, desc=\"Scanning PHI\")\n\n    all_findings = []\n    for findings in results:\n        all_findings.extend(findings)\n\n    # Rehydrate Entities!\n    self._rehydrate_findings(all_findings)\n\n    get_logger().info(f\"PHI Scan Complete. Found {len(all_findings)} issues.\")\n\n    return PhiReport(all_findings)\n</code></pre>"},{"location":"api/session/#gantry.session.DicomSession.redact","title":"<code>redact(show_progress=True)</code>","text":"<p>User Action: 'Apply the currently loaded rules to the pixel data.'</p> Source code in <code>gantry/session.py</code> <pre><code>def redact(self, show_progress=True):\n    \"\"\"\n    User Action: 'Apply the currently loaded rules to the pixel data.'\n    \"\"\"\n    if not self.active_rules:\n        get_logger().warning(\"No configuration loaded. Use .load_config() first.\")\n        print(\"No configuration loaded. Use .load_config() first.\")\n        return\n\n    service = RedactionService(self.store, self.store_backend)\n\n    try:\n        from concurrent.futures import ThreadPoolExecutor\n        import os\n\n        # Parallel Execution for Speed\n        # Threading works well here because pixel I/O and NumPy ops release GIL.\n        # Shared memory allows in-place modification of instances.\n        # OPTIMIZATION: Limited to 0.5x CPU or Max 8 to prevent OOM with large datasets\n        cpu_count = os.cpu_count() or 1\n        max_workers = max(1, min(int(cpu_count * 0.5), 8))\n        # Generate granular tasks for better load balancing\n        all_tasks = []\n        get_logger().info(\"Analyzing workload...\")\n        for rule in self.active_rules:\n            tasks = service.prepare_redaction_tasks(rule)\n            all_tasks.extend(tasks)\n\n        if not all_tasks:\n            get_logger().warning(\"No matching images found for any loaded rules.\")\n            print(\"No matching images found for any loaded rules.\")\n            return\n\n        print(f\"Queued {len(all_tasks)} redaction tasks across {len(self.active_rules)} rules.\")\n        print(f\"Executing using {max_workers} threads...\")\n        # 2. Parallel Redaction (Granular)\n        get_logger().info(f\"Starting granular redaction ({len(all_tasks)} tasks, workers={max_workers})...\")\n\n        # NOTE: We force threads for redaction because pixel manipulation in numpy releases GIL, \n        # and pickling full objects for Processes is slower and less robust (pickling errors).\n        # However, for huge loads, Processes might be better. \n        # BUT the user issue \"semaphore leak\" implies Processes were being used implicitly or somewhere else.\n        # Check 'force_threads'. Providing self._executor (ProcessPool) will conflict if force_threads=True.\n        # run_parallel logic: if executor is passed, it uses it.\n        # So we MUST NOT pass self._executor if we strictly want threads.\n\n        # DECISION: Redaction currently uses force_threads=True.\n        # If we stick to threads, we don't use the shared ProcessPool.\n        # So we leave this call alone (creating a ThreadPool is cheap).\n\n        # run_parallel(service.execute_redaction_task, all_tasks, desc=\"Redacting Pixels\", max_workers=max_workers, force_threads=True)\n        # However, if we move to Processes later, we should use self._executor.\n        # For now, keep as is to avoid regression on the threading model.\n\n        # Enable GC Optimization\n        import gc\n        gc.disable()\n        try:\n            run_parallel(service.execute_redaction_task, all_tasks, desc=\"Redacting Pixels\", max_workers=max_workers, force_threads=True, progress=show_progress)\n        finally:\n            gc.enable()\n            gc.collect()\n\n        # Save state after modification\n        # self._save()\n        # Run Safety Checks\n        service.scan_burned_in_annotations()\n\n        print(\"Execution Complete. Remember to call .save() to persist.\")\n        # get_logger().info(\"Execution Complete. Session saved.\")\n        print(\"Execution Complete. Session saved.\")\n\n        # Clear rules after execution?\n        # Optional: Keep them if user wants to run again on new imports.\n        # self.active_rules = []\n\n    except Exception as e:\n        get_logger().error(f\"Execution interrupted: {e}\")\n        print(f\"Execution interrupted: {e}\")\n</code></pre>"},{"location":"api/session/#gantry.session.DicomSession.load_config","title":"<code>load_config(config_file)</code>","text":"<p>User Action: 'Load these rules into memory, but DO NOT run them yet.' Useful for validation or previewing what will happen.</p> Source code in <code>gantry/session.py</code> <pre><code>def load_config(self, config_file: str):\n    \"\"\"\n    User Action: 'Load these rules into memory, but DO NOT run them yet.'\n    Useful for validation or previewing what will happen.\n    \"\"\"\n    try:\n        get_logger().info(f\"Loading configuration from {config_file}...\")\n        print(f\"Loading configuration from {config_file}...\")\n\n        # UNIFIED LOAD (v2)\n        tags, rules, jitter, remove_private = ConfigLoader.load_unified_config(config_file)\n\n        self.active_phi_tags = tags\n        self.active_rules = rules\n        self.active_date_jitter = jitter\n        self.active_remove_private_tags = remove_private\n\n        get_logger().info(f\"Loaded {len(self.active_rules)} machine rules and {len(self.active_phi_tags)} PHI tags.\")\n        print(f\"Configuration Loaded:\\n - {len(self.active_rules)} Machine Redaction Rules\\n - {len(self.active_phi_tags)} PHI Tags\")\n        print(f\" - Date Jitter: {self.active_date_jitter['min_days']} to {self.active_date_jitter['max_days']} days\")\n        print(f\" - Remove Private Tags: {self.active_remove_private_tags}\")\n        print(\"Tip: Run .audit() to check PHI, or .redact_pixels() to apply redaction.\")\n    except Exception as e:\n        import traceback\n        get_logger().error(f\"Load failed: {e}\")\n        print(f\"Load failed: {e}\")\n        print(traceback.format_exc())\n        self.active_rules = []\n        self.active_phi_tags = {}\n</code></pre>"}]}